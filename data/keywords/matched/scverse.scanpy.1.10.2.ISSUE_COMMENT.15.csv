id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/pull/1105:181,modifiability,maintain,maintaining,181,"I did not change the basis to `coords` because a lot of the time the basis is specified as. ```python. elif f""X_{basis}"" in adata.obsm.keys():. basis_key = f""X_{basis}"". ```. hence maintaining the `X` makes it consistent with the others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:60,performance,time,time,60,"I did not change the basis to `coords` because a lot of the time the basis is specified as. ```python. elif f""X_{basis}"" in adata.obsm.keys():. basis_key = f""X_{basis}"". ```. hence maintaining the `X` makes it consistent with the others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:181,safety,maintain,maintaining,181,"I did not change the basis to `coords` because a lot of the time the basis is specified as. ```python. elif f""X_{basis}"" in adata.obsm.keys():. basis_key = f""X_{basis}"". ```. hence maintaining the `X` makes it consistent with the others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:210,usability,consist,consistent,210,"I did not change the basis to `coords` because a lot of the time the basis is specified as. ```python. elif f""X_{basis}"" in adata.obsm.keys():. basis_key = f""X_{basis}"". ```. hence maintaining the `X` makes it consistent with the others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:185,safety,review,review,185,"Is that in embedding? I had thought we'd removed all that. There shouldn't be any special meaning to strings starting with `X_` anyways, it's just a convention. I'd like to give this a review, but I'd need to see that tests are passing. Do you know why travis isn't running on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:218,safety,test,tests,218,"Is that in embedding? I had thought we'd removed all that. There shouldn't be any special meaning to strings starting with `X_` anyways, it's just a convention. I'd like to give this a review, but I'd need to see that tests are passing. Do you know why travis isn't running on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:185,testability,review,review,185,"Is that in embedding? I had thought we'd removed all that. There shouldn't be any special meaning to strings starting with `X_` anyways, it's just a convention. I'd like to give this a review, but I'd need to see that tests are passing. Do you know why travis isn't running on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:218,testability,test,tests,218,"Is that in embedding? I had thought we'd removed all that. There shouldn't be any special meaning to strings starting with `X_` anyways, it's just a convention. I'd like to give this a review, but I'd need to see that tests are passing. Do you know why travis isn't running on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:76,deployability,build,builds,76,"Just re run it, [tests passed](https://travis-ci.org/github/theislab/scanpy/builds/663089197?utm_medium=notification&utm_source=github_status). No sure why it does not appear on github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:159,reliability,doe,does,159,"Just re run it, [tests passed](https://travis-ci.org/github/theislab/scanpy/builds/663089197?utm_medium=notification&utm_source=github_status). No sure why it does not appear on github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:17,safety,test,tests,17,"Just re run it, [tests passed](https://travis-ci.org/github/theislab/scanpy/builds/663089197?utm_medium=notification&utm_source=github_status). No sure why it does not appear on github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:17,testability,test,tests,17,"Just re run it, [tests passed](https://travis-ci.org/github/theislab/scanpy/builds/663089197?utm_medium=notification&utm_source=github_status). No sure why it does not appear on github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:49,availability,error,errors,49,"> If you change `X_coords` to `coords`, where do errors occur? Could you also point me to where you're seeing this code? I've definitely been using embeddings in `obsm` whose keys don't start with `""X_""`. Right, my bad, switched to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:49,performance,error,errors,49,"> If you change `X_coords` to `coords`, where do errors occur? Could you also point me to where you're seeing this code? I've definitely been using embeddings in `obsm` whose keys don't start with `""X_""`. Right, my bad, switched to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:49,safety,error,errors,49,"> If you change `X_coords` to `coords`, where do errors occur? Could you also point me to where you're seeing this code? I've definitely been using embeddings in `obsm` whose keys don't start with `""X_""`. Right, my bad, switched to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:49,usability,error,errors,49,"> If you change `X_coords` to `coords`, where do errors occur? Could you also point me to where you're seeing this code? I've definitely been using embeddings in `obsm` whose keys don't start with `""X_""`. Right, my bad, switched to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:63,deployability,fail,failing,63,"Hi @ivirshup this would be ready for review. . Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:63,reliability,fail,failing,63,"Hi @ivirshup this would be ready for review. . Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:37,safety,review,review,37,"Hi @ivirshup this would be ready for review. . Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:54,safety,test,test,54,"Hi @ivirshup this would be ready for review. . Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:37,testability,review,review,37,"Hi @ivirshup this would be ready for review. . Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/pull/1105:54,testability,test,test,54,"Hi @ivirshup this would be ready for review. . Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105
https://github.com/scverse/scanpy/issues/1107:32,interoperability,specif,specific,32,"I realize that I'm working on a specific dataset that fits into `layer` specification (i.e. same obs, same var), while in general this is not true (e.g. scRNA + scATAC). Still, multiplex could be analyzed from 2+ `AnnData` objects when `adata.uns['neighbors']` is present",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:72,interoperability,specif,specification,72,"I realize that I'm working on a specific dataset that fits into `layer` specification (i.e. same obs, same var), while in general this is not true (e.g. scRNA + scATAC). Still, multiplex could be analyzed from 2+ `AnnData` objects when `adata.uns['neighbors']` is present",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:65,modifiability,layer,layer,65,"I realize that I'm working on a specific dataset that fits into `layer` specification (i.e. same obs, same var), while in general this is not true (e.g. scRNA + scATAC). Still, multiplex could be analyzed from 2+ `AnnData` objects when `adata.uns['neighbors']` is present",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:177,performance,multiplex,multiplex,177,"I realize that I'm working on a specific dataset that fits into `layer` specification (i.e. same obs, same var), while in general this is not true (e.g. scRNA + scATAC). Still, multiplex could be analyzed from 2+ `AnnData` objects when `adata.uns['neighbors']` is present",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:312,interoperability,coordinat,coordinates,312,"Hi @dawe ! Just to chip in real quick, I think your suggestion makes a lot of sense and beside the examples you already mention I believe spatial transcriptomics data could also benefit a lot from such approach (since with such data you have both a knn graph from gene expression as well as a graph from spatial coordinates).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,deployability,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:585,energy efficiency,optim,optimizer,585,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:849,energy efficiency,optim,optimize,849,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,integrability,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,interoperability,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:913,interoperability,specif,specified,913,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:960,interoperability,specif,specific,960,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,modifiability,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:585,performance,optimiz,optimizer,585,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:688,performance,multiplex,multiplex,688,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:849,performance,optimiz,optimize,849,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,reliability,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,security,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:114,testability,integr,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python. def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]. G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:. part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]. # then run the optimizer. else:. membership, improv = la.find_partitions_multiplex(**params). for a in adata:. a.obs['multiplex'] = pd.Categorical(membership). ```. where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`. Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/issues/1107:193,energy efficiency,cool,cool,193,"Spatial data would also be a single anndata object, similarly to CITE-seq but with images in addition to expression counts. . Anyway, I think this functionality would be very useful and really cool to try out. Worth mentioning that `uns['neighbors']` will be moved to `obsp` (see theislab/scanpy-tutorials#14) and that probably @ivirshup is working along similar lines (see #1117 ).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107
https://github.com/scverse/scanpy/pull/1109:35,deployability,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1109:35,integrability,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1109:35,interoperability,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1109:35,modifiability,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1109:35,reliability,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1109:35,security,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1109:35,testability,integr,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109
https://github.com/scverse/scanpy/pull/1111:17,safety,test,test,17,"Could this get a test? Also, should this be documented somewhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1111
https://github.com/scverse/scanpy/pull/1111:17,testability,test,test,17,"Could this get a test? Also, should this be documented somewhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1111
https://github.com/scverse/scanpy/pull/1111:44,usability,document,documented,44,"Could this get a test? Also, should this be documented somewhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1111
https://github.com/scverse/scanpy/pull/1113:143,deployability,build,builds,143,cleaning caches did the trick. let’s see if the absolute path is also needed (rebuilding master): https://travis-ci.org/github/theislab/scanpy/builds/663442852. /edit: it did,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113
https://github.com/scverse/scanpy/pull/1113:9,performance,cach,caches,9,cleaning caches did the trick. let’s see if the absolute path is also needed (rebuilding master): https://travis-ci.org/github/theislab/scanpy/builds/663442852. /edit: it did,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113
https://github.com/scverse/scanpy/issues/1114:526,deployability,upgrad,upgrading,526,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:570,deployability,Version,Versions,570,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:776,deployability,version,version,776,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1085,deployability,api,api-wrap,1085,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1001,energy efficiency,core,core,1001,". I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:570,integrability,Version,Versions,570,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:776,integrability,version,version,776,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1085,integrability,api,api-wrap,1085,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1085,interoperability,api,api-wrap,1085,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:526,modifiability,upgrad,upgrading,526,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:570,modifiability,Version,Versions,570,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:717,modifiability,deco,decorator,717,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:776,modifiability,version,version,776,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1335,modifiability,pac,packaging,1335,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1257,performance,network,networkx,1257,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:521,reliability,doe,does,521,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:485,safety,input,inputs,485,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1859,safety,test,testpath,1859,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:680,security,certif,certifi,680,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1257,security,network,networkx,1257,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1859,testability,test,testpath,1859,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:25,usability,minim,minimum,25,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:367,usability,user,user-images,367,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:485,usability,input,inputs,485,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:543,usability,help,help,543,"Hi,. I just tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1486,usability,tool,toolkit,1486,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1698,usability,learn,learn,1698,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1929,usability,learn,learn,1929,"t tried with a minimum reproducible example and it seemed to work:. ```python. sc.__version__. >>> '1.4.5.1'. ```. ```python. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'). sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8). ```. ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object? Also, does upgrading pandas help? <details>. <summary> Versions</summary>. ```python. anndata==0.7.1. appnope==0.1.0. attrs==19.3.0. backcall==0.1.0. bleach==3.1.0. certifi==2019.11.28. cycler==0.10.0. decorator==4.4.2. defusedxml==0.6.0. entrypoints==0.3. get-version==2.1. h5py==2.10.0. importlib-metadata==1.5.0. ipykernel==5.1.4. ipython==7.13.0. ipython-genutils==0.2.0. jedi==0.16.0. Jinja2==2.11.1. joblib==0.14.1. json5==0.9.1. jsonschema==3.2.0. jupyter-client==5.3.4. jupyter-core==4.6.1. jupyterlab==1.2.6. jupyterlab-server==1.0.6. kiwisolver==1.1.0. legacy-api-wrap==1.2. leidenalg==0.7.0. llvmlite==0.31.0. louvain==0.6.1. MarkupSafe==1.1.1. matplotlib==3.2.0. mistune==0.8.4. natsort==7.0.1. nbconvert==5.6.1. nbformat==5.0.4. networkx==2.4. notebook==6.0.3. numba==0.48.0. numexpr==2.7.1. numpy==1.18.2. packaging==20.3. pandas==1.0.2. pandocfilters==1.4.2. parso==0.6.1. patsy==0.5.1. pexpect==4.8.0. pickleshare==0.7.5. prometheus-client==0.7.1. prompt-toolkit==3.0.3. ptyprocess==0.6.0. pycairo==1.19.0. Pygments==2.5.2. pyparsing==2.4.6. pyrsistent==0.15.7. python-dateutil==2.8.1. python-igraph==0.7.1.post7. pytz==2019.3. pyzmq==18.1.1. scanpy==1.4.5.1. scikit-learn==0.22.2.post1. scipy==1.4.1. seaborn==0.10.0. Send2Trash==1.5.0. setuptools-scm==3.5.0. six==1.14.0. statsmodels==0.11.1. tables==3.6.1. terminado==0.8.3. testpath==0.4.4. tornado==6.0.4. tqdm==4.43.0. traitlets==4.3.3. umap-learn==0.3.10. wcwidth==0.1.8. webencodings==0.5.1. zipp==2.2.0. ```. </details>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:156,availability,error,error,156,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:499,availability,error,error,499,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:38,deployability,upgrad,upgraded,38,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:253,deployability,upgrad,upgrading,253,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:638,deployability,stage,stage,638,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:38,modifiability,upgrad,upgraded,38,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:253,modifiability,upgrad,upgrading,253,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1545,modifiability,layer,layers,1545,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:156,performance,error,error,156,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:499,performance,error,error,499,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:156,safety,error,error,156,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:499,safety,error,error,499,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1042,security,lineag,lineages,1042,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:156,usability,error,error,156,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:499,usability,error,error,499,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. . After running your example I will just look into how I created adata to see if I can find the error. . This is what it looks like now: . ```. AnnData object with n_obs × n_vars = 2773 × 3783 . obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'. var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'. uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'. varm: 'PCs'. layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'. ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`. My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:99,deployability,version,version,99,"@fidelram, any thoughts on this? @liekevandehaar is this still occurring? Are you using the newest version of `anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:99,integrability,version,version,99,"@fidelram, any thoughts on this? @liekevandehaar is this still occurring? Are you using the newest version of `anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:99,modifiability,version,version,99,"@fidelram, any thoughts on this? @liekevandehaar is this still occurring? Are you using the newest version of `anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:28,availability,error,error,28,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:38,deployability,version,version,38,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:38,integrability,version,version,38,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:38,modifiability,version,version,38,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:28,performance,error,error,28,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:28,safety,error,error,28,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:28,usability,error,error,28,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:490,deployability,modul,module,490,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:707,energy efficiency,core,core,707,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:834,energy efficiency,core,core,834,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:964,energy efficiency,core,core,964,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1094,energy efficiency,core,core,1094,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1264,energy efficiency,core,core,1264,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:490,modifiability,modul,module,490,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:551,modifiability,pac,packages,551,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:691,modifiability,pac,packages,691,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:818,modifiability,pac,packages,818,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:948,modifiability,pac,packages,948,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1078,modifiability,pac,packages,1078,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1248,modifiability,pac,packages,1248,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:167,safety,except,exception,167,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:452,safety,input,input-,452,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:490,safety,modul,module,490,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1332,safety,Except,Exception,1332,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1373,safety,Except,Exception,1373,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:401,testability,Trace,Traceback,401,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:4,usability,experien,experiencing,4,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:452,usability,input,input-,452,"I'm experiencing a similar issue with my data. Like in the case of @liekevandehaar , sc.pl.rank_genes_groups() and sc.pl.violin() work fine, but I am getting the same exception saying 'Data must be 1-dimensional' when I attempt to run sc.pl.rank_genes_groups_violin. However, unlike in her case, sc.pl.tracksplot() works fine for me. ```. sc.pl.rank_genes_groups_violin(tmp,. groups='0',. n_genes=8). Traceback (most recent call last):. File ""<ipython-input-126-3db850431424>"", line 3, in <module>. n_genes=8). File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 729, in rank_genes_groups_violin. df[g] = X_col. File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__. self._set_item(key, value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item. self._ensure_valid_index(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index. value = Series(value). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__. data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). File ""/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array. raise Exception(""Data must be 1-dimensional""). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:44,deployability,observ,observed,44,"Btw, I can confirm what @liekevandehaar has observed. I am experiencing the issue with a data set in which adata.X was initially imported as a dense matrix, but NOT with another data set, in which adata.X was imported as a sparse matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:44,testability,observ,observed,44,"Btw, I can confirm what @liekevandehaar has observed. I am experiencing the issue with a data set in which adata.X was initially imported as a dense matrix, but NOT with another data set, in which adata.X was imported as a sparse matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:11,usability,confirm,confirm,11,"Btw, I can confirm what @liekevandehaar has observed. I am experiencing the issue with a data set in which adata.X was initially imported as a dense matrix, but NOT with another data set, in which adata.X was imported as a sparse matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:59,usability,experien,experiencing,59,"Btw, I can confirm what @liekevandehaar has observed. I am experiencing the issue with a data set in which adata.X was initially imported as a dense matrix, but NOT with another data set, in which adata.X was imported as a sparse matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:188,availability,error,error,188,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:111,integrability,transform,transforming,111,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:111,interoperability,transform,transforming,111,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:188,performance,error,error,188,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:188,safety,error,error,188,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:188,usability,error,error,188,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:86,integrability,batch,batch,86,"same issue with dense data:. AnnData object with n_obs × n_vars = 1008 × 30810. obs: 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'. var: 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'. uns: 'log1p', 'hvg', 'pca', 'neighbors', 'leiden', 'paga', 'leiden_sizes', 'leiden_colors', 'umap', 'rank_genes_groups'. obsm: 'X_pca', 'X_umap'. varm: 'PCs'. obsp: 'distances', 'connectivities'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:86,performance,batch,batch,86,"same issue with dense data:. AnnData object with n_obs × n_vars = 1008 × 30810. obs: 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'leiden'. var: 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'. uns: 'log1p', 'hvg', 'pca', 'neighbors', 'leiden', 'paga', 'leiden_sizes', 'leiden_colors', 'umap', 'rank_genes_groups'. obsm: 'X_pca', 'X_umap'. varm: 'PCs'. obsp: 'distances', 'connectivities'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3346,availability,sli,slice,3346,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2602,deployability,modul,module,2602," ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2887,deployability,scale,scale,2887,"scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise V",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1730,energy efficiency,core,core,1730,"3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1967,energy efficiency,core,core,1967,"GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2220,energy efficiency,core,core,2220," anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2887,energy efficiency,scale,scale,2887,"scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise V",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3189,energy efficiency,core,core,3189,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3440,energy efficiency,core,core,3440,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3730,energy efficiency,core,core,3730,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1255,integrability,batch,batch,1255,"06_10x_A375_BL.tsv"", delimiter=""\t"").transpose(). MM001 = sc.read_csv(""GSM3946506_10x_MM001_BL.tsv"", delimiter=""\t"").transpose(). MM011 = sc.read_csv(""GSM3946506_10x_MM011_BL.tsv"", delimiter=""\t"").transpose(). MM029 = sc.read_csv(""GSM3946506_10x_MM029_BL.tsv"", delimiter=""\t"").transpose(). MM031 = sc.read_csv(""GSM3946506_10x_MM031_BL.tsv"", delimiter=""\t"").transpose(). MM047 = sc.read_csv(""GSM3946506_10x_MM047_BL.tsv"", delimiter=""\t"").transpose(). MM057 = sc.read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1314,integrability,batch,batch,1314,"c.read_csv(""GSM3946506_10x_MM001_BL.tsv"", delimiter=""\t"").transpose(). MM011 = sc.read_csv(""GSM3946506_10x_MM011_BL.tsv"", delimiter=""\t"").transpose(). MM029 = sc.read_csv(""GSM3946506_10x_MM029_BL.tsv"", delimiter=""\t"").transpose(). MM031 = sc.read_csv(""GSM3946506_10x_MM031_BL.tsv"", delimiter=""\t"").transpose(). MM047 = sc.read_csv(""GSM3946506_10x_MM047_BL.tsv"", delimiter=""\t"").transpose(). MM057 = sc.read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1714,modifiability,pac,packages,1714,".read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1951,modifiability,pac,packages,1951," sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2204,modifiability,pac,packages,2204,"MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2602,modifiability,modul,module,2602," ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2744,modifiability,pac,packages,2744,"ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2887,modifiability,scal,scale,2887,"scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise V",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3173,modifiability,pac,packages,3173,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3424,modifiability,pac,packages,3424,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3714,modifiability,pac,packages,3714,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1255,performance,batch,batch,1255,"06_10x_A375_BL.tsv"", delimiter=""\t"").transpose(). MM001 = sc.read_csv(""GSM3946506_10x_MM001_BL.tsv"", delimiter=""\t"").transpose(). MM011 = sc.read_csv(""GSM3946506_10x_MM011_BL.tsv"", delimiter=""\t"").transpose(). MM029 = sc.read_csv(""GSM3946506_10x_MM029_BL.tsv"", delimiter=""\t"").transpose(). MM031 = sc.read_csv(""GSM3946506_10x_MM031_BL.tsv"", delimiter=""\t"").transpose(). MM047 = sc.read_csv(""GSM3946506_10x_MM047_BL.tsv"", delimiter=""\t"").transpose(). MM057 = sc.read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1314,performance,batch,batch,1314,"c.read_csv(""GSM3946506_10x_MM001_BL.tsv"", delimiter=""\t"").transpose(). MM011 = sc.read_csv(""GSM3946506_10x_MM011_BL.tsv"", delimiter=""\t"").transpose(). MM029 = sc.read_csv(""GSM3946506_10x_MM029_BL.tsv"", delimiter=""\t"").transpose(). MM031 = sc.read_csv(""GSM3946506_10x_MM031_BL.tsv"", delimiter=""\t"").transpose(). MM047 = sc.read_csv(""GSM3946506_10x_MM047_BL.tsv"", delimiter=""\t"").transpose(). MM057 = sc.read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2887,performance,scale,scale,2887,"scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise V",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3346,reliability,sli,slice,3346,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1828,safety,except,except,1828,"L.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2461,safety,except,exception,2461,"n(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2509,safety,except,exception,2509,"ults in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 324",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2576,safety,input,input-,2576,"-------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2602,safety,modul,module,2602," ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:3814,safety,except,except,3814,"_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self._sanitize_column(key, value). 3243 NDFrame._set_item(self, key, value). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. -> 3294 raise ValueError(. 3295 ""Cannot set a frame with no defined index "". 3296 ""and a value that cannot be converted to a Series"". ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:1617,testability,Trace,Traceback,1617,"se(). MM047 = sc.read_csv(""GSM3946506_10x_MM047_BL.tsv"", delimiter=""\t"").transpose(). MM057 = sc.read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2532,testability,Trace,Traceback,2532,"-------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._en",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:60,usability,minim,minimal,60,"We (mainly my student :smile: ) also gets this problem now, minimal reproducable example with data from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE134432:. ```. import pandas as pd. import scanpy as sc. import anndata. A375 = sc.read_csv(""GSM3946506_10x_A375_BL.tsv"", delimiter=""\t"").transpose(). MM001 = sc.read_csv(""GSM3946506_10x_MM001_BL.tsv"", delimiter=""\t"").transpose(). MM011 = sc.read_csv(""GSM3946506_10x_MM011_BL.tsv"", delimiter=""\t"").transpose(). MM029 = sc.read_csv(""GSM3946506_10x_MM029_BL.tsv"", delimiter=""\t"").transpose(). MM031 = sc.read_csv(""GSM3946506_10x_MM031_BL.tsv"", delimiter=""\t"").transpose(). MM047 = sc.read_csv(""GSM3946506_10x_MM047_BL.tsv"", delimiter=""\t"").transpose(). MM057 = sc.read_csv(""GSM3946506_10x_MM057_BL.tsv"", delimiter=""\t"").transpose(). MM074 = sc.read_csv(""GSM3946506_10x_MM074_BL.tsv"", delimiter=""\t"").transpose(). MM087 = sc.read_csv(""GSM3946506_10x_MM087_BL.tsv"", delimiter=""\t"").transpose(). MM099 = sc.read_csv(""GSM3946506_10x_MM099_BL.tsv"", delimiter=""\t"").transpose(). AnnDatas = [A375, MM001, MM011, MM029, MM031, MM047, MM057, MM074, MM087, MM099]. AnnData_names = [""A375"", ""MM001"", ""MM011"", ""MM029"", ""MM031"", ""MM047"", ""MM057"", ""MM074"", ""MM087"", ""MM099""]. All = anndata.concat(AnnDatas, label=""batch"", keys=AnnData_names). sc.tl.rank_genes_groups(All, 'batch', groups=['A375'], reference='MM001', method='wilcoxon'). sc.pl.rank_genes_groups(All, groups=['A375'], n_genes=20). sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). ```. Which results in:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/issues/1114:2576,usability,input,input-,2576,"-------------------------. ValueError Traceback (most recent call last). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _ensure_valid_index(self, value). 3291 try:. -> 3292 value = Series(value). 3293 except (ValueError, NotImplementedError, TypeError) as err:. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/series.py in __init__(self, data, index, dtype, name, copy, fastpath). 363 else:. --> 364 data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True). 365 . /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 528 if isinstance(data, np.ndarray):. --> 529 raise ValueError(""Data must be 1-dimensional""). 530 else:. ValueError: Data must be 1-dimensional. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-6-edeb6431ddac> in <module>. ----> 1 sc.pl.rank_genes_groups_violin(All, groups='A375', n_genes=8). /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in rank_genes_groups_violin(adata, groups, n_genes, gene_names, gene_symbols, use_raw, key, split, scale, strip, jitter, size, ax, show, save). 911 X_col = X_col.toarray().flatten(). 912 new_gene_names.append(g). --> 913 df[g] = X_col. 914 df['hue'] = adata.obs[groups_key].astype(str).values. 915 if reference == 'rest':. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in __setitem__(self, key, value). 3161 else:. 3162 # set column. -> 3163 self._set_item(key, value). 3164 . 3165 def _setitem_slice(self, key: slice, value):. /scratch/estroucken/miniconda3/envs/scepia/lib/python3.8/site-packages/pandas/core/frame.py in _set_item(self, key, value). 3239 ensure homogeneity. 3240 """""". -> 3241 self._ensure_valid_index(value). 3242 value = self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114
https://github.com/scverse/scanpy/pull/1116:81,availability,error,errors,81,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:46,deployability,updat,update,46,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:81,performance,error,errors,81,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:46,safety,updat,update,46,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:81,safety,error,errors,81,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:46,security,updat,update,46,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:81,usability,error,errors,81,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both? I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:9,energy efficiency,current,currently,9,"Hey, I'm currently trying to finish up my thesis, sorry for the lack of communication. As this currently mostly adds code, go ahead and change what you want, I'll rebase this once theislab/anndata#342 is through.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:95,energy efficiency,current,currently,95,"Hey, I'm currently trying to finish up my thesis, sorry for the lack of communication. As this currently mostly adds code, go ahead and change what you want, I'll rebase this once theislab/anndata#342 is through.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:74,energy efficiency,power,powerful,74,@flying-sheep do you want to close this in favor of your future much more powerful plotting revamp?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1116:29,usability,close,close,29,@flying-sheep do you want to close this in favor of your future much more powerful plotting revamp?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116
https://github.com/scverse/scanpy/pull/1117:117,performance,multiplex,multiplex,117,I just issued (and soon closed) #1107 about the same idea. Wouldn’t be better to support more than two graphs in the multiplex? `leidenalg` functions work perfectly in case of 3+ graphs and it is extremely fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:24,usability,close,closed,24,I just issued (and soon closed) #1107 about the same idea. Wouldn’t be better to support more than two graphs in the multiplex? `leidenalg` functions work perfectly in case of 3+ graphs and it is extremely fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:81,usability,support,support,81,I just issued (and soon closed) #1107 about the same idea. Wouldn’t be better to support more than two graphs in the multiplex? `leidenalg` functions work perfectly in case of 3+ graphs and it is extremely fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:923,availability,down,downstream,923,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1759,deployability,log,log,1759," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2503,deployability,scale,scale,2503," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2532,deployability,depend,depending,2532," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2503,energy efficiency,scale,scale,2503," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:481,integrability,transform,transforms,481,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:566,integrability,transform,transform,566,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:708,integrability,sub,subsets,708,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1245,integrability,event,eventually,1245,"rs? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1645,integrability,transform,transform,1645," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1786,integrability,sub,sub,1786," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1909,integrability,transform,transform,1909," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2100,integrability,transform,transform,2100," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2395,integrability,standardiz,standardized,2395," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2532,integrability,depend,depending,2532," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:481,interoperability,transform,transforms,481,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:566,interoperability,transform,transform,566,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:749,interoperability,distribut,distribution,749,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:997,interoperability,distribut,distribution,997,"nks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1645,interoperability,transform,transform,1645," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1909,interoperability,transform,transform,1909," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2100,interoperability,transform,transform,2100," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2373,interoperability,distribut,distribution,2373," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2395,interoperability,standard,standardized,2395," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2575,interoperability,compatib,compatible,2575," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:988,modifiability,variab,variable,988,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2503,modifiability,scal,scale,2503," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2532,modifiability,depend,depending,2532," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1527,performance,time,time-tested,1527,"lar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2503,performance,scale,scale,2503," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1061,safety,test,testing,1061,"erent signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transfor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1532,safety,test,tested,1532,"r panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, dep",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1759,safety,log,log,1759," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2532,safety,depend,depending,2532," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:71,security,sign,signal,71,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1759,security,log,log,1759," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2468,security,sign,signal,2468," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1061,testability,test,testing,1061,"erent signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transfor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1532,testability,test,tested,1532,"r panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, dep",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1759,testability,log,log,1759," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2435,testability,simpl,simple,2435," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2532,testability,depend,depending,2532," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:464,usability,prefer,prefer,464,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:624,usability,visual,visual,624,"thanks for getting this started! since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1183,usability,user,user,1183," normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1198,usability,document,documenting,1198," justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1303,usability,behavi,behaviors,1303,"/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1680,usability,confirm,confirm,1680," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2317,usability,workflow,workflows,2317," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2435,usability,simpl,simple,2435," of:. - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets). - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes). - making it easier to spot nonspecific antibody staining / off-target effects. - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python. def CLR_transform(df):. '''. implements the CLR transform used in CITEseq (need to confirm in Seurat's code). https://doi.org/10.1038/nmeth.4380. '''. logn1 = np.log(df + 1). T_clr = logn1.sub(logn1.mean(axis=1), axis=0). return T_clr. def asinh_transform(df, cofactor=5):. '''. implements the hyperbolic arcsin transform used in CyTOF/mass cytometry. https://doi.org/10.1038/nmeth.4380. '''. T_cytof = np.arcsinh(df / cofactor). return T_cytof. def geometric_transform(df):. '''. implements the scanpy transform originating from ivirshup:multimodal. '''. from scipy.stats.mstats import gmean. T_geometric = np.divide(df, gmean(df + 1, axis=0)). return T_geometric. #optionally, for each of these, similar to some cytof workflows, . #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range. #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale. #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:407,availability,reliab,reliable,407,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:794,availability,error,error,794,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:198,deployability,artifact,artifacts,198,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:752,deployability,scale,scale,752,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:858,deployability,log,log,858,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1115,deployability,log,log,1115,"sed in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1167,deployability,scale,scale,1167,"s (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1380,deployability,log,log,1380," statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1959,deployability,log,log,1959," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2335,deployability,compos,compositional,2335," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2510,deployability,observ,observed,2510," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:752,energy efficiency,scale,scale,752,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1167,energy efficiency,scale,scale,1167,"s (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1203,energy efficiency,power,power,1203,"s); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2178,integrability,inject,injecting,2178," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2650,integrability,transform,transforms,2650," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2832,integrability,transform,transform,2832," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:20,interoperability,distribut,distribution,20,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2263,interoperability,specif,specifically,2263," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2597,interoperability,distribut,distribution,2597," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2650,interoperability,transform,transforms,2650," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2832,interoperability,transform,transform,2832," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:752,modifiability,scal,scale,752,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1167,modifiability,scal,scale,1167,"s (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2335,modifiability,compos,compositional,2335," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:752,performance,scale,scale,752,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:794,performance,error,error,794,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1167,performance,scale,scale,1167,"s (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2752,performance,time,time,2752," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:407,reliability,reliab,reliable,407,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:419,reliability,pra,practice,419,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1124,reliability,doe,doesn,1124,"e cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1403,reliability,doe,doesn,1403," reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2118,reliability,doe,does,2118," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:794,safety,error,error,794,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:858,safety,log,log,858,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1115,safety,log,log,1115,"sed in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1380,safety,log,log,1380," statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1959,safety,log,log,1959," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2692,safety,valid,validated,2692," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:858,security,log,log,858,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1115,security,log,log,1115,"sed in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1380,security,log,log,1380," statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1959,security,log,log,1959," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2178,security,inject,injecting,2178," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2692,security,validat,validated,2692," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:218,testability,simpl,simply,218,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:851,testability,simpl,simple,851,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:858,testability,log,log,858,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1115,testability,log,log,1115,"sed in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1380,testability,log,log,1380," statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1411,testability,assert,assert,1411,"e in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the ab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1959,testability,log,log,1959," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2510,testability,observ,observed,2510," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:218,usability,simpl,simply,218,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:469,usability,user,user-images,469,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:631,usability,user,user-images,631,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:794,usability,error,error,794,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:851,usability,simpl,simple,851,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:905,usability,user,user-images,905,"Here's some initial distribution plots for comparison:. Legend: . - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1269,usability,user,user-images,1269,"ge for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):. ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1554,usability,user,user-images,1554,"g). ### geometric mean (as used in Issac's notebook). ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2014,usability,user,user-images,2014," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2369,usability,prefer,preference,2369," simple log(n+1) (as used in RNAseq). ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF). ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry). ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`. - https://doi.org/10.1002/cyto.a.23017. - https://doi.org/10.1002/cyto.a.22030. - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper). ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:320,availability,state,stated,320,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:7,deployability,observ,observed,7,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:532,deployability,artifact,artifacts,532,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:729,deployability,depend,depending,729,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:892,deployability,artifact,artifacts,892,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2099,deployability,log,log,2099,"initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2125,deployability,log,logicle,2125,"t the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry abou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3356,deployability,build,building,3356,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3472,deployability,artifact,artifacts,3472,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:147,integrability,transform,transforms,147,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:320,integrability,state,stated,320,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:729,integrability,depend,depending,729,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:955,integrability,transform,transform-overview,955,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:977,integrability,transform,transform-digital,977,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1543,integrability,inject,injects,1543,"iscreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2537,integrability,transform,transforms,2537,"""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-case",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2863,integrability,sub,subset,2863,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3131,integrability,sub,subtle,3131,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3197,integrability,transform,transformation,3197,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3329,integrability,sub,subtle,3329,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3431,integrability,sub,subtleties,3431,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3498,integrability,transform,transforms,3498,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3703,integrability,sub,subtleties,3703,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:94,interoperability,distribut,distribution,94,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:147,interoperability,transform,transforms,147,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:955,interoperability,transform,transform-overview,955,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:977,interoperability,transform,transform-digital,977,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1955,interoperability,specif,specific,1955,"sform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2537,interoperability,transform,transforms,2537,"""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-case",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3197,interoperability,transform,transformation,3197,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3498,interoperability,transform,transforms,3498,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:729,modifiability,depend,depending,729,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:806,modifiability,exten,extends,806,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1941,performance,perform,performs,1941,"gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:656,reliability,doe,doesn,656,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2026,reliability,doe,doesn,2026,"one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:189,safety,valid,validated,189,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:729,safety,depend,depending,729,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1722,safety,valid,validation,1722,"r depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2099,safety,log,log,2099,"initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2125,safety,log,logicle,2125,"t the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry abou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2164,safety,test,tested,2164,"[image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and ther",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:189,security,validat,validated,189,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:408,security,sign,signal,408,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1543,security,inject,injects,1543,"iscreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1722,security,validat,validation,1722,"r depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1797,security,sign,signal,1797," that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); whi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2099,security,log,log,2099,"initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2125,security,log,logicle,2125,"t the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry abou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3398,security,ident,identity,3398,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3696,security,sign,signal,3696,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:7,testability,observ,observed,7,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:729,testability,depend,depending,729,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1850,testability,simpl,simply,1850,"cts are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2099,testability,log,log,2099,"initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2125,testability,log,logicle,2125,"t the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry abou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2164,testability,test,tested,2164,"[image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and ther",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1183,usability,user,user-images,1183,"validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1295,usability,user,user-images,1295,"c mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1407,usability,user,user-images,1407,"l, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:. - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian. - skewing of the ""absence"" of a marker depending on presence of another marker. - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1850,usability,simpl,simply,1850,"cts are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:1941,usability,perform,performs,1941,"gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2197,usability,clear,clear,2197,"busercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first tra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:2917,usability,mous,mouse,2917,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3028,usability,intuit,intuitively,3028,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3573,usability,help,helpful,3573,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:3638,usability,document,document,3638,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:377,availability,cluster,clustering,377,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:441,availability,cluster,clustering,441,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:377,deployability,cluster,clustering,377,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:441,deployability,cluster,clustering,441,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:360,energy efficiency,cool,cool,360,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:107,performance,content,content,107,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:325,performance,multiplex,multiplex,325,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:393,performance,multiplex,multiplex,393,Thoughts on Seurat's solution to multimodal datasets? [Weighted nearest neighbors](https://www.biorxiv.org/content/10.1101/2020.10.12.335331v1). I think it's a bit more sophisticated (though similar in spirit) than finding the union of the two graphs (as suggested in your tutorial). I really like the idea of using leiden's multiplex partition function. It's cool that leiden clustering on a multiplex graph gives really similar results to clustering on the joint graph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:80,availability,avail,available,80,@ivirshup Is the code for all the normalisations (including quantile rescaling) available somewhere?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:80,reliability,availab,available,80,@ivirshup Is the code for all the normalisations (including quantile rescaling) available somewhere?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:80,safety,avail,available,80,@ivirshup Is the code for all the normalisations (including quantile rescaling) available somewhere?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:80,security,availab,available,80,@ivirshup Is the code for all the normalisations (including quantile rescaling) available somewhere?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:85,testability,plan,plans,85,"I don't have an opinion on normalisation methods, but it would be great if there are plans to merge this PR to provide initial CITE-seq support?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:136,usability,support,support,136,"I don't have an opinion on normalisation methods, but it would be great if there are plans to merge this PR to provide initial CITE-seq support?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:90,deployability,log,log,90,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:170,deployability,scale,scale,170,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:170,energy efficiency,scale,scale,170,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:170,modifiability,scal,scale,170,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:170,performance,scale,scale,170,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:90,safety,log,log,90,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:90,security,log,log,90,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:90,testability,log,log,90,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:43,usability,help,help,43,Very interesting discussion. . Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:13,usability,support,support,13,Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:461,deployability,compos,composition,461,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:191,integrability,transform,transformation,191,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:249,integrability,transform,transform,249,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:360,integrability,transform,transform,360,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:191,interoperability,transform,transformation,191,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:249,interoperability,transform,transform,249,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:360,interoperability,transform,transform,360,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:461,modifiability,compos,composition,461,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:272,performance,perform,perform,272,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:15,usability,support,support,15,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:272,usability,perform,perform,272,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:15,usability,support,support,15,> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. Yes. We are now officially supporting muon and recommend the usage of muon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:192,usability,support,supporting,192,> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. Yes. We are now officially supporting muon and recommend the usage of muon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:169,usability,help,help,169,"Hi! Trying to follow the Muon tutorial, I get stuck on:. pt.pp.dsb(mdata, raw=mdata_raw, empty_droplets=droplets). NameError: name 'droplets' is not defined. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:203,reliability,doe,doesn,203,"> Hi! Trying to follow the Muon tutorial, I get stuck on: pt.pp.dsb(mdata, raw=mdata_raw, empty_droplets=droplets). > . > NameError: name 'droplets' is not defined. > . > Can anyone help? @livyring this doesn't belong here. Please ask usage questions here: https://discourse.scverse.org/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:182,usability,help,help,182,"> Hi! Trying to follow the Muon tutorial, I get stuck on: pt.pp.dsb(mdata, raw=mdata_raw, empty_droplets=droplets). > . > NameError: name 'droplets' is not defined. > . > Can anyone help? @livyring this doesn't belong here. Please ask usage questions here: https://discourse.scverse.org/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1117:26,usability,close,close,26,@ivirshup @gtca should we close this PR since we have muon now?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117
https://github.com/scverse/scanpy/pull/1118:29,safety,test,tests,29,"@ivirshup . Thanks, i'll add tests ofc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:29,testability,test,tests,29,"@ivirshup . Thanks, i'll add tests ofc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:30,deployability,fail,fails,30,Without fallback to .uns this fails on every test which uses `sc.datasets.` with neighbors precomputed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:30,reliability,fail,fails,30,Without fallback to .uns this fails on every test which uses `sc.datasets.` with neighbors precomputed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:45,safety,test,test,45,Without fallback to .uns this fails on every test which uses `sc.datasets.` with neighbors precomputed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:45,testability,test,test,45,Without fallback to .uns this fails on every test which uses `sc.datasets.` with neighbors precomputed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:17,usability,user,user-images,17,"![image](https://user-images.githubusercontent.com/1140359/79777753-b69c6780-8305-11ea-9393-b77243bdbe9f.png). shall we maybe change . `sparse matrix (.uns['neighbors'], dtype float32)`. to. `sparse matrix (.uns[key_added], dtype float32) where key_added is ""neighbors"" if not supplied.` . or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:28,usability,feedback,feedback,28,"@Koncopd sorry for the late feedback, but I don't see the ""neighbors_key"" in the scanpy.tl.paga function. It'd be great to make sure that everything that uses the neighbor graph is covered :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:51,safety,test,tested,51,"Also sc.tl.dpt and sc.tl.diffmap functions are not tested in tests, AFAICS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:61,safety,test,tests,61,"Also sc.tl.dpt and sc.tl.diffmap functions are not tested in tests, AFAICS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:51,testability,test,tested,51,"Also sc.tl.dpt and sc.tl.diffmap functions are not tested in tests, AFAICS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/pull/1118:61,testability,test,tests,61,"Also sc.tl.dpt and sc.tl.diffmap functions are not tested in tests, AFAICS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118
https://github.com/scverse/scanpy/issues/1120:2,testability,simpl,simple,2,"A simple way to do it is by creating a new `.obs` categorical column as follows:. ```python. adata = sc.datasets.pbmc68k_reduced(). # create new categorical column called `selection`. adata.obs['selection'] = pd.Categorical((adata.obs_vector('CD3G') > 2) & (adata.obs_vector('CD4') < 3)). # adjust colors. adata.uns['selection_colors'] = ['blue', 'yellow']. sc.pl.umap(adata, color='selection', add_outline=True, s=20). ```. ![image](https://user-images.githubusercontent.com/4964309/77539317-75996a80-6ea1-11ea-8762-bb29b00d8e43.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1120
https://github.com/scverse/scanpy/issues/1120:2,usability,simpl,simple,2,"A simple way to do it is by creating a new `.obs` categorical column as follows:. ```python. adata = sc.datasets.pbmc68k_reduced(). # create new categorical column called `selection`. adata.obs['selection'] = pd.Categorical((adata.obs_vector('CD3G') > 2) & (adata.obs_vector('CD4') < 3)). # adjust colors. adata.uns['selection_colors'] = ['blue', 'yellow']. sc.pl.umap(adata, color='selection', add_outline=True, s=20). ```. ![image](https://user-images.githubusercontent.com/4964309/77539317-75996a80-6ea1-11ea-8762-bb29b00d8e43.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1120
https://github.com/scverse/scanpy/issues/1120:442,usability,user,user-images,442,"A simple way to do it is by creating a new `.obs` categorical column as follows:. ```python. adata = sc.datasets.pbmc68k_reduced(). # create new categorical column called `selection`. adata.obs['selection'] = pd.Categorical((adata.obs_vector('CD3G') > 2) & (adata.obs_vector('CD4') < 3)). # adjust colors. adata.uns['selection_colors'] = ['blue', 'yellow']. sc.pl.umap(adata, color='selection', add_outline=True, s=20). ```. ![image](https://user-images.githubusercontent.com/4964309/77539317-75996a80-6ea1-11ea-8762-bb29b00d8e43.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1120
https://github.com/scverse/scanpy/issues/1121:218,availability,down,downgrading,218,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:32,deployability,depend,dependency,32,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:47,deployability,updat,update,47,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:89,deployability,releas,releases,89,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:194,deployability,version,versions,194,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:32,integrability,depend,dependency,32,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:194,integrability,version,versions,194,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:169,interoperability,compatib,compatibility,169,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:32,modifiability,depend,dependency,32,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:194,modifiability,version,versions,194,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:32,safety,depend,dependency,32,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:47,safety,updat,update,47,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:47,security,updat,update,47,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1121:32,testability,depend,dependency,32,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121
https://github.com/scverse/scanpy/issues/1122:87,security,auth,authors,87,"BBKNN is an external tool, and feature requests for it should be directed to the bbknn authors: https://github.com/Teichlab/bbknn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1122
https://github.com/scverse/scanpy/issues/1122:21,usability,tool,tool,21,"BBKNN is an external tool, and feature requests for it should be directed to the bbknn authors: https://github.com/Teichlab/bbknn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1122
https://github.com/scverse/scanpy/issues/1122:221,usability,user,user-images,221,"Hi,. I tried ingest using the reference made with BBKNN. As @ivirshup said, ingest was worked by adding `adata_ref.uns['neighbors']['params']['metric'] = 'euclidean'`. However, the result was quite poor. ![image](https://user-images.githubusercontent.com/19543497/117822090-2aaa0480-b2a7-11eb-80ea-a31240861a4b.png). In contrast, if I merge all datasets (eg references and query), it worked well, but when we want to take over the reference embedding, I actually want to use ingest rather than run bbknn again. Is there any option to feed in this case? or should I ask this in the BBKNN repo? ![image](https://user-images.githubusercontent.com/19543497/117822235-562cef00-b2a7-11eb-875f-262452baff0d.png). This is the notebook can reproduce the problem. https://nbviewer.jupyter.org/github/yyoshiaki/ingest_after_bbknn/blob/main/notebook.ipynb. https://github.com/yyoshiaki/ingest_after_bbknn/blob/main/notebook.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1122
https://github.com/scverse/scanpy/issues/1122:610,usability,user,user-images,610,"Hi,. I tried ingest using the reference made with BBKNN. As @ivirshup said, ingest was worked by adding `adata_ref.uns['neighbors']['params']['metric'] = 'euclidean'`. However, the result was quite poor. ![image](https://user-images.githubusercontent.com/19543497/117822090-2aaa0480-b2a7-11eb-80ea-a31240861a4b.png). In contrast, if I merge all datasets (eg references and query), it worked well, but when we want to take over the reference embedding, I actually want to use ingest rather than run bbknn again. Is there any option to feed in this case? or should I ask this in the BBKNN repo? ![image](https://user-images.githubusercontent.com/19543497/117822235-562cef00-b2a7-11eb-875f-262452baff0d.png). This is the notebook can reproduce the problem. https://nbviewer.jupyter.org/github/yyoshiaki/ingest_after_bbknn/blob/main/notebook.ipynb. https://github.com/yyoshiaki/ingest_after_bbknn/blob/main/notebook.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1122
https://github.com/scverse/scanpy/pull/1123:43,usability,feedback,feedback,43,"Hey @ivirshup , could you please give me a feedback on this if there should be any improvements?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:420,availability,slo,slow,420,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:420,reliability,slo,slow,420,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:191,safety,test,test,191,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:304,safety,test,tests,304,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:180,testability,regress,regression,180,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:191,testability,test,test,191,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:304,testability,test,tests,304,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774). * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:621,deployability,fail,fail,621,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:789,deployability,contain,contain,789,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:127,energy efficiency,green,green,127,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:855,interoperability,specif,specify,855,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:188,performance,time,time,188,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:621,reliability,fail,fail,621,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:49,safety,test,test,49,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:871,safety,test,test,871,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:38,testability,regress,regression,38,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:49,testability,test,test,49,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:871,testability,test,test,871,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:15,usability,feedback,feedback,15,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:820,usability,user,user,820,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact). As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:. ```. for node in nodes:. for pie_fraction in fractions[node]:. ... ```. I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example. ```. foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}. foo[0] = {'black': 0.5}. ```. the the nodes don't contain the same colors, which user could (although not sure why) specify. I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:646,deployability,fail,failed,646,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:264,energy efficiency,Current,Currently,264,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:107,interoperability,specif,specify,107,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:691,interoperability,specif,specifically,691,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:448,modifiability,paramet,parameters,448,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:646,reliability,fail,failed,646,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:375,safety,test,test,375,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:622,safety,test,test,622,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:364,testability,regress,regression,364,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:375,testability,test,test,375,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:622,testability,test,test,622,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:4,performance,time,time,4,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:176,performance,time,time,176,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:63,testability,simpl,simple,63,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:63,usability,simpl,simple,63,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:106,energy efficiency,current,currently,106,"Hi all, is there any progress on this? @michalk8 and myself rely on these pie charts for a package we are currently developing and it would be really important to have a fix for this in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:91,modifiability,pac,package,91,"Hi all, is there any progress on this? @michalk8 and myself rely on these pie charts for a package we are currently developing and it would be really important to have a fix for this in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:21,usability,progress,progress,21,"Hi all, is there any progress on this? @michalk8 and myself rely on these pie charts for a package we are currently developing and it would be really important to have a fix for this in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:398,modifiability,paramet,parameter,398,"Hi. This looks good to me. I had used the same trick before. . I tried to search how to trigger the 'pie' option of paga but I could not find it in the documentation. Do you mind adding some documentation and an example maybe using `sc.datasets.pbmc68k_reduced()`. As a suggestion, I think that the plots may benefit from adding and outline around the legend as is done for the embeddings. See the parameter legend_fontoutline) https://github.com/theislab/scanpy/blob/668b67765534643bc4257357ef639b2087df0716/scanpy/plotting/_tools/scatterplots.py#L435",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:152,usability,document,documentation,152,"Hi. This looks good to me. I had used the same trick before. . I tried to search how to trigger the 'pie' option of paga but I could not find it in the documentation. Do you mind adding some documentation and an example maybe using `sc.datasets.pbmc68k_reduced()`. As a suggestion, I think that the plots may benefit from adding and outline around the legend as is done for the embeddings. See the parameter legend_fontoutline) https://github.com/theislab/scanpy/blob/668b67765534643bc4257357ef639b2087df0716/scanpy/plotting/_tools/scatterplots.py#L435",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:191,usability,document,documentation,191,"Hi. This looks good to me. I had used the same trick before. . I tried to search how to trigger the 'pie' option of paga but I could not find it in the documentation. Do you mind adding some documentation and an example maybe using `sc.datasets.pbmc68k_reduced()`. As a suggestion, I think that the plots may benefit from adding and outline around the legend as is done for the embeddings. See the parameter legend_fontoutline) https://github.com/theislab/scanpy/blob/668b67765534643bc4257357ef639b2087df0716/scanpy/plotting/_tools/scatterplots.py#L435",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:199,availability,robust,robust,199,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:238,deployability,version,version,238,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:238,integrability,version,version,238,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:238,modifiability,version,version,238,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:48,reliability,doe,doesn,48,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:199,reliability,robust,robust,199,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:199,safety,robust,robust,199,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:231,security,triag,triage,231,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this). Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/pull/1123:20,reliability,doe,does,20,"Thanks! @michalk8 , does this still matter to us, now that we use scvelo's implementation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123
https://github.com/scverse/scanpy/issues/1125:27,availability,state,state,27,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:4,deployability,depend,dependencies,4,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:72,deployability,updat,update,72,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:233,deployability,instal,installed,233,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:259,deployability,version,version,259,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:305,deployability,instal,installed,305,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:4,integrability,depend,dependencies,4,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:27,integrability,state,state,27,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:259,integrability,version,version,259,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:4,modifiability,depend,dependencies,4,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:259,modifiability,version,version,259,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:4,safety,depend,dependencies,4,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:72,safety,updat,update,72,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:72,security,updat,update,72,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/issues/1125:4,testability,depend,dependencies,4,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125
https://github.com/scverse/scanpy/pull/1127:162,availability,cluster,cluster,162,"I really like the plots above, and really cool that you solved the issue with the legends. Would it also be possible to add some kind of grouping brackets on the cluster level in the same way you have for the gene level? I have cases where I would like to say that club, goblet, basal, and ciliated cells are all airway epithelial cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:162,deployability,cluster,cluster,162,"I really like the plots above, and really cool that you solved the issue with the legends. Would it also be possible to add some kind of grouping brackets on the cluster level in the same way you have for the gene level? I have cases where I would like to say that club, goblet, basal, and ciliated cells are all airway epithelial cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:42,energy efficiency,cool,cool,42,"I really like the plots above, and really cool that you solved the issue with the legends. Would it also be possible to add some kind of grouping brackets on the cluster level in the same way you have for the gene level? I have cases where I would like to say that club, goblet, basal, and ciliated cells are all airway epithelial cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:27,modifiability,exten,extend,27,@LuckyMD should be easy to extend the `Plot` class to add this. I will think about it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:171,deployability,API,APIs,171,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:257,deployability,API,APIs,257,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:544,deployability,API,API,544,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:20,energy efficiency,cool,cool,20,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:171,integrability,API,APIs,171,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:257,integrability,API,APIs,257,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:544,integrability,API,API,544,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:145,interoperability,specif,specific,145,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:171,interoperability,API,APIs,171,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:257,interoperability,API,APIs,257,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:522,interoperability,specif,specific,522,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:544,interoperability,API,API,544,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:366,modifiability,exten,extending,366,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:469,modifiability,exten,extended,469,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:498,modifiability,maintain,maintain,498,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:316,safety,review,review,316,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:498,safety,maintain,maintain,498,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:316,testability,review,review,316,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:35,usability,undo,undoubtedly,35,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:339,deployability,API,APIs,339,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,deployability,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:523,energy efficiency,current,current,523,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:1493,energy efficiency,current,current,1493,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:339,integrability,API,APIs,339,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,integrability,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:339,interoperability,API,APIs,339,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:461,interoperability,specif,specific,461,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,interoperability,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:1566,interoperability,specif,specific,1566,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:213,modifiability,reu,reused,213,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:220,modifiability,exten,extended,220,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:287,modifiability,extens,extensively,287,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,modifiability,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,reliability,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,security,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:543,testability,integr,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:605,testability,context,context,605,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:353,usability,close,closely,353,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:1405,usability,tool,tooltip,1405,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:1469,usability,interact,interactive,1469,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**. ```PYTHON. import plotly.graph_objects as go. fig = go.Figure(. data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],. layout=go.Layout(. title=go.layout.Title(text=""A Bar Chart""). ). ). fig.show(). ```. **Plotnine:**. ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap. from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')). + geom_point(). + stat_smooth(method='lm'). + facet_wrap('~gear')). ```. **Altair:**. ```PYTHON. import altair as alt. from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(. x='Horsepower',. y='Miles_per_Gallon',. color='Origin',. tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']. ).interactive(). ```. The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:206,deployability,updat,update,206,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:149,energy efficiency,current,current,149,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:719,energy efficiency,Current,Currently,719,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:943,integrability,sub,subdivided,943,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:423,reliability,doe,doesn,423,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:206,safety,updat,update,206,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:356,safety,test,tests,356,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:487,safety,test,tests,487,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:679,safety,review,reviewing,679,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:206,security,updat,update,206,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:1088,security,polic,policies,1088,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:356,testability,test,tests,356,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:436,testability,coverag,coverage,436,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:487,testability,test,tests,487,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:679,testability,review,reviewing,679,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:277,usability,visual,visualizing-marker-genes,277,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:914,usability,visual,visualize,914,"@ivirshup . > * Could you show some examples of the new additions/ let me know where you are on tutorials? I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: . * embedding scatter plots which are separated already. * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this? > * Could you run `black` over this? Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:133,deployability,Stack,StackedViolin,133,"I just added the class `ViolinPlot` to the group plots. Now is possible to do this:. ```PYTHON. from scanpy.plotting._anndata import StackedViolin, DotPlot, MatrixPlot. adata = sc.datasets.pbmc68k_reduced(). markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}. fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(19,5), gridspec_kw={'width_ratios': [.8, 1, 1]}). StackedViolin(adata, markers, groupby='bulk_labels', ax=ax1).add_dendrogram(True).swap_axes(True).show(False). DotPlot(adata, markers, groupby='bulk_labels', ax=ax2).add_dendrogram(True).swap_axes(True).show(False). _ = MatrixPlot(adata, markers, groupby='bulk_labels', ax=ax3).add_dendrogram(True).swap_axes(True).show(False). ```. ![image](https://user-images.githubusercontent.com/4964309/78922159-5e0fd380-7a96-11ea-8afa-ecdba39adc6b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:424,deployability,Stack,StackedViolin,424,"I just added the class `ViolinPlot` to the group plots. Now is possible to do this:. ```PYTHON. from scanpy.plotting._anndata import StackedViolin, DotPlot, MatrixPlot. adata = sc.datasets.pbmc68k_reduced(). markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}. fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(19,5), gridspec_kw={'width_ratios': [.8, 1, 1]}). StackedViolin(adata, markers, groupby='bulk_labels', ax=ax1).add_dendrogram(True).swap_axes(True).show(False). DotPlot(adata, markers, groupby='bulk_labels', ax=ax2).add_dendrogram(True).swap_axes(True).show(False). _ = MatrixPlot(adata, markers, groupby='bulk_labels', ax=ax3).add_dendrogram(True).swap_axes(True).show(False). ```. ![image](https://user-images.githubusercontent.com/4964309/78922159-5e0fd380-7a96-11ea-8afa-ecdba39adc6b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:350,integrability,sub,subplots,350,"I just added the class `ViolinPlot` to the group plots. Now is possible to do this:. ```PYTHON. from scanpy.plotting._anndata import StackedViolin, DotPlot, MatrixPlot. adata = sc.datasets.pbmc68k_reduced(). markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}. fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(19,5), gridspec_kw={'width_ratios': [.8, 1, 1]}). StackedViolin(adata, markers, groupby='bulk_labels', ax=ax1).add_dendrogram(True).swap_axes(True).show(False). DotPlot(adata, markers, groupby='bulk_labels', ax=ax2).add_dendrogram(True).swap_axes(True).show(False). _ = MatrixPlot(adata, markers, groupby='bulk_labels', ax=ax3).add_dendrogram(True).swap_axes(True).show(False). ```. ![image](https://user-images.githubusercontent.com/4964309/78922159-5e0fd380-7a96-11ea-8afa-ecdba39adc6b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/pull/1127:774,usability,user,user-images,774,"I just added the class `ViolinPlot` to the group plots. Now is possible to do this:. ```PYTHON. from scanpy.plotting._anndata import StackedViolin, DotPlot, MatrixPlot. adata = sc.datasets.pbmc68k_reduced(). markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}. fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(19,5), gridspec_kw={'width_ratios': [.8, 1, 1]}). StackedViolin(adata, markers, groupby='bulk_labels', ax=ax1).add_dendrogram(True).swap_axes(True).show(False). DotPlot(adata, markers, groupby='bulk_labels', ax=ax2).add_dendrogram(True).swap_axes(True).show(False). _ = MatrixPlot(adata, markers, groupby='bulk_labels', ax=ax3).add_dendrogram(True).swap_axes(True).show(False). ```. ![image](https://user-images.githubusercontent.com/4964309/78922159-5e0fd380-7a96-11ea-8afa-ecdba39adc6b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127
https://github.com/scverse/scanpy/issues/1128:249,modifiability,variab,variables,249,"Hi, @andrea-tango . About the second issue - you dont need pca. you can do something like. ```. # project reference adata to latent dimensions with your autoencoder. adata_ref.obsm['X_latent'] = autoencoder.to_latent(adata_ref.X). # use your latent variables to calculate neighbors. sc.pp.neighbors(adata_ref, use_rep='X_latent'). sc.tl.umap(adata_ref). # project your new adata to latent dimensions with your autoencoder. adata_new.obsm['X_latent'] = autoencoder.to_latent(adata_new.X). sc.tl.ingest(adata_new, adata_ref, embedding_method='umap'). ```. About the first, yes, ingest needs vars in the same order. The ordering thing you describe is definitely not the issue with ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128
https://github.com/scverse/scanpy/issues/1128:102,integrability,filter,filteringGenesCells,102,"Hi @Koncopd,. Thank you for the suggestions. Regarding the vars, I wrote the attached function. > def filteringGenesCells(adata, genes=None, cells=None, sortGenes=False, sortCells=False):. > . > df = pd.DataFrame(index = adata.obs.index.tolist(),. > columns = adata.var.index.tolist(),. > data = adata.X). > . > if genes is not None:. > df = df[genes]. > . > if sortGenes:. > df1 = df.T. > df1.sort_index(inplace=True). > df = df1.T. > . > if cells is not None:. > df1 = df.T. > df1 = df1[cells]. > df = df1.T. > . > if sortCells:. > df.sort_index(inplace=True). > . > adata = adata[:, adata.var.index.isin(df.columns)]. > adata = adata[adata.obs.index.isin(df.index)]. > . > if sortGenes:. > adata.var.sort_index(inplace=True). > . > if sortCells:. > adata.obs.sort_index(inplace=True). > . > adata.X = df.values. > . > return adata. Best,. Andrea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128
https://github.com/scverse/scanpy/issues/1128:269,reliability,doe,doesn,269,"Andrea, I think you'll want something more like:. ```python. shared_genes = adata1.var_names.intersection(adata2.var_names). adata1 = adata1[:, shared_genes].copy(). adata2 = adata2[:, shared_genes].copy(). ```. Your code only sorts the index of the var dataframe, but doesn't actually reorder the anndata object. It's definitely a little confusing that this is possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128
https://github.com/scverse/scanpy/pull/1130:13,safety,review,review,13,Requesting a review from @flying-sheep since it looks like he had a hand in implementing this for `tqdm`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130
https://github.com/scverse/scanpy/pull/1130:13,testability,review,review,13,Requesting a review from @flying-sheep since it looks like he had a hand in implementing this for `tqdm`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130
https://github.com/scverse/scanpy/pull/1130:259,deployability,instal,installed,259,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130
https://github.com/scverse/scanpy/pull/1130:244,modifiability,extens,extensions,244,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130
https://github.com/scverse/scanpy/pull/1130:197,usability,progress,progress,197,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130
https://github.com/scverse/scanpy/pull/1132:22,modifiability,paramet,parameter,22,And what about metric parameter?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:88,usability,tool,tools,88,https://github.com/theislab/scanpy/blob/2a16436e9ac3b3ba039a99b9a3674885fe6a4590/scanpy/tools/_umap.py#L171. Is it conceptually right to pass custom connectivities with `metric='euclidean'` by default?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:142,usability,custom,custom,142,https://github.com/theislab/scanpy/blob/2a16436e9ac3b3ba039a99b9a3674885fe6a4590/scanpy/tools/_umap.py#L171. Is it conceptually right to pass custom connectivities with `metric='euclidean'` by default?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:58,integrability,compon,components,58,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:173,integrability,compon,components,173,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:58,interoperability,compon,components,58,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:173,interoperability,compon,components,173,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:58,modifiability,compon,components,58,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:173,modifiability,compon,components,173,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:256,reliability,doe,does,256,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/pull/1132:272,security,hack,hack-y,272,"The metric is only used in the case of multiple connected components, which is also the case where `data` is used. It's for calculating distances between the mediods of the components. I'm not sure there is a great default for either of these values. This does feel kinda hack-y to me. I'm not really sure what the right thing to do in this case is, and maybe we should not be using UMAPs layout code with graphs that UMAP didn't generate. Unfortunately, it's just much faster than other graph layout algorithms I've tried, and provides good looking results. @falexwolf, any thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132
https://github.com/scverse/scanpy/issues/1133:175,availability,avail,available,175,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1133:122,deployability,updat,updates,122,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1133:175,reliability,availab,available,175,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1133:122,safety,updat,updates,122,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1133:175,safety,avail,available,175,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1133:122,security,updat,updates,122,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1133:175,security,availab,available,175,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133
https://github.com/scverse/scanpy/issues/1134:337,integrability,sub,subplots,337,"This is already done with the `{colname}_colors` entries in `uns`. For some reason, this isn't working when `ax` is passed and `show=False` (@fidelram @flying-sheep). Here's a reproducer using the built-in data:. ```python. import scanpy as sc. from matplotlib import pyplot as plt. pbmc = sc.datasets.pbmc3k_processed(). fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). # Trying to set attribute `.uns` of view, copying. sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). plt.show(). ```. <img width=""922"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877321-2d16ee00-72a1-11ea-99e7-1d7eb75d036c.png"">. Interestingly, if you plot the full object before the view this works fine:. ```python. fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). plt.show(). ```. <img width=""912"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877417-736c4d00-72a1-11ea-9aad-1afb98089c25.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1134
https://github.com/scverse/scanpy/issues/1134:864,integrability,sub,subplots,864,"This is already done with the `{colname}_colors` entries in `uns`. For some reason, this isn't working when `ax` is passed and `show=False` (@fidelram @flying-sheep). Here's a reproducer using the built-in data:. ```python. import scanpy as sc. from matplotlib import pyplot as plt. pbmc = sc.datasets.pbmc3k_processed(). fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). # Trying to set attribute `.uns` of view, copying. sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). plt.show(). ```. <img width=""922"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877321-2d16ee00-72a1-11ea-99e7-1d7eb75d036c.png"">. Interestingly, if you plot the full object before the view this works fine:. ```python. fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). plt.show(). ```. <img width=""912"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877417-736c4d00-72a1-11ea-9aad-1afb98089c25.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1134
https://github.com/scverse/scanpy/issues/1134:666,usability,user,user-images,666,"This is already done with the `{colname}_colors` entries in `uns`. For some reason, this isn't working when `ax` is passed and `show=False` (@fidelram @flying-sheep). Here's a reproducer using the built-in data:. ```python. import scanpy as sc. from matplotlib import pyplot as plt. pbmc = sc.datasets.pbmc3k_processed(). fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). # Trying to set attribute `.uns` of view, copying. sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). plt.show(). ```. <img width=""922"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877321-2d16ee00-72a1-11ea-99e7-1d7eb75d036c.png"">. Interestingly, if you plot the full object before the view this works fine:. ```python. fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). plt.show(). ```. <img width=""912"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877417-736c4d00-72a1-11ea-9aad-1afb98089c25.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1134
https://github.com/scverse/scanpy/issues/1134:1142,usability,user,user-images,1142,"This is already done with the `{colname}_colors` entries in `uns`. For some reason, this isn't working when `ax` is passed and `show=False` (@fidelram @flying-sheep). Here's a reproducer using the built-in data:. ```python. import scanpy as sc. from matplotlib import pyplot as plt. pbmc = sc.datasets.pbmc3k_processed(). fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). # Trying to set attribute `.uns` of view, copying. sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). plt.show(). ```. <img width=""922"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877321-2d16ee00-72a1-11ea-99e7-1d7eb75d036c.png"">. Interestingly, if you plot the full object before the view this works fine:. ```python. fig, axs = plt.subplots(1,2, figsize=(8,5)) . sc.pl.umap(pbmc, color=""louvain"", show=False, ax=axs[1]). sc.pl.umap(. pbmc[pbmc.obs[""louvain""].isin(['Dendritic cells', 'Megakaryocytes'])],. color=""louvain"",. show=False,. ax=axs[0]. ). plt.show(). ```. <img width=""912"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/77877417-736c4d00-72a1-11ea-9aad-1afb98089c25.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1134
https://github.com/scverse/scanpy/pull/1135:322,performance,time,time,322,"I think that literally nobody is using this function for arrays and sparse matrices, everyone only uses it for AnnDatas. It's complicated to reintroduce the functionality and there is no way the purpose of this PR can be accomplished. Hence, I'd really just merge it. Quite a few things will break towards 1.5. Now is the time. I'd make the same change for all other preprocessing functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:126,safety,compl,complicated,126,"I think that literally nobody is using this function for arrays and sparse matrices, everyone only uses it for AnnDatas. It's complicated to reintroduce the functionality and there is no way the purpose of this PR can be accomplished. Hence, I'd really just merge it. Quite a few things will break towards 1.5. Now is the time. I'd make the same change for all other preprocessing functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:126,security,compl,complicated,126,"I think that literally nobody is using this function for arrays and sparse matrices, everyone only uses it for AnnDatas. It's complicated to reintroduce the functionality and there is no way the purpose of this PR can be accomplished. Hence, I'd really just merge it. Quite a few things will break towards 1.5. Now is the time. I'd make the same change for all other preprocessing functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1080,availability,operat,operation,1080,". I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:750,deployability,version,version,750,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:812,deployability,scale,scale,812,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:862,deployability,scale,scale,862,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:896,deployability,Scale,Scale,896,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1028,deployability,observ,observations,1028," using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1561,deployability,Depend,Depending,1561,"e same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1592,deployability,updat,updates,1592,"andling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1615,deployability,scale,scaled,1615,"using. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1732,deployability,scale,scale,1732," my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1965,deployability,log,logg,1965," that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=Fa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2126,deployability,log,logg,2126,"NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'den",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2397,deployability,scale,scale,2397," to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2683,deployability,scale,scale,2683,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2871,deployability,scale,scale,2871,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3036,deployability,log,logic,3036,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3065,deployability,log,logg,3065,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3418,deployability,log,logic,3418,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:812,energy efficiency,scale,scale,812,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:862,energy efficiency,scale,scale,862,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:896,energy efficiency,Scale,Scale,896,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1615,energy efficiency,scale,scaled,1615,"using. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1732,energy efficiency,scale,scale,1732," my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2397,energy efficiency,scale,scale,2397," to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2683,energy efficiency,scale,scale,2683,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2871,energy efficiency,scale,scale,2871,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:750,integrability,version,version,750,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1561,integrability,Depend,Depending,1561,"e same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3454,integrability,wrap,wrappers,3454,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:221,interoperability,convers,conversation,221,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:418,interoperability,format,formats,418,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2018,interoperability,specif,specific,2018,". all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3454,interoperability,wrapper,wrappers,3454,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:750,modifiability,version,version,750,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:812,modifiability,scal,scale,812,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:862,modifiability,scal,scale,862,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:896,modifiability,Scal,Scale,896,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:950,modifiability,Variab,Variables,950,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1134,modifiability,Paramet,Parameters,1134,"ttps://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1313,modifiability,variab,variables,1313,"as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1421,modifiability,scal,scaling,1421," of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1561,modifiability,Depend,Depending,1561,"e same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1615,modifiability,scal,scaled,1615,"using. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1732,modifiability,scal,scale,1732," my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2397,modifiability,scal,scale,2397," to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2683,modifiability,scal,scale,2683,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2871,modifiability,scal,scale,2871,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3629,modifiability,pac,packages,3629,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:812,performance,scale,scale,812,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:862,performance,scale,scale,862,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:896,performance,Scale,Scale,896,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1615,performance,scale,scaled,1615,"using. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1732,performance,scale,scale,1732," my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2397,performance,scale,scale,2397," to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2683,performance,scale,scale,2683,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2871,performance,scale,scale,2871,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3157,performance,memor,memory,3157,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:695,safety,test,tested,695,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1355,safety,input,input,1355,"days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1561,safety,Depend,Depending,1561,"e same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1592,safety,updat,updates,1592,"andling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1965,safety,log,logg,1965," that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=Fa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2126,safety,log,logg,2126,"NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'den",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3036,safety,log,logic,3036,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3065,safety,log,logg,3065,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3112,safety,input,input,3112,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3418,safety,log,logic,3418,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1592,security,updat,updates,1592,"andling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1965,security,log,logg,1965," that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=Fa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2126,security,log,logg,2126,"NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'den",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3036,security,log,logic,3036,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3065,security,log,logg,3065,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3418,security,log,logic,3418,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:695,testability,test,tested,695,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:910,testability,unit,unit,910,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1028,testability,observ,observations,1028," using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1561,testability,Depend,Depending,1561,"e same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1965,testability,log,logg,1965," that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=Fa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:2126,testability,log,logg,2126,"NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'den",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3036,testability,log,logic,3036,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3065,testability,log,logg,3065,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3418,testability,log,logic,3418,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:478,usability,support,support,478,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? Th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1355,usability,input,input,1355,"days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:1361,usability,efficien,efficiently,1361,"canpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>. <summary> Alternative implementation of scale </summary>. ```python. @singledispatch. def scale(X, *args, **kwargs):. """"""\. Scale data to unit variance and zero mean. .. note::. Variables (genes) that do not display any variation (are constant across. all observations) are retained and set to 0 during this operation. In. the future, they might be set to NaNs. Parameters. ----------. data. The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond to cells and columns to genes. zero_center. If `False`, omit zero-centering variables, which allows to handle sparse. input efficiently. max_value. Clip (truncate) to this value after scaling. If `None`, do not clip. copy. If an :class:`~anndata.AnnData` is passed,. determines whether a copy is returned. Returns. -------. Depending on `copy` returns or updates `adata` with a scaled `adata.X`,. annotated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3112,usability,input,input,3112,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:3157,usability,memor,memory,3157,"tated with `'mean'` and `'std'` in `adata.var`. """""". return scale_array(X, *args, **kwargs). @scale.register(np.ndarray). def scale_array(. X,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. return_mean_var=False,. ):. if copy:. X = X.copy(). if not zero_center and max_value is not None:. logg.info( # Be careful of what? This should be more specific. '... be careful when using `max_value` '. 'without `zero_center`.'. ). if max_value is not None:. logg.debug(f'... clipping at max_value {max_value}'). mean, std = _scale(X, zero_center) # the code from here could probably just be . # do the clipping. if max_value is not None:. X[X > max_value] = max_value. if return_mean_var:. return X, mean, var. else:. return X. @scale.register(AnnData). def scale_anndata(. adata: AnnData,. *,. zero_center: bool = True,. max_value: Optional[float] = None,. copy: bool = False,. ) -> Optional[AnnData]:. adata = adata.copy() if copy else adata. view_to_actual(adata). adata.X, adata.var[""mean""], adata.var[""std""] = scale(. X, . zero_center=zero_center, . max_value=max_value, . copy=False, # because a copy has already been made, if it were to be made. return_mean_var=True. ). if copy:. return adata. @scale.register(sparse.spmatrix). def scale_sparse(. X, . *, . zero_center: bool = True,. copy=False,. **kwargs. ):. # need to add the following here to make inplace logic work. if zero_center:. logg.info(. '... as `zero_center=True`, sparse input is '. 'densified and may lead to large memory consumption'. ). X = X.toarray(). copy = False # Since the data has been copied. return scale_array(X, zero_center=zero_center, copy=copy, **kwargs). ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:56,deployability,scale,scale,56,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:56,energy efficiency,scale,scale,56,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:430,integrability,wrap,wrap,430,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:56,modifiability,scal,scale,56,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:56,performance,scale,scale,56,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:508,reliability,doe,doesn,508,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:28,usability,prototyp,prototype,28,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:252,usability,workflow,workflow,252,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:. ```. adata = scanpy.AnnData(data). dpt = scanpy.tl.dpt(adata). del adata. ```. and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:125,safety,compl,complex,125,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:155,safety,input,input,155,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:125,security,compl,complex,125,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:155,usability,input,input,155,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:240,usability,prototyp,prototype,240,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:291,usability,support,supporting,291,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/pull/1135:340,usability,document,document,340,"Closing as this is was superceded by #1173. @scottgigante I would be open to that, especially for functions which don't have complex requirements for data input. I'm not sure exactly what this would look like. I'd be interested in seeing a prototype – and PRs 😉. I think the main issue with supporting this is figuring out the right way to document it. In general, this fits a pattern of multiple dispatch. However, I'm not sure how to make that play nice with sphinx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135
https://github.com/scverse/scanpy/issues/1136:3,reliability,doe,doesn,3,It doesn't look like there's any information here. Could you add that?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136
https://github.com/scverse/scanpy/issues/1136:116,deployability,updat,updates,116,I'm going to close this since no more information was provided. I'd be happy to re-open this if @ yuxiaokang-source updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136
https://github.com/scverse/scanpy/issues/1136:116,safety,updat,updates,116,I'm going to close this since no more information was provided. I'd be happy to re-open this if @ yuxiaokang-source updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136
https://github.com/scverse/scanpy/issues/1136:116,security,updat,updates,116,I'm going to close this since no more information was provided. I'd be happy to re-open this if @ yuxiaokang-source updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136
https://github.com/scverse/scanpy/issues/1136:13,usability,close,close,13,I'm going to close this since no more information was provided. I'd be happy to re-open this if @ yuxiaokang-source updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136
https://github.com/scverse/scanpy/issues/1139:42,deployability,version,version,42,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:65,deployability,updat,updating,65,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:88,deployability,fail,fails,88,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:112,deployability,version,version,112,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:140,deployability,log,logging,140,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:42,integrability,version,version,42,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:112,integrability,version,version,112,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:42,modifiability,version,version,42,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:112,modifiability,version,version,112,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:88,reliability,fail,fails,88,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:65,safety,updat,updating,65,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:140,safety,log,logging,140,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:65,security,updat,updating,65,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:140,security,log,logging,140,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:140,testability,log,logging,140,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:6,deployability,upgrad,upgrading,6,"After upgrading the package, the problem resolved. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:6,modifiability,upgrad,upgrading,6,"After upgrading the package, the problem resolved. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1139:20,modifiability,pac,package,20,"After upgrading the package, the problem resolved. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139
https://github.com/scverse/scanpy/issues/1141:96,availability,consist,consistent,96,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:393,availability,cluster,cluster,393,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:533,availability,cluster,cluster,533,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:393,deployability,cluster,cluster,393,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:533,deployability,cluster,cluster,533,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:113,integrability,sub,subsetting,113,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:341,integrability,sub,subsetting,341,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:454,integrability,sub,subsetted,454,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:804,modifiability,paramet,parameter,804,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:96,usability,consist,consistent,96,"Hi @sfortma2 ,. There are two things going on here as far as I can tell. 1. The colours are not consistent after subsetting, and. 2. The x- and y-axes are ""distorted"" as you put it. For point 1, I recently saw a PR for this... so I assume it has been fixed in master, or is being fixed at the moment. For point 2, this is a normal result of subsetting the data. The data points in your second cluster span a similar x-axis and y-axis range. Thus, as the subsetted object is displayed in a square plot it looks similar. In your first cluster, the y-axis range the data points span is greater than the x-axis range, so the plot looks pulled apart in the x-axis. You can try to counteract this by changing the figure size to be higher than it is wide. I believe you should be able to pass a `figsize=(x,y)` parameter to the function. Otherwise you can work with `rcParams['figure.figsize'] = (x,y)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1141:281,usability,user,user-images,281,"Thanks for the explanation and solution Malte! Altering the rcParams setting works well and solves the problem (see below). Much appreciated. `rcParams['figure.figsize'] = (4,6)`. `sc.pl.umap(ec, color='louvain_r0.8_sub1', palette=sc.pl.palettes.vega_20_scanpy)`. ![image](https://user-images.githubusercontent.com/56206488/78280867-62a91a80-74df-11ea-8e0e-314797f78152.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1141
https://github.com/scverse/scanpy/issues/1142:34,deployability,depend,dependency,34,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:139,deployability,instal,installation,139,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:34,integrability,depend,dependency,34,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:34,modifiability,depend,dependency,34,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:66,modifiability,maintain,maintaining,66,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:34,safety,depend,dependency,34,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:66,safety,maintain,maintaining,66,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:34,testability,depend,dependency,34,"That's weird. Why would cuda be a dependency? I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:26,deployability,instal,install,26,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:84,deployability,instal,installed,84,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:118,deployability,instal,installed,118,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:223,deployability,instal,installation,223,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:191,reliability,doe,doesn,191,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:163,usability,ui,uing,163,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:249,usability,close,closer,249,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:676,availability,Avail,Available,676,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:20,deployability,instal,install,20,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:142,deployability,version,version,142,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:181,deployability,version,version,181,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:236,deployability,instal,install,236,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:336,deployability,fail,failed,336,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:531,deployability,fail,failed,531,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:686,deployability,version,versions,686,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:142,integrability,version,version,142,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:181,integrability,version,version,181,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:686,integrability,version,versions,686,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:431,interoperability,conflict,conflicts,431,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:454,interoperability,incompatib,incompatible,454,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:574,interoperability,specif,specifications,574,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:606,interoperability,incompatib,incompatible,606,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:647,interoperability,format,format,647,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:142,modifiability,version,version,142,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:181,modifiability,version,version,181,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:275,modifiability,pac,package,275,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:467,modifiability,pac,packages,467,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:665,modifiability,pac,package,665,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:686,modifiability,version,versions,686,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:336,reliability,fail,failed,336,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:531,reliability,fail,failed,531,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:676,reliability,Availab,Available,676,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:676,safety,Avail,Available,676,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:676,security,Availab,Available,676,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```. (base) $ conda activate SCA. (SCA) $ conda --version. conda 4.8.2. (SCA) $ python --version. Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy. Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: \ . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:76,deployability,instal,install,76,"Make sure you are searching the `conda-forge` channel, too. . Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:187,deployability,instal,install,187,"Make sure you are searching the `conda-forge` channel, too. . Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:123,integrability,configur,configure,123,"Make sure you are searching the `conda-forge` channel, too. . Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:123,modifiability,configur,configure,123,"Make sure you are searching the `conda-forge` channel, too. . Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:123,security,configur,configure,123,"Make sure you are searching the `conda-forge` channel, too. . Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1142:182,usability,user,user,182,"Make sure you are searching the `conda-forge` channel, too. . Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142
https://github.com/scverse/scanpy/issues/1143:88,deployability,version,version,88,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:110,deployability,log,logging,110,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:88,integrability,version,version,88,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:88,modifiability,version,version,88,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:110,safety,log,logging,110,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:110,security,log,logging,110,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:110,testability,log,logging,110,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1143:47,usability,close,close,47,"Hi! Since there was no follow-up, I’m going to close this as not reproducible. Feel free to reply and we’ll happily reopen this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143
https://github.com/scverse/scanpy/issues/1144:111,performance,time,time,111,"Sorry for that @ivirshup ! I saw you already fixed it in #1145 , thank you and I will keep it in mind for next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1144
https://github.com/scverse/scanpy/issues/1144:17,deployability,build,builds,17,We've got CI doc builds now!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1144
https://github.com/scverse/scanpy/issues/1146:99,availability,avail,available,99,Seems this has been fixed in https://github.com/bioconda/bioconda-recipes/pull/21423. . `1.4.6` is available again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1146
https://github.com/scverse/scanpy/issues/1146:99,reliability,availab,available,99,Seems this has been fixed in https://github.com/bioconda/bioconda-recipes/pull/21423. . `1.4.6` is available again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1146
https://github.com/scverse/scanpy/issues/1146:99,safety,avail,available,99,Seems this has been fixed in https://github.com/bioconda/bioconda-recipes/pull/21423. . `1.4.6` is available again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1146
https://github.com/scverse/scanpy/issues/1146:99,security,availab,available,99,Seems this has been fixed in https://github.com/bioconda/bioconda-recipes/pull/21423. . `1.4.6` is available again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1146
https://github.com/scverse/scanpy/issues/1147:190,deployability,instal,installing,190,"Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:361,availability,error,error,361,"> . > . > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:200,deployability,instal,installing,200,"> . > . > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:361,performance,error,error,361,"> . > . > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:361,safety,error,error,361,"> . > . > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:361,usability,error,error,361,"> . > . > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:12,deployability,version,version,12,What's your version of `numba` in this environment?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:12,integrability,version,version,12,What's your version of `numba` in this environment?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:12,modifiability,version,version,12,What's your version of `numba` in this environment?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:228,availability,error,error,228,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:22,deployability,version,version,22,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:67,deployability,version,version,67,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:119,deployability,build,build,119,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:239,deployability,version,version,239,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:267,deployability,build,build,267,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:22,integrability,version,version,22,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:67,integrability,version,version,67,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:239,integrability,version,version,239,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:22,modifiability,version,version,22,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:67,modifiability,version,version,67,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:239,modifiability,version,version,239,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:228,performance,error,error,228,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:228,safety,error,error,228,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:228,usability,error,error,228,"> . > . > What's your version of `numba` in this environment? It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:232,integrability,repositor,repository,232,"Could you show the result of `print(adata_10x.X)`? -------------------. I'm not sure how much I can help with this, since I don't have a windows machine to try this on. I think this might be best raised an issue over on the `numba` repository. @giovp, maybe you could look into more about how came across this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:232,interoperability,repositor,repository,232,"Could you show the result of `print(adata_10x.X)`? -------------------. I'm not sure how much I can help with this, since I don't have a windows machine to try this on. I think this might be best raised an issue over on the `numba` repository. @giovp, maybe you could look into more about how came across this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:100,usability,help,help,100,"Could you show the result of `print(adata_10x.X)`? -------------------. I'm not sure how much I can help with this, since I don't have a windows machine to try this on. I think this might be best raised an issue over on the `numba` repository. @giovp, maybe you could look into more about how came across this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:931,integrability,repositor,repository,931,"Here's what I got, @ivirshup:. ```python. print(adata_10x.X). (0, 70)	1.0. (0, 166)	1.0. (0, 178)	2.0. (0, 326)	1.0. (0, 363)	1.0. (0, 410)	1.0. (0, 412)	1.0. (0, 492)	41.0. (0, 494)	1.0. (0, 495)	1.0. (0, 496)	1.0. (0, 525)	1.0. (0, 556)	2.0. (0, 558)	6.0. (0, 671)	1.0. (0, 684)	1.0. (0, 735)	1.0. (0, 770)	1.0. (0, 793)	1.0. (0, 820)	1.0. (0, 859)	2.0. (0, 871)	1.0. (0, 908)	15.0. (0, 926)	1.0. (0, 941)	1.0. :	:. (2699, 31849)	1.0. (2699, 31855)	1.0. (2699, 31887)	1.0. (2699, 31949)	2.0. (2699, 31970)	2.0. (2699, 32022)	17.0. (2699, 32044)	1.0. (2699, 32047)	2.0. (2699, 32059)	1.0. (2699, 32065)	1.0. (2699, 32066)	1.0. (2699, 32082)	1.0. (2699, 32186)	1.0. (2699, 32193)	1.0. (2699, 32322)	1.0. (2699, 32442)	1.0. (2699, 32543)	1.0. (2699, 32581)	1.0. (2699, 32641)	1.0. (2699, 32696)	3.0. (2699, 32697)	1.0. (2699, 32698)	7.0. (2699, 32702)	1.0. (2699, 32705)	1.0. (2699, 32708)	3.0. ```. I took a look over at the numba repository, any chance this is related to https://github.com/numba/numba/issues/4529 (i.e. #843 , which I somehow missed when I was looking through issues in this repository before I posted)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:1094,integrability,repositor,repository,1094,"Here's what I got, @ivirshup:. ```python. print(adata_10x.X). (0, 70)	1.0. (0, 166)	1.0. (0, 178)	2.0. (0, 326)	1.0. (0, 363)	1.0. (0, 410)	1.0. (0, 412)	1.0. (0, 492)	41.0. (0, 494)	1.0. (0, 495)	1.0. (0, 496)	1.0. (0, 525)	1.0. (0, 556)	2.0. (0, 558)	6.0. (0, 671)	1.0. (0, 684)	1.0. (0, 735)	1.0. (0, 770)	1.0. (0, 793)	1.0. (0, 820)	1.0. (0, 859)	2.0. (0, 871)	1.0. (0, 908)	15.0. (0, 926)	1.0. (0, 941)	1.0. :	:. (2699, 31849)	1.0. (2699, 31855)	1.0. (2699, 31887)	1.0. (2699, 31949)	2.0. (2699, 31970)	2.0. (2699, 32022)	17.0. (2699, 32044)	1.0. (2699, 32047)	2.0. (2699, 32059)	1.0. (2699, 32065)	1.0. (2699, 32066)	1.0. (2699, 32082)	1.0. (2699, 32186)	1.0. (2699, 32193)	1.0. (2699, 32322)	1.0. (2699, 32442)	1.0. (2699, 32543)	1.0. (2699, 32581)	1.0. (2699, 32641)	1.0. (2699, 32696)	3.0. (2699, 32697)	1.0. (2699, 32698)	7.0. (2699, 32702)	1.0. (2699, 32705)	1.0. (2699, 32708)	3.0. ```. I took a look over at the numba repository, any chance this is related to https://github.com/numba/numba/issues/4529 (i.e. #843 , which I somehow missed when I was looking through issues in this repository before I posted)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:931,interoperability,repositor,repository,931,"Here's what I got, @ivirshup:. ```python. print(adata_10x.X). (0, 70)	1.0. (0, 166)	1.0. (0, 178)	2.0. (0, 326)	1.0. (0, 363)	1.0. (0, 410)	1.0. (0, 412)	1.0. (0, 492)	41.0. (0, 494)	1.0. (0, 495)	1.0. (0, 496)	1.0. (0, 525)	1.0. (0, 556)	2.0. (0, 558)	6.0. (0, 671)	1.0. (0, 684)	1.0. (0, 735)	1.0. (0, 770)	1.0. (0, 793)	1.0. (0, 820)	1.0. (0, 859)	2.0. (0, 871)	1.0. (0, 908)	15.0. (0, 926)	1.0. (0, 941)	1.0. :	:. (2699, 31849)	1.0. (2699, 31855)	1.0. (2699, 31887)	1.0. (2699, 31949)	2.0. (2699, 31970)	2.0. (2699, 32022)	17.0. (2699, 32044)	1.0. (2699, 32047)	2.0. (2699, 32059)	1.0. (2699, 32065)	1.0. (2699, 32066)	1.0. (2699, 32082)	1.0. (2699, 32186)	1.0. (2699, 32193)	1.0. (2699, 32322)	1.0. (2699, 32442)	1.0. (2699, 32543)	1.0. (2699, 32581)	1.0. (2699, 32641)	1.0. (2699, 32696)	3.0. (2699, 32697)	1.0. (2699, 32698)	7.0. (2699, 32702)	1.0. (2699, 32705)	1.0. (2699, 32708)	3.0. ```. I took a look over at the numba repository, any chance this is related to https://github.com/numba/numba/issues/4529 (i.e. #843 , which I somehow missed when I was looking through issues in this repository before I posted)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:1094,interoperability,repositor,repository,1094,"Here's what I got, @ivirshup:. ```python. print(adata_10x.X). (0, 70)	1.0. (0, 166)	1.0. (0, 178)	2.0. (0, 326)	1.0. (0, 363)	1.0. (0, 410)	1.0. (0, 412)	1.0. (0, 492)	41.0. (0, 494)	1.0. (0, 495)	1.0. (0, 496)	1.0. (0, 525)	1.0. (0, 556)	2.0. (0, 558)	6.0. (0, 671)	1.0. (0, 684)	1.0. (0, 735)	1.0. (0, 770)	1.0. (0, 793)	1.0. (0, 820)	1.0. (0, 859)	2.0. (0, 871)	1.0. (0, 908)	15.0. (0, 926)	1.0. (0, 941)	1.0. :	:. (2699, 31849)	1.0. (2699, 31855)	1.0. (2699, 31887)	1.0. (2699, 31949)	2.0. (2699, 31970)	2.0. (2699, 32022)	17.0. (2699, 32044)	1.0. (2699, 32047)	2.0. (2699, 32059)	1.0. (2699, 32065)	1.0. (2699, 32066)	1.0. (2699, 32082)	1.0. (2699, 32186)	1.0. (2699, 32193)	1.0. (2699, 32322)	1.0. (2699, 32442)	1.0. (2699, 32543)	1.0. (2699, 32581)	1.0. (2699, 32641)	1.0. (2699, 32696)	3.0. (2699, 32697)	1.0. (2699, 32698)	7.0. (2699, 32702)	1.0. (2699, 32705)	1.0. (2699, 32708)	3.0. ```. I took a look over at the numba repository, any chance this is related to https://github.com/numba/numba/issues/4529 (i.e. #843 , which I somehow missed when I was looking through issues in this repository before I posted)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1147:73,interoperability,specif,specific,73,"Ah, sorry, looks like I meant to say `print(repr(adata.X))`. I think the specific bug in that previous issue was fixed, but this seems kinda related in that it's windows and 32 bit numbers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147
https://github.com/scverse/scanpy/issues/1148:82,availability,slo,slot,82,"The use case for me is to plot from a large object with many samples, so no image slot present in the object. At the same time I need to see the tissue in a consistent orientation. Thanks for fixing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1148
https://github.com/scverse/scanpy/issues/1148:157,availability,consist,consistent,157,"The use case for me is to plot from a large object with many samples, so no image slot present in the object. At the same time I need to see the tissue in a consistent orientation. Thanks for fixing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1148
https://github.com/scverse/scanpy/issues/1148:122,performance,time,time,122,"The use case for me is to plot from a large object with many samples, so no image slot present in the object. At the same time I need to see the tissue in a consistent orientation. Thanks for fixing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1148
https://github.com/scverse/scanpy/issues/1148:82,reliability,slo,slot,82,"The use case for me is to plot from a large object with many samples, so no image slot present in the object. At the same time I need to see the tissue in a consistent orientation. Thanks for fixing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1148
https://github.com/scverse/scanpy/issues/1148:157,usability,consist,consistent,157,"The use case for me is to plot from a large object with many samples, so no image slot present in the object. At the same time I need to see the tissue in a consistent orientation. Thanks for fixing!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1148
https://github.com/scverse/scanpy/pull/1149:664,deployability,scale,scalefactors,664,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:732,deployability,scale,scalefactors,732,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1024,deployability,scale,scalefactors,1024,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1371,deployability,modul,module,1371,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:664,energy efficiency,scale,scalefactors,664,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:732,energy efficiency,scale,scalefactors,732,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1024,energy efficiency,scale,scalefactors,1024,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:664,modifiability,scal,scalefactors,664,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:732,modifiability,scal,scalefactors,732,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1024,modifiability,scal,scalefactors,1024,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1371,modifiability,modul,module,1371,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1381,modifiability,refact,refactored,1381,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:664,performance,scale,scalefactors,664,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:732,performance,scale,scalefactors,732,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1024,performance,scale,scalefactors,1024,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1381,performance,refactor,refactored,1381,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:775,reliability,doe,does,775,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:930,reliability,doe,does,930,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1371,safety,modul,module,1371,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:547,testability,simpl,simple,547,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:508,usability,behavi,behavior,508,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:547,usability,simpl,simple,547,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:925,usability,user,user,925,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1672,usability,user,user-images,1672,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1799,usability,behavi,behavior,1799,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1871,usability,feedback,feedback,1871,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:1935,usability,user,users,1935,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`. > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301. > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. . Now the point would be:. * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior. * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:. * we probably should assume that also `scalefactors` are empty. * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do. * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right? The reason for flipping is that the coords from space ranger are given with upper origin. ```python. sc.pl.embedding(adata_spatial, basis = ""coords""). ```. ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point. And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:131,deployability,updat,updates,131,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:252,deployability,scale,scale,252,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:252,energy efficiency,scale,scale,252,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:252,modifiability,scal,scale,252,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:252,performance,scale,scale,252,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:131,safety,updat,updates,131,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:131,security,updat,updates,131,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:75,safety,test,test,75,I think it makes sense. I will open a new PR cause I'll have to change the test as well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/pull/1149:75,testability,test,test,75,I think it makes sense. I will open a new PR cause I'll have to change the test as well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149
https://github.com/scverse/scanpy/issues/1150:203,interoperability,coordinat,coordinates,203,"Adding either more parameters or passing `**kwargs` arguments to an underlying tone library sounds reasonable. A PR would be welcome 😄. So you don't have to overwrite the ""X_tsne"" key, you can write the coordinates to whatever key in `obsm` you want, then call `sc.pl.embedding(adata, basis={key})`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1150
https://github.com/scverse/scanpy/issues/1150:19,modifiability,paramet,parameters,19,"Adding either more parameters or passing `**kwargs` arguments to an underlying tone library sounds reasonable. A PR would be welcome 😄. So you don't have to overwrite the ""X_tsne"" key, you can write the coordinates to whatever key in `obsm` you want, then call `sc.pl.embedding(adata, basis={key})`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1150
https://github.com/scverse/scanpy/issues/1151:13,availability,error,error,13,Exactly same error here. info:. scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:13,performance,error,error,13,Exactly same error here. info:. scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:13,safety,error,error,13,Exactly same error here. info:. scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:13,usability,error,error,13,Exactly same error here. info:. scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:129,usability,learn,learn,129,Exactly same error here. info:. scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:4,reliability,doe,does,4,How does that happen?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:47,deployability,version,version,47,"@jipeifeng, I believe this has to do with your version of anndata being out of date. @fidelram, could this be the case for your environment too?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:47,integrability,version,version,47,"@jipeifeng, I believe this has to do with your version of anndata being out of date. @fidelram, could this be the case for your environment too?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:47,modifiability,version,version,47,"@jipeifeng, I believe this has to do with your version of anndata being out of date. @fidelram, could this be the case for your environment too?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:44,deployability,version,version,44,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:80,deployability,updat,updated,80,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:44,integrability,version,version,44,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:44,modifiability,version,version,44,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:80,safety,updat,updated,80,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1151:80,security,updat,updated,80,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151
https://github.com/scverse/scanpy/issues/1152:738,availability,robust,robust,738,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:162,deployability,log,log,162,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:182,deployability,log,logreg,182,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:226,deployability,log,log,226,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:327,deployability,log,logFC,327,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:21,modifiability,paramet,parameter,21,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:738,reliability,robust,robust,738,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:162,safety,log,log,162,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:182,safety,log,logreg,182,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:226,safety,log,log,226,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:327,safety,log,logFC,327,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:491,safety,test,test,491,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:522,safety,test,test,522,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:598,safety,test,test,598,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:695,safety,test,test,695,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:738,safety,robust,robust,738,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:757,safety,test,test,757,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:162,security,log,log,162,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:182,security,log,logreg,182,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:226,security,log,log,226,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:327,security,log,logFC,327,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:162,testability,log,log,162,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:182,testability,log,logreg,182,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:226,testability,log,log,226,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:327,testability,log,logFC,327,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:491,testability,test,test,491,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:522,testability,test,test,522,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:598,testability,test,test,598,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:695,testability,test,test,695,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:757,testability,test,test,757,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here. 1. Not all methods have log fold changes (`'logreg'` for example). 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be... 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:25,deployability,log,log,25,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:45,deployability,log,logreg,45,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:25,safety,log,log,25,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:45,safety,log,logreg,45,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:25,security,log,log,25,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:45,security,log,logreg,45,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:25,testability,log,log,25,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:45,testability,log,logreg,45,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:136,deployability,log,logFC,136,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:183,deployability,log,logistic,183,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:203,energy efficiency,model,model,203,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:136,safety,log,logFC,136,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:183,safety,log,logistic,183,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:136,security,log,logFC,136,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:183,security,log,logistic,183,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:203,security,model,model,203,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:136,testability,log,logFC,136,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:183,testability,log,logistic,183,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1152:192,testability,regress,regression,192,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152
https://github.com/scverse/scanpy/issues/1153:153,usability,workflow,workflows,153,"Thanks for the report! I think I see underlying issue, but can't promise a quick fix. As a heads up, at the moment, backed mode works best for read only workflows like plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1153
https://github.com/scverse/scanpy/issues/1154:72,deployability,updat,update,72,"Hi, @KabitaBaral1. These happens because of changes in umap 0.4. Please update scanpy to solve this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:72,safety,updat,update,72,"Hi, @KabitaBaral1. These happens because of changes in umap 0.4. Please update scanpy to solve this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:72,security,updat,update,72,"Hi, @KabitaBaral1. These happens because of changes in umap 0.4. Please update scanpy to solve this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:112,availability,error,error,112,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:39,deployability,updat,updated,39,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:244,deployability,modul,module,244,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:606,deployability,Configurat,Configuration,606,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1160,deployability,version,version,1160,"). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1269,deployability,build,building,1269,"on__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1424,deployability,contain,contain,1424," get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1687,deployability,Modul,ModuleNotFoundError,1687,"npy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1886,deployability,version,version,1886,"/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1924,deployability,Modul,ModuleNotFoundError,1924,"it__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1948,deployability,modul,module,1948,"rse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2052,deployability,Modul,ModuleNotFoundError,2052,"roj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2143,deployability,modul,module,2143,"ble to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2338,deployability,modul,module,2338," sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_ve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2489,deployability,version,version,2489,"'re using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2510,deployability,version,versioneer,2510," of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2599,deployability,modul,module,2599,"j.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2623,deployability,api,api,2623,"handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2721,deployability,log,logging,2721,"cent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to instal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2732,deployability,log,logg,2732," last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2835,deployability,modul,module,2835,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2944,deployability,log,logging,2944,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2962,deployability,log,logging,2962,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3085,deployability,log,logging,3085,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3100,deployability,modul,module,3100,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3167,deployability,log,logging,3167,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3263,deployability,modul,module,3263,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3508,deployability,version,version,3508,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3585,deployability,version,version,3585,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3609,deployability,version,version,3609,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3641,deployability,Modul,ModuleNotFoundError,3641,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3665,deployability,modul,module,3665,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3719,deployability,instal,install,3719,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3731,deployability,modul,module,3731,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3762,deployability,instal,installed,3762,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:606,integrability,Configur,Configuration,606,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1160,integrability,version,version,1160,"). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1302,integrability,repositor,repository,1302,"tive_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1886,integrability,version,version,1886,"/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2489,integrability,version,version,2489,"'re using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2510,integrability,version,versioneer,2510," of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2623,integrability,api,api,2623,"handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3508,integrability,version,version,3508,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3585,integrability,version,version,3585,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3609,integrability,version,version,3609,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1302,interoperability,repositor,repository,1302,"tive_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2623,interoperability,api,api,2623,"handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:211,modifiability,pac,packages,211,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:244,modifiability,modul,module,244,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:390,modifiability,pac,packages,390,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:606,modifiability,Configur,Configuration,606,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:720,modifiability,pac,packages,720,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:906,modifiability,pac,packages,906,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1160,modifiability,version,version,1160,"). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1234,modifiability,pac,packages,1234,"y in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1687,modifiability,Modul,ModuleNotFoundError,1687,"npy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1786,modifiability,pac,packages,1786,"get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1828,modifiability,pac,package,1828,"on = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1886,modifiability,version,version,1886,"/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1924,modifiability,Modul,ModuleNotFoundError,1924,"it__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1948,modifiability,modul,module,1948,"rse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2052,modifiability,Modul,ModuleNotFoundError,2052,"roj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2143,modifiability,modul,module,2143,"ble to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2306,modifiability,pac,packages,2306,"ry or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2338,modifiability,modul,module,2338," sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_ve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2489,modifiability,version,version,2489,"'re using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2510,modifiability,version,versioneer,2510," of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2570,modifiability,pac,packages,2570,"+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2599,modifiability,modul,module,2599,"j.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2802,modifiability,pac,packages,2802,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2835,modifiability,modul,module,2835,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3069,modifiability,pac,packages,3069,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3100,modifiability,modul,module,3100,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3230,modifiability,pac,packages,3230,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3263,modifiability,modul,module,3263,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3422,modifiability,pac,packages,3422,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3464,modifiability,pac,package,3464,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3508,modifiability,version,version,3508,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3585,modifiability,version,version,3585,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3609,modifiability,version,version,3609,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3625,modifiability,pac,package,3625,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3641,modifiability,Modul,ModuleNotFoundError,3641,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3665,modifiability,modul,module,3665,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3731,modifiability,modul,module,3731,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:112,performance,error,error,112,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:39,safety,updat,updated,39,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:112,safety,error,error,112,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:244,safety,modul,module,244,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1153,safety,detect,detect,1153,"all last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1647,safety,except,exception,1647,"_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1666,safety,except,exception,1666,"44 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1687,safety,Modul,ModuleNotFoundError,1687,"npy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1903,safety,except,except,1903,"packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1924,safety,Modul,ModuleNotFoundError,1924,"it__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1948,safety,modul,module,1948,"rse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2012,safety,except,exception,2012,""". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2031,safety,except,exception,2031,"+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2052,safety,Modul,ModuleNotFoundError,2052,"roj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2116,safety,input,input-,2116,"ror: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2143,safety,modul,module,2143,"ble to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . --",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2338,safety,modul,module,2338," sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_ve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2599,safety,modul,module,2599,"j.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2721,safety,log,logging,2721,"cent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to instal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2732,safety,log,logg,2732," last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2835,safety,modul,module,2835,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2944,safety,log,logging,2944,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2962,safety,log,logging,2962,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3085,safety,log,logging,3085,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3100,safety,modul,module,3100,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3167,safety,log,logging,3167,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3263,safety,modul,module,3263,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3525,safety,except,except,3525,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3641,safety,Modul,ModuleNotFoundError,3641,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3665,safety,modul,module,3665,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3731,safety,modul,module,3731,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:39,security,updat,updated,39,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:606,security,Configur,Configuration,606,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1153,security,detect,detect,1153,"all last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2721,security,log,logging,2721,"cent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to instal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2732,security,log,logg,2732," last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2944,security,log,logging,2944,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2962,security,log,logging,2962,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3085,security,log,logging,3085,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3167,security,log,logging,3167,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:132,testability,Trace,Traceback,132,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1707,testability,Trace,Traceback,1707,".6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2072,testability,Trace,Traceback,2072,"j"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2721,testability,log,logging,2721,"cent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to instal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2732,testability,log,logg,2732," last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the mod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2944,testability,log,logging,2944,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2962,testability,log,logging,2962,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3085,testability,log,logging,3085,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:3167,testability,log,logging,3167,"-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 93 from .compat import pkg_version. 94 . ---> 95 __version__ = pkg_version(__name__). 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 57 from importlib.metadata import version as v. 58 except ImportError:. ---> 59 from importlib_metadata import version as v. 60 return version.parse(v(package)). 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:112,usability,error,error,112,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:988,usability,user,user,988,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1056,usability,user,user,1056,"when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. Module",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1174,usability,User,Users,1174,"a3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>. 89 . ---> 90 __version__ = get_version(root="".."", relative_to=__file__). 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 impo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1538,usability,user,user,1538,"ersion, fallback_root, parse, git_describe_command). 142 config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1594,usability,user,user,1594," config = Configuration(**locals()). --> 143 return _get_version(config). 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config). 146 def _get_version(config):. --> 147 parsed_version = _do_parse(config). 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config). 117 ""https://github.com/user/proj/archive/master.zip "". --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root. 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:2116,usability,input,input-,2116,"ror: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package). 56 try:. ---> 57 from importlib.metadata import version as v. 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last). <ipython-input-11-495a6d84c058> in <module>. 1 import os. ----> 2 import scanpy as sc. 3 import numpy as np. 4 import pandas as pd. 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>. 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>. 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>. 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>. 7 from typ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:72,availability,error,error,72,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:212,availability,error,error,212,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:630,deployability,modul,module,630,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:630,modifiability,modul,module,630,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:834,modifiability,pac,packages,834,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1213,modifiability,pac,packages,1213,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:1555,modifiability,pac,packages,1555,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:72,performance,error,error,72,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:212,performance,error,error,212,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:72,safety,error,error,72,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:212,safety,error,error,212,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:603,safety,input,input-,603,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:630,safety,modul,module,630,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:559,testability,Trace,Traceback,559,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:72,usability,error,error,72,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:98,usability,help,help,98,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:200,usability,command,command,200,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:212,usability,error,error,212,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:603,usability,input,input-,603,"Hello,. I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1. numpy==1.18.2. The full command and error:. ```. adata_pp = adata.copy(). sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6). sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp, n_comps=15). sc.pp.neighbors(adata_pp). sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ```. ```. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-50ed2ff29926> in <module>. 4 sc.pp.log1p(adata_pp). 5 sc.pp.pca(adata_pp, n_comps=15). ----> 6 sc.pp.neighbors(adata_pp). 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 681 knn_distances,. 682 self._adata.shape[0],. --> 683 self.n_neighbors,. 684 ). 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity). 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors). 323 . --> 324 return distances, connectivities.tocsr(). 325 . 326 . AttributeError: 'tuple' object has no attribute 'tocsr'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:53,deployability,version,version,53,"Hi, @aheravi . scanpy==1.4.4.post1 is not the latest version, the latest version is 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:73,deployability,version,version,73,"Hi, @aheravi . scanpy==1.4.4.post1 is not the latest version, the latest version is 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:53,integrability,version,version,53,"Hi, @aheravi . scanpy==1.4.4.post1 is not the latest version, the latest version is 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:73,integrability,version,version,73,"Hi, @aheravi . scanpy==1.4.4.post1 is not the latest version, the latest version is 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:53,modifiability,version,version,53,"Hi, @aheravi . scanpy==1.4.4.post1 is not the latest version, the latest version is 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:73,modifiability,version,version,73,"Hi, @aheravi . scanpy==1.4.4.post1 is not the latest version, the latest version is 1.4.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:20,deployability,updat,updating,20,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:60,deployability,updat,update,60,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:94,deployability,updat,update,94,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:44,reliability,doe,does,44,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:20,safety,updat,updating,20,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:60,safety,updat,update,60,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:94,safety,updat,update,94,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:20,security,updat,updating,20,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:60,security,updat,update,60,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:94,security,updat,update,94,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda? Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:28,deployability,updat,update,28,"Hi @KabitaBaral1 ,. You can update it using:. ```. pip install --upgrade scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:55,deployability,instal,install,55,"Hi @KabitaBaral1 ,. You can update it using:. ```. pip install --upgrade scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:65,deployability,upgrad,upgrade,65,"Hi @KabitaBaral1 ,. You can update it using:. ```. pip install --upgrade scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:65,modifiability,upgrad,upgrade,65,"Hi @KabitaBaral1 ,. You can update it using:. ```. pip install --upgrade scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:28,safety,updat,update,28,"Hi @KabitaBaral1 ,. You can update it using:. ```. pip install --upgrade scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:28,security,updat,update,28,"Hi @KabitaBaral1 ,. You can update it using:. ```. pip install --upgrade scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:32,deployability,updat,update,32,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:67,deployability,instal,install,67,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:77,deployability,upgrad,upgrade,77,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:121,deployability,instal,install,121,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:77,modifiability,upgrad,upgrade,77,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:32,safety,updat,update,32,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:32,security,updat,update,32,"> Hi @KabitaBaral1 ,. > You can update it using:. > . > ```. > pip install --upgrade scanpy. > ```. in conda. ```. conda install -c bioconda scanpy=1.4.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:11,availability,error,error,11,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:40,deployability,updat,updating,40,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:91,deployability,modul,module,91,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:91,modifiability,modul,module,91,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:11,performance,error,error,11,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:11,safety,error,error,11,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:40,safety,updat,updating,40,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:91,safety,modul,module,91,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:40,security,updat,updating,40,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:11,usability,error,error,11,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:81,deployability,instal,installation,81,"Great! I'm going to close this then, since the discussion seems to be more about installation issues now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:20,usability,close,close,20,"Great! I'm going to close this then, since the discussion seems to be more about installation issues now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:18,availability,error,error,18,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:526,deployability,modul,module,526,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:535,deployability,updat,updated,535,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:574,deployability,updat,update,574,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:99,modifiability,pac,packages,99,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:478,modifiability,pac,package,478,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:526,modifiability,modul,module,526,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:18,performance,error,error,18,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:18,safety,error,error,18,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:526,safety,modul,module,526,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:535,safety,updat,updated,535,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:574,safety,updat,update,574,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:535,security,updat,updated,535,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:574,security,updat,update,574,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:18,usability,error,error,18,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:303,usability,learn,learn,303,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:412,usability,learn,learn,412,"I am getting this error when I run scanpy.pp.neighbors(adata). As far as I know, I have the latest packages mentioned here. anndata 0.7.6 pypi_0 pypi. louvain 0.7.0 py38h9dedd22_1 conda-forge. pandas 1.1.3 py38hb1e8313_0. python-igraph 0.9.1 py38h3dab7cd_0 conda-forge. scanpy 1.7.2 pypi_0 pypi. scikit-learn 0.23.2 py38h959d312_0. scipy 1.6.3 py38h431c0a8_0 conda-forge. statsmodels 0.12.0 py38haf1e3a3_0. umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated? **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:47,deployability,version,version,47,"I face the same problem. However I used latest version . </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:47,integrability,version,version,47,"I face the same problem. However I used latest version . </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:47,modifiability,version,version,47,"I face the same problem. However I used latest version . </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/issues/1154:148,usability,learn,learn,148,"I face the same problem. However I used latest version . </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154
https://github.com/scverse/scanpy/pull/1156:80,energy efficiency,current,currently,80,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:379,performance,time,times,379,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:421,performance,perform,performance,421,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:603,performance,time,times,603,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:90,reliability,doe,doesn,90,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:389,reliability,doe,doesn,389,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:31,safety,review,review,31,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:550,safety,avoid,avoid,550,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:31,testability,review,review,31,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:421,usability,perform,performance,421,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon. No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:1218,availability,replic,replicating,1218,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:265,deployability,log,logfoldchanges,265,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:420,deployability,log,logfoldchanges,420,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:708,deployability,log,logfoldchanges,708,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:590,interoperability,specif,specific,590,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:265,safety,log,logfoldchanges,265,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:420,safety,log,logfoldchanges,420,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:708,safety,log,logfoldchanges,708,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:265,security,log,logfoldchanges,265,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:420,security,log,logfoldchanges,420,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:708,security,log,logfoldchanges,708,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:265,testability,log,logfoldchanges,265,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:420,testability,log,logfoldchanges,420,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:708,testability,log,logfoldchanges,708,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:876,testability,simpl,simple,876,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:876,usability,simpl,simple,876,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:769,availability,sla,slack,769,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:414,deployability,API,API,414,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:312,energy efficiency,model,model,312,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:394,energy efficiency,current,current,394,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:414,integrability,API,API,414,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:414,interoperability,API,API,414,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:321,performance,time,time,321,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:751,performance,time,time,751,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:769,reliability,sla,slack,769,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:312,security,model,model,312,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:446,testability,simpl,simple,446,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:446,usability,simpl,simple,446,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:800,usability,feedback,feedback,800,"Thank you all for this! In particular, @Koncopd! I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:537,deployability,contain,contains,537,"Instead of having `self.d` as the central attribute, why not having a central property `self.stats` and that's a DataFrame with a multi-index for the columns. The outer index is the group that you're comparing against a reference, the inner index loops over `gene`, `score`, `pval`, `pval_adj`, etc. Meaning, there is a class for this besides the convenience function:. ```. rg = sc.RankGenes(). ```. And if you just want a dataframe and not store something in `adata`, you can do. ```. rg.compute_stats(...). ```. After this `rg.stats` contains your results. Within the convenience wrapper `sc.tl.rank_genes`, this `stats` attribute will be written to `adata`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:583,integrability,wrap,wrapper,583,"Instead of having `self.d` as the central attribute, why not having a central property `self.stats` and that's a DataFrame with a multi-index for the columns. The outer index is the group that you're comparing against a reference, the inner index loops over `gene`, `score`, `pval`, `pval_adj`, etc. Meaning, there is a class for this besides the convenience function:. ```. rg = sc.RankGenes(). ```. And if you just want a dataframe and not store something in `adata`, you can do. ```. rg.compute_stats(...). ```. After this `rg.stats` contains your results. Within the convenience wrapper `sc.tl.rank_genes`, this `stats` attribute will be written to `adata`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:583,interoperability,wrapper,wrapper,583,"Instead of having `self.d` as the central attribute, why not having a central property `self.stats` and that's a DataFrame with a multi-index for the columns. The outer index is the group that you're comparing against a reference, the inner index loops over `gene`, `score`, `pval`, `pval_adj`, etc. Meaning, there is a class for this besides the convenience function:. ```. rg = sc.RankGenes(). ```. And if you just want a dataframe and not store something in `adata`, you can do. ```. rg.compute_stats(...). ```. After this `rg.stats` contains your results. Within the convenience wrapper `sc.tl.rank_genes`, this `stats` attribute will be written to `adata`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:133,deployability,stage,stages,133,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:190,deployability,releas,releases,190,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:242,deployability,pipelin,pipelines,242,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:180,integrability,coupl,couple,180,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:242,integrability,pipelin,pipelines,242,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:256,integrability,pub,published,256,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:180,modifiability,coupl,couple,180,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:351,performance,time,time,351,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:180,testability,coupl,couple,180,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`? Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:30,usability,user,user-images,30,"@falexwolf . ![image](https://user-images.githubusercontent.com/3065736/80020192-e8b1f300-84d8-11ea-861a-de53894232d8.png). Looks fine with miltindex dataframe. However a bit awkward with this names column when `n_genes` is set. I will set `n_genes` to all by default but do we need this at all? If there is no `n_genes`, then names can be moved to dataframe's index as they are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:26,reliability,doe,doesn,26,`adata.rename_categories` doesn't work with dataframes in uns.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:605,availability,operat,operations,605,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:26,reliability,doe,doesn,26,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:616,reliability,doe,does,616,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:910,safety,test,test,910,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:910,testability,test,test,910,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:320,usability,person,personally,320,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:683,usability,support,support,683,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing? * @falexwolf, why did this method get un-deprecated? > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all? I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest? I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier? **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python. adata.uns[key_added] = {. ""params"": {. ""groupby"": ""leiden"",. ""reference"": ""rest"",. ""test"": ""wilcoxon"",. ""rep"": ""X"". },. ""results"": {. ""1"": ..., # pd.DataFrame, with index of .var_names. ""2"": ..., #etc. },. }. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:212,energy efficiency,adapt,adapting,212,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:143,integrability,messag,message,143,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:212,integrability,adapt,adapting,212,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:143,interoperability,messag,message,143,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:212,interoperability,adapt,adapting,212,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:203,modifiability,requires adapt,requires adapting,203,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:74,testability,simpl,simpler,74,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:74,usability,simpl,simpler,74,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:218,deployability,API,API,218,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:282,deployability,API,APIs,282,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:476,deployability,API,API,476,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:218,integrability,API,API,218,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:282,integrability,API,APIs,282,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:476,integrability,API,API,476,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:218,interoperability,API,API,218,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:282,interoperability,API,APIs,282,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:476,interoperability,API,API,476,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:462,performance,parallel,parallel,462,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:395,security,hack,hack,395,"@Koncopd @falexwolf, I have a proposal for this PR. This PR makes a huge improvement to the internals of the differential expression code. Could we aim to limit the scope here to just improving the internals (i.e., no API introductions or changes)? Then we can introduce and change APIs in later PRs? My interest here is getting the improved internals onto master ASAP, so that other people can hack on it. This would allow correctness improvements to happen in parallel with API design.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:129,testability,plan,plan,129,"Sorry for the delay, i'll return to this soon. @ivirshup . I have nothing against this suggestion, actually this was my original plan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:5,availability,restor,restored,5,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:90,deployability,api,api,90,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:90,integrability,api,api,90,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:34,interoperability,format,format,34,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:90,interoperability,api,api,90,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:5,reliability,restor,restored,5,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:318,safety,compl,complement,318,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:318,security,compl,complement,318,"I've restored the original output format. . However, there are still a few changes to the api. . if `pts=True` (default `False`) the function also computes the fraction of cells expressing the genes for each group, and if `reference='rest'` then `pts_rest` is added with fraction of cells expressing the genes for the complement of each group. `n_genes` is set to all genes by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:58,deployability,fail,fails,58,"Docs are built fine locally, i don't know why readthedocs fails.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/pull/1156:58,reliability,fail,fails,58,"Docs are built fine locally, i don't know why readthedocs fails.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156
https://github.com/scverse/scanpy/issues/1157:22,deployability,version,version,22,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:957,deployability,automat,automatically,957,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:22,integrability,version,version,22,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:395,integrability,batch,batch-corrected,395,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:424,integrability,batch,batch,424,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:770,integrability,batch,batch,770,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1057,integrability,batch,batch,1057,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1145,interoperability,specif,specific,1145,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:22,modifiability,version,version,22,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:629,modifiability,layer,layers,629,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:690,modifiability,layer,layer,690,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:822,modifiability,layer,layers,822,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:918,modifiability,layer,layers,918,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:395,performance,batch,batch-corrected,395,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:424,performance,batch,batch,424,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:770,performance,batch,batch,770,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1057,performance,batch,batch,1057,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:957,testability,automat,automatically,957,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:75,usability,workflow,workflow,75,"A few things! 1. What version is UMAP and numba? 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data. . 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1133,availability,error,error,1133,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:134,deployability,version,version,134,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:134,integrability,version,version,134,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1139,integrability,messag,message,1139,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1139,interoperability,messag,message,1139,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:134,modifiability,version,version,134,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:149,modifiability,pac,package,149,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1133,performance,error,error,1133,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:456,safety,test,test,456,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1133,safety,error,error,1133,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:378,security,modif,modification,378,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:456,testability,test,test,456,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:248,usability,learn,learn,248,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:357,usability,command,commands,357,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:480,usability,support,support,480,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:565,usability,support,support,565,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:661,usability,support,support,661,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:1133,usability,error,error,1133,"So it will only work on non-negative expression values without any pre-process? I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3. https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing? I tried removed all the antibody read counts from adata.X and ran it once, still got same error message. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:111,availability,operat,operating,111,Can you give me the version information from all the packages in your environment (output of conda list)? What operating system are you running?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:20,deployability,version,version,20,Can you give me the version information from all the packages in your environment (output of conda list)? What operating system are you running?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:20,integrability,version,version,20,Can you give me the version information from all the packages in your environment (output of conda list)? What operating system are you running?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:20,modifiability,version,version,20,Can you give me the version information from all the packages in your environment (output of conda list)? What operating system are you running?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:53,modifiability,pac,packages,53,Can you give me the version information from all the packages in your environment (output of conda list)? What operating system are you running?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:49,deployability,version,version,49,I fixed this issue! It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:71,deployability,updat,update,71,I fixed this issue! It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:49,integrability,version,version,49,I fixed this issue! It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:49,modifiability,version,version,49,I fixed this issue! It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:71,safety,updat,update,71,I fixed this issue! It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1157:71,security,updat,update,71,I fixed this issue! It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157
https://github.com/scverse/scanpy/issues/1158:18,integrability,sub,subplots,18,"Same problem with subplots here but I use the same adata object, I plot different columns with `color` and I need to use a different `size` for each column. I get empty subplot panels but one and the rest of plots are shown and individual plots below the panels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:169,integrability,sub,subplot,169,"Same problem with subplots here but I use the same adata object, I plot different columns with `color` and I need to use a different `size` for each column. I get empty subplot panels but one and the rest of plots are shown and individual plots below the panels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:65,deployability,updat,update,65,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. ```python. fig, ax = plt.subplots(1,3, figsize=(20,6)). sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). plt.tight_layout(pad=3.0). plt.show(). ```. ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:149,integrability,sub,subplots,149,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. ```python. fig, ax = plt.subplots(1,3, figsize=(20,6)). sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). plt.tight_layout(pad=3.0). plt.show(). ```. ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:65,safety,updat,update,65,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. ```python. fig, ax = plt.subplots(1,3, figsize=(20,6)). sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). plt.tight_layout(pad=3.0). plt.show(). ```. ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:65,security,updat,update,65,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. ```python. fig, ax = plt.subplots(1,3, figsize=(20,6)). sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). plt.tight_layout(pad=3.0). plt.show(). ```. ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:511,usability,user,user-images,511,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. ```python. fig, ax = plt.subplots(1,3, figsize=(20,6)). sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). plt.tight_layout(pad=3.0). plt.show(). ```. ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:541,availability,cluster,clusters,541,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:2,deployability,updat,updated,2,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:28,deployability,instal,install,28,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:84,deployability,instal,install,84,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:140,deployability,Instal,Installing,140,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:224,deployability,instal,installation,224,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:386,deployability,instal,installed,386,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:541,deployability,cluster,clusters,541,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:161,modifiability,pac,packages,161,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:610,modifiability,pac,packages,610,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:2,safety,updat,updated,2,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:2,security,updat,updated,2,"I updated scanpy:. ```. pip install git+https://github.com/theislab/scanpy.git. pip install spatialde. ```. ```. Successfully built scanpy. Installing collected packages: scanpy. Attempting uninstall: scanpy. Found existing installation: scanpy 1.4.5.2.dev38+gae88b949. Uninstalling scanpy-1.4.5.2.dev38+gae88b949:. Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949. Successfully installed scanpy-1.4.7.dev47+gf6a49e81. ```. Now I'm not able to generate a spatial plot at all. ```. ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs). 763 """""". 764 if library_id is _empty:. --> 765 library_id = next((i for i in adata.uns['spatial'].keys())). 766 else:. 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:97,deployability,version,version,97,"Okay I figured it out. You changed the key 'X_spatial' to 'coords' in anndata objects in the new version, which is not compatible with my previously saved .h5ad files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:97,integrability,version,version,97,"Okay I figured it out. You changed the key 'X_spatial' to 'coords' in anndata objects in the new version, which is not compatible with my previously saved .h5ad files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:119,interoperability,compatib,compatible,119,"Okay I figured it out. You changed the key 'X_spatial' to 'coords' in anndata objects in the new version, which is not compatible with my previously saved .h5ad files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:97,modifiability,version,version,97,"Okay I figured it out. You changed the key 'X_spatial' to 'coords' in anndata objects in the new version, which is not compatible with my previously saved .h5ad files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:52,reliability,Doe,Does,52,"Where can I find the description of how it changed? Does it support. multiple samples now? Thanks! On Thu, 16 Apr 2020, 16:40 giovp, <notifications@github.com> wrote:. > Yes exactly, the structure of adata.uns has changed so you might have to. > import that visium dataset again and reprocess it. Sorry for that, should. > have mentioned it before. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-614730361>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTV3TYYVXW77K32CRBJLRM4RFLANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:598,security,auth,auth,598,"Where can I find the description of how it changed? Does it support. multiple samples now? Thanks! On Thu, 16 Apr 2020, 16:40 giovp, <notifications@github.com> wrote:. > Yes exactly, the structure of adata.uns has changed so you might have to. > import that visium dataset again and reprocess it. Sorry for that, should. > have mentioned it before. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-614730361>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTV3TYYVXW77K32CRBJLRM4RFLANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:60,usability,support,support,60,"Where can I find the description of how it changed? Does it support. multiple samples now? Thanks! On Thu, 16 Apr 2020, 16:40 giovp, <notifications@github.com> wrote:. > Yes exactly, the structure of adata.uns has changed so you might have to. > import that visium dataset again and reprocess it. Sorry for that, should. > have mentioned it before. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-614730361>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTV3TYYVXW77K32CRBJLRM4RFLANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:283,safety,test,tested,283,"You can have a look at the new structure in the function definition [here](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/readwrite.py#L281) . . Regarding multiple samples support, that is on its way! Will post here as soon as it's ready and tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:283,testability,test,tested,283,"You can have a look at the new structure in the function definition [here](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/readwrite.py#L281) . . Regarding multiple samples support, that is on its way! Will post here as soon as it's ready and tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:213,usability,support,support,213,"You can have a look at the new structure in the function definition [here](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/readwrite.py#L281) . . Regarding multiple samples support, that is on its way! Will post here as soon as it's ready and tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,deployability,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,integrability,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,interoperability,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,modifiability,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,reliability,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,security,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:112,testability,integr,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:32,usability,support,supported,32,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,deployability,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,integrability,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,interoperability,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,modifiability,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,reliability,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,security,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:568,security,auth,auth,568,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:228,testability,integr,integration-scanorama,228,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:143,usability,support,supported,143,"Nice! Thanks! On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see. > here. > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>. > for description on how to use the new concat strategy. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:67,deployability,updat,update,67,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > . > ```python. > fig, ax = plt.subplots(1,3, figsize=(20,6)). > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > plt.tight_layout(pad=3.0). > plt.show(). > ```. > . > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:159,integrability,sub,subplots,159,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > . > ```python. > fig, ax = plt.subplots(1,3, figsize=(20,6)). > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > plt.tight_layout(pad=3.0). > plt.show(). > ```. > . > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:67,safety,updat,update,67,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > . > ```python. > fig, ax = plt.subplots(1,3, figsize=(20,6)). > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > plt.tight_layout(pad=3.0). > plt.show(). > ```. > . > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:67,security,updat,update,67,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > . > ```python. > fig, ax = plt.subplots(1,3, figsize=(20,6)). > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > plt.tight_layout(pad=3.0). > plt.show(). > ```. > . > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:539,usability,user,user-images,539,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > . > ```python. > fig, ax = plt.subplots(1,3, figsize=(20,6)). > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > plt.tight_layout(pad=3.0). > plt.show(). > ```. > . > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:69,deployability,updat,update,69,"> > Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > > ```python. > > fig, ax = plt.subplots(1,3, figsize=(20,6)). > > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > > plt.tight_layout(pad=3.0). > > plt.show(). > > ```. > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). > . > Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code. I've got the reason: if the data type is category, sc.pl.spetial would append lengend, in others conditon it would append colorbar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:161,integrability,sub,subplots,161,"> > Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > > ```python. > > fig, ax = plt.subplots(1,3, figsize=(20,6)). > > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > > plt.tight_layout(pad=3.0). > > plt.show(). > > ```. > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). > . > Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code. I've got the reason: if the data type is category, sc.pl.spetial would append lengend, in others conditon it would append colorbar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:69,safety,updat,update,69,"> > Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > > ```python. > > fig, ax = plt.subplots(1,3, figsize=(20,6)). > > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > > plt.tight_layout(pad=3.0). > > plt.show(). > > ```. > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). > . > Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code. I've got the reason: if the data type is category, sc.pl.spetial would append lengend, in others conditon it would append colorbar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:69,security,updat,update,69,"> > Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > > ```python. > > fig, ax = plt.subplots(1,3, figsize=(20,6)). > > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > > plt.tight_layout(pad=3.0). > > plt.show(). > > ```. > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). > . > Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code. I've got the reason: if the data type is category, sc.pl.spetial would append lengend, in others conditon it would append colorbar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1158:617,usability,user,user-images,617,"> > Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work. > > ```python. > > fig, ax = plt.subplots(1,3, figsize=(20,6)). > > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False). > > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False). > > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False). > > plt.tight_layout(pad=3.0). > > plt.show(). > > ```. > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > . > > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). > . > Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code. I've got the reason: if the data type is category, sc.pl.spetial would append lengend, in others conditon it would append colorbar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158
https://github.com/scverse/scanpy/issues/1159:13,integrability,sub,subsampling,13,"I'm not sure subsampling is the most straightforward solution here. If you want the area of the plot for each sample to take the same amount of space, why not just plot the mean for each group? If you want even use of space and an idea of the distribution, why not use something like a violin plot?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1159
https://github.com/scverse/scanpy/issues/1159:243,interoperability,distribut,distribution,243,"I'm not sure subsampling is the most straightforward solution here. If you want the area of the plot for each sample to take the same amount of space, why not just plot the mean for each group? If you want even use of space and an idea of the distribution, why not use something like a violin plot?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1159
https://github.com/scverse/scanpy/issues/1159:150,availability,cluster,cluster,150,This issue has been mentioned on **Scanpy**. There might be relevant details there:. https://scanpy.discourse.group/t/heatmap-max-number-of-cells-per-cluster/202/2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1159
https://github.com/scverse/scanpy/issues/1159:150,deployability,cluster,cluster,150,This issue has been mentioned on **Scanpy**. There might be relevant details there:. https://scanpy.discourse.group/t/heatmap-max-number-of-cells-per-cluster/202/2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1159
https://github.com/scverse/scanpy/issues/1159:118,energy efficiency,heat,heatmap-max-number-of-cells-per-cluster,118,This issue has been mentioned on **Scanpy**. There might be relevant details there:. https://scanpy.discourse.group/t/heatmap-max-number-of-cells-per-cluster/202/2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1159
https://github.com/scverse/scanpy/pull/1160:127,deployability,fail,failing,127,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:154,deployability,releas,release,154,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:55,reliability,doe,doesn,55,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:127,reliability,fail,failing,127,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:42,safety,test,test,42,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:111,safety,test,tests,111,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:42,testability,test,test,42,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:111,testability,test,tests,111,"Thanks for catching this! Could you add a test so this doesn't happen again in the future? Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:447,availability,error,errors,447,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:696,availability,state,statement,696,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:738,availability,consist,consistent,738,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:954,availability,state,stated,954,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1207,availability,error,error,1207,"tely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1510,availability,error,errors,1510,"uched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1556,availability,ERROR,ERROR,1556,"here zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib ver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:47,deployability,scale,scale,47,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:179,deployability,scale,scaled,179,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:228,deployability,scale,scale,228,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:234,deployability,scale,scale,234,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:275,deployability,scale,scale,275,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:281,deployability,scale,scale,281,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1451,deployability,version,version,1451," But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1475,deployability,version,version,1475,"False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not sup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1659,deployability,modul,module,1659,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1811,deployability,modul,module,1811,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2100,deployability,version,version,2100,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2238,deployability,version,version,2238,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2303,deployability,version,version,2303,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2396,deployability,version,version,2396,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2429,deployability,version,version,2429,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2445,deployability,version,version,2445,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2556,deployability,version,version,2556,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:47,energy efficiency,scale,scale,47,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:179,energy efficiency,scale,scaled,179,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:228,energy efficiency,scale,scale,228,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:234,energy efficiency,scale,scale,234,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:275,energy efficiency,scale,scale,275,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:281,energy efficiency,scale,scale,281,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2537,energy efficiency,current,current,2537,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:696,integrability,state,statement,696,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:954,integrability,state,stated,954,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1451,integrability,version,version,1451," But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1475,integrability,version,version,1475,"False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not sup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2100,integrability,version,version,2100,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2238,integrability,version,version,2238,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2303,integrability,version,version,2303,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2396,integrability,version,version,2396,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2429,integrability,version,version,2429,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2445,integrability,version,version,2445,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2556,integrability,version,version,2556,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1459,interoperability,conflict,conflict,1459,"`zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeEr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:47,modifiability,scal,scale,47,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:179,modifiability,scal,scaled,179,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:228,modifiability,scal,scale,228,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:234,modifiability,scal,scale,234,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:275,modifiability,scal,scale,275,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:281,modifiability,scal,scale,281,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1260,modifiability,scal,scaling,1260,"t instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.con",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1451,modifiability,version,version,1451," But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1475,modifiability,version,version,1475,"False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not sup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1659,modifiability,modul,module,1659,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1763,modifiability,pac,packages,1763,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1811,modifiability,modul,module,1811,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1881,modifiability,pac,packages,1881,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2019,modifiability,pac,packages,2019,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2100,modifiability,version,version,2100,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2165,modifiability,pac,packages,2165,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2238,modifiability,version,version,2238,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2303,modifiability,version,version,2303,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2396,modifiability,version,version,2396,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2429,modifiability,version,version,2429,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2445,modifiability,version,version,2445,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2556,modifiability,version,version,2556,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:47,performance,scale,scale,47,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:179,performance,scale,scaled,179,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:228,performance,scale,scale,228,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:234,performance,scale,scale,234,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:275,performance,scale,scale,275,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:281,performance,scale,scale,281,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:447,performance,error,errors,447,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1207,performance,error,error,1207,"tely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1510,performance,error,errors,1510,"uched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1556,performance,ERROR,ERROR,1556,"here zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib ver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:13,safety,test,test,13,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:204,safety,compl,completely,204,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:447,safety,error,errors,447,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1093,safety,test,test,1093,"as until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1207,safety,error,error,1207,"tely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1313,safety,test,tests,1313,"ame result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1414,safety,test,testing,1414,"bers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1510,safety,error,errors,1510,"uched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1556,safety,ERROR,ERROR,1556,"here zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib ver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1580,safety,test,tests,1580,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1628,safety,test,tests,1628,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1659,safety,modul,module,1659,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1684,safety,test,testing,1684,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1783,safety,test,testing,1783,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1811,safety,modul,module,1811,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1901,safety,test,testing,1901,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:204,security,compl,completely,204,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:13,testability,test,test,13,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1093,testability,test,test,1093,"as until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1313,testability,test,tests,1313,"ame result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1414,testability,test,testing,1414,"bers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1580,testability,test,tests,1580,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1628,testability,test,tests,1628,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1684,testability,test,testing,1684,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1783,testability,test,testing,1783,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1901,testability,test,testing,1901,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:34,usability,behavi,behaviour,34,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:447,usability,error,errors,447,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:738,usability,consist,consistent,738,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:936,usability,behavi,behaviour,936,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:968,usability,document,documentation,968,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:985,usability,person,personally,985,"Just added a test and changed the behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1028,usability,undo,undocumented,1028,"behaviour of scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1041,usability,behavi,behaviour,1041," scale a little more. The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1126,usability,behavi,behaviour,1126,"ry tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.con",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1207,usability,error,error,1207,"tely meaningless `scale[scale == 0] = 1e-12`. Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1510,usability,error,errors,1510,"uched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1556,usability,ERROR,ERROR,1556,"here zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib ver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1737,usability,custom,custom,1737,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1855,usability,custom,custom,1855,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1993,usability,custom,custom,1993,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2139,usability,custom,custom,2139,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2272,usability,custom,custom,2272,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2365,usability,custom,custom,2365,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2476,usability,support,supported,2476,"entering was done afterwards anyway due to the original bug. Therefore this is no code breaking change. But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway... In the new test I explicitly check for this behaviour to make it well defined. Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:. `________________ ERROR collecting scanpy/tests/test_plotting.py ________________. scanpy/tests/test_plotting.py:16: in <module>. from matplotlib.testing.compare import compare_images. ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>. _update_converter(). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter. mpl._get_executable_info(""gs""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info. return impl([e, ""--version""], ""(.*)"", ""9""). ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl. if min_ver is not None and version < min_ver:. ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__. c = self._cmp(other). ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp. if self.version < other.version:. E TypeError: '<' not supported between instances of 'str' and 'int''`. I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:267,deployability,scale,scale,267,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:267,energy efficiency,scale,scale,267,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:88,integrability,topic,topic,88,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:392,integrability,sub,substituted,392,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:267,modifiability,scal,scale,267,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:267,performance,scale,scale,267,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:153,safety,review,review,153,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:250,security,control,control,250,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:153,testability,review,review,153,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:234,testability,simpl,simplifying,234,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:250,testability,control,control,250,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:220,usability,progress,progress,220,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:234,usability,simpl,simplifying,234,"Sorry for the late response! In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:384,availability,sli,slightly,384,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1559,availability,consist,consistent,1559,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1611,availability,state,statement,1611,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:576,deployability,scale,scale,576,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2029,deployability,observ,observations,2029,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2174,deployability,automat,automatic,2174,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:576,energy efficiency,scale,scale,576,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1282,energy efficiency,optim,optimized,1282,"rse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without ze",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1611,integrability,state,statement,1611,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:118,modifiability,deco,decorator,118,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:576,modifiability,scal,scale,576,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1257,modifiability,scal,scaling,1257,"`inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1952,modifiability,Variab,Variables,1952,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:576,performance,scale,scale,576,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1282,performance,optimiz,optimized,1282,"rse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without ze",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:384,reliability,sli,slightly,384,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:416,safety,test,tests,416,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:43,security,control,control,43,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:393,security,modif,modified,393,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:43,testability,control,control,43,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:416,testability,test,tests,416,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2029,testability,observ,observations,2029,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:2174,testability,automat,automatic,2174,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1159,usability,efficien,efficient,1159,"tory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1539,usability,behavi,behaviour,1539,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1559,usability,consist,consistent,1559,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:1915,usability,behavi,behaviour,1915,"ghtly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:. ```. X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]). X = sc.pp.scale(Xtest, copy=True, zero_center=False). X. ```. If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`. if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`. if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`. But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]. Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1160:0,energy efficiency,Cool,Cool,0,Cool! I knew the .astype method only from pandas...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160
https://github.com/scverse/scanpy/pull/1161:94,energy efficiency,current,currently,94,"Hi all, is there any progress on this? @michalk8 and myself rely on this for a package we are currently developing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:79,modifiability,pac,package,79,"Hi all, is there any progress on this? @michalk8 and myself rely on this for a package we are currently developing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:21,usability,progress,progress,21,"Hi all, is there any progress on this? @michalk8 and myself rely on this for a package we are currently developing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:182,modifiability,paramet,parameter,182,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:44,safety,review,reviewed,44,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:161,safety,test,tests,161,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:134,security,modif,modify,134,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:44,testability,review,reviewed,44,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:161,testability,test,tests,161,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:36,safety,test,test,36,"@michalk8 , would be great to add a test for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:36,testability,test,test,36,"@michalk8 , would be great to add a test for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:70,safety,test,tests,70,@fidelram sorry it took so long... I've added the param to one of the tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/pull/1161:70,testability,test,tests,70,@fidelram sorry it took so long... I've added the param to one of the tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161
https://github.com/scverse/scanpy/issues/1163:1038,availability,down,downloads,1038,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,deployability,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,deployability,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1011,deployability,loader,loaders,1011," a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise tho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,deployability,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,deployability,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:390,energy efficiency,predict,prediction,390,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1011,energy efficiency,load,loaders,1011," a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise tho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:565,integrability,pub,public,565,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,integrability,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,integrability,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,integrability,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,integrability,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:378,interoperability,specif,specificity,378,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,interoperability,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,interoperability,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1071,interoperability,specif,specific,1071,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,interoperability,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,interoperability,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1371,interoperability,specif,specificity,1371,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1413,interoperability,standard,standard,1413,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1490,interoperability,standard,standard,1490,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,modifiability,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,modifiability,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,modifiability,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,modifiability,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1960,modifiability,extens,extension,1960,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1011,performance,load,loaders,1011," a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise tho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:75,reliability,doe,does,75,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,reliability,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,reliability,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,reliability,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,reliability,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:390,safety,predict,prediction,390,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,security,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,security,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,security,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,security,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:909,testability,integr,integrated,909,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:962,testability,Integr,Integration,962,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1094,testability,integr,integrate,1094,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1322,testability,integr,integration,1322,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1556,testability,context,context,1556,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1735,testability,context,context,1735,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:843,usability,learn,learned,843,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great doc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1422,usability,multi-mod,multi-modal,1422,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1942,usability,help,helpful,1942,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1973,usability,custom,custom,1973,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1980,usability,workflow,workflows,1980,"ok at the functionalities and setup and it does look very nice! - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data. - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so. - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case. - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think. - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too! Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! Best,. David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1070,availability,down,downloads,1070,"look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2435,availability,sli,slight,2435,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,deployability,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1043,deployability,loader,loaders,1043,"rest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,deployability,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1291,deployability,automat,automatically,1291,"rently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1623,deployability,continu,continue,1623,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,deployability,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:233,energy efficiency,predict,prediction,233,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:295,energy efficiency,current,currently,295,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1043,energy efficiency,load,loaders,1043,"rest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2326,energy efficiency,Current,Currently,2326,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:519,integrability,sub,subgraph,519,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:690,integrability,sub,subclass,690,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,integrability,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,integrability,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,integrability,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:221,interoperability,specif,specificity,221,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,interoperability,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1103,interoperability,specif,specific,1103,"setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,interoperability,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1383,interoperability,specif,specific,1383,"stance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm sti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,interoperability,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1762,interoperability,specif,specificity,1762,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,modifiability,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,modifiability,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,modifiability,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1981,modifiability,extens,extension,1981,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:355,performance,network,networks,355,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:448,performance,network,networks,448,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1043,performance,load,loaders,1043,"rest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:120,reliability,doe,does,120,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:960,reliability,doe,does,960,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,reliability,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,reliability,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,reliability,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2435,reliability,sli,slight,2435,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:233,safety,predict,prediction,233,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:355,security,network,networks,355,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:448,security,network,networks,448,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,security,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,security,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1354,security,ident,identify,1354,"works based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,security,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2442,security,modif,modifications,2442,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:994,testability,Integr,Integration,994,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1126,testability,integr,integrate,1126,"ery nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.githu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1291,testability,automat,automatically,1291,"rently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1713,testability,integr,integration,1713,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2340,testability,simpl,simply,2340,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:149,usability,learn,learned,149,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:305,usability,support,supports,305,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:563,usability,Support,Supporting,563,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:590,usability,learn,learned,590,"Hi David, . thanks for your reply and your interest! > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1257,usability,help,helpful,1257,"sily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1963,usability,help,helpful,1963,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:1994,usability,custom,custom,1994,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2001,usability,workflow,workflows,2001,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:2340,usability,simpl,simply,2340,"cellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps? > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though! There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, . Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:170,integrability,repositor,repository,170,With scirpy being officially part of the scverse this issue can be closed I guess. . Anyone interested in single-cell TCR/BCR analysis can always reach out in the scirpy repository!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:170,interoperability,repositor,repository,170,With scirpy being officially part of the scverse this issue can be closed I guess. . Anyone interested in single-cell TCR/BCR analysis can always reach out in the scirpy repository!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1163:67,usability,close,closed,67,With scirpy being officially part of the scverse this issue can be closed I guess. . Anyone interested in single-cell TCR/BCR analysis can always reach out in the scirpy repository!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163
https://github.com/scverse/scanpy/issues/1164:228,availability,replic,replicates,228,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:28,deployability,version,version,28,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:50,deployability,updat,updating,50,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:64,deployability,version,version,64,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:207,energy efficiency,reduc,reduced,207,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:28,integrability,version,version,28,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:64,integrability,version,version,64,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:28,modifiability,version,version,28,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:64,modifiability,version,version,64,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:50,safety,updat,updating,50,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:50,security,updat,updating,50,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:135,availability,down,downgrade,135,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:38,deployability,Version,Versions,38,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:164,deployability,version,version,164,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:219,deployability,Stack,Stackoverflow,219,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:245,deployability,updat,updated,245,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:286,deployability,updat,update,286,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:844,deployability,fail,failed,844,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:2024,deployability,fail,failed,2024,"/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:3000,deployability,fail,failed,3000,"e Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4648,deployability,modul,module,4648,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:38,integrability,Version,Versions,38,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:164,integrability,version,version,164,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:38,modifiability,Version,Versions,38,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:164,modifiability,version,version,164,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:337,modifiability,variab,variable,337,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:687,modifiability,pac,packages,687,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:1012,modifiability,pac,packages,1012,"Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:1107,modifiability,pac,packages,1107,"'0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:1497,modifiability,pac,packages,1497,"ver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:1592,modifiability,pac,packages,1592,". ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:1864,modifiability,pac,packages,1864,"e to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:2171,modifiability,pac,packages,2171,"data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:2423,modifiability,pac,packages,2423,"f assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:2608,modifiability,pac,packages,2608,"rocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:2840,modifiability,pac,packages,2840,"/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:3168,modifiability,pac,packages,3168,"ackages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/pyth",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:3263,modifiability,pac,packages,3263,"2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:3653,modifiability,pac,packages,3653,"70:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:3748,modifiability,pac,packages,3748,", float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4020,modifiability,pac,packages,4020,"e to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4183,modifiability,pac,packages,4183,"processing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). Att",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4443,modifiability,pac,packages,4443,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4648,modifiability,modul,module,4648,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4758,modifiability,pac,packages,4758,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4909,modifiability,pac,packages,4909,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:5060,modifiability,pac,packages,5060,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:844,reliability,fail,failed,844,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:2024,reliability,fail,failed,2024,"/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:3000,reliability,fail,failed,3000,"e Function ""_it_sol"" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True, but has lifted loops. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 270:. @numba.jit. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:245,safety,updat,updated,245,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:286,safety,updat,update,286,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4612,safety,input,input-,4612,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4648,safety,modul,module,4648,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:245,security,updat,updated,245,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:286,security,updat,update,286,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4561,testability,Trace,Traceback,4561,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:32,usability,help,help,32,"Hi @ivirshup ,. Thanks for your help. Versions:. ```. In [1]: import numba. In [2]: numba.__version__. Out[2]: '0.45.0'. ```. I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post. Now I updated anndata through conda:. ```conda update anndata```. And ran this code (minus highly variable gene calculation):. ```. adataCombat = sc.read_h5ad(results_file). #Run combat:. # sc.pp.highly_variable_genes(adataCombat). sc.pp.pca(adataCombat, svd_solver='arpack'). sc.pp.combat(adataCombat, key='sample'). sc.pp.neighbors(adataCombat, n_pcs =50). ```. with even worse output:. ```. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: . Compilation is falling back to object mode WITHOUT looplifting enabled bec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:4612,usability,input,input-,4612,"t.py"", line 311:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old). sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])). ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. sum2 = sum2 ** 2. sum2 = sum2.sum(axis=1). ^. @numba.jit. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/compiler.py:742: NumbaWarning: Function ""_it_sol"" was compiled in object mode without forceobj=True. File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 305:. def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:. <source elided>. change = 1. count = 0. ^. self.func_ir.loc)). /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:235: RuntimeWarning: divide by zero encountered in true_divide. b_prior[i],. Traceback (most recent call last):. File ""<ipython-input-2-523dfec14840>"", line 6, in <module>. sc.pp.neighbors(adataCombat, n_pcs =50). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 95, in neighbors. random_state=random_state,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 683, in compute_neighbors. self.n_neighbors,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/neighbors/__init__.py"", line 324, in compute_connectivities_umap. return distances, connectivities.tocsr(). AttributeError: 'tuple' object has no attribute 'tocsr'. ```. Now ```sc.pp.neighbors()``` cannot run on the data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:63,deployability,instal,installs,63,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:85,deployability,version,versions,85,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:105,deployability,upgrad,upgrade,105,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:85,integrability,version,versions,85,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:85,modifiability,version,versions,85,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1164:105,modifiability,upgrad,upgrade,105,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164
https://github.com/scverse/scanpy/issues/1166:12,deployability,instal,installing,12,Hi! so when installing via pip this is not an issue... somehow with conda it doesnt work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:77,reliability,doe,doesnt,77,Hi! so when installing via pip this is not an issue... somehow with conda it doesnt work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:109,availability,error,error,109,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:136,deployability,modul,module,136,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:136,modifiability,modul,module,136,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:109,performance,error,error,109,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:9,reliability,doe,doesnt,9,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:109,safety,error,error,109,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:136,safety,modul,module,136,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:109,usability,error,error,109,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:53,deployability,instal,install,53,"So, it looks like the conda environment has a broken install of Matplotlib, which I don't think I can help with too much. Are you able to create an environment with just Matplotlib, where you're able to import `pyplot`? Does adding scanpy to this environment break matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:220,reliability,Doe,Does,220,"So, it looks like the conda environment has a broken install of Matplotlib, which I don't think I can help with too much. Are you able to create an environment with just Matplotlib, where you're able to import `pyplot`? Does adding scanpy to this environment break matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1166:102,usability,help,help,102,"So, it looks like the conda environment has a broken install of Matplotlib, which I don't think I can help with too much. Are you able to create an environment with just Matplotlib, where you're able to import `pyplot`? Does adding scanpy to this environment break matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166
https://github.com/scverse/scanpy/issues/1167:172,availability,replic,replicate,172,"This looks like an issue with `mnn_correct` , and is probably more appropriate for that repo (https://github.com/chriscainx/mnnpy). I would note that on my end I'm able to replicate the warnings but not the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167
https://github.com/scverse/scanpy/issues/1167:207,availability,error,error,207,"This looks like an issue with `mnn_correct` , and is probably more appropriate for that repo (https://github.com/chriscainx/mnnpy). I would note that on my end I'm able to replicate the warnings but not the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167
https://github.com/scverse/scanpy/issues/1167:207,performance,error,error,207,"This looks like an issue with `mnn_correct` , and is probably more appropriate for that repo (https://github.com/chriscainx/mnnpy). I would note that on my end I'm able to replicate the warnings but not the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167
https://github.com/scverse/scanpy/issues/1167:207,safety,error,error,207,"This looks like an issue with `mnn_correct` , and is probably more appropriate for that repo (https://github.com/chriscainx/mnnpy). I would note that on my end I'm able to replicate the warnings but not the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167
https://github.com/scverse/scanpy/issues/1167:207,usability,error,error,207,"This looks like an issue with `mnn_correct` , and is probably more appropriate for that repo (https://github.com/chriscainx/mnnpy). I would note that on my end I'm able to replicate the warnings but not the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167
https://github.com/scverse/scanpy/issues/1168:43,availability,error,error,43,"@rsggsr, that looks like a warning, not an error to me. Do the plots look wrong to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:43,performance,error,error,43,"@rsggsr, that looks like a warning, not an error to me. Do the plots look wrong to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:43,safety,error,error,43,"@rsggsr, that looks like a warning, not an error to me. Do the plots look wrong to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:43,usability,error,error,43,"@rsggsr, that looks like a warning, not an error to me. Do the plots look wrong to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16,performance,time,time,16,"@ivirshup, this time no any warnings. The sc.pl.draw_graph looks totally fine. But the sc.tl.paga looks terrible with pie charts and connectivity edges were separated (There should be the figure below). . ![下載 (2)](https://user-images.githubusercontent.com/57272642/79574029-877bc100-808d-11ea-9c0a-114942fb1f4e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:223,usability,user,user-images,223,"@ivirshup, this time no any warnings. The sc.pl.draw_graph looks totally fine. But the sc.tl.paga looks terrible with pie charts and connectivity edges were separated (There should be the figure below). . ![下載 (2)](https://user-images.githubusercontent.com/57272642/79574029-877bc100-808d-11ea-9c0a-114942fb1f4e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:602,deployability,modul,module,602,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1927,energy efficiency,core,core,1927,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2132,energy efficiency,core,core,2132,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2371,energy efficiency,core,core,2371,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2461,integrability,wrap,wrap,2461,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2543,integrability,wrap,wrap,2543,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:630,interoperability,format,format,630,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:742,interoperability,format,format,742,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:602,modifiability,modul,module,602,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:850,modifiability,pac,packages,850,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1380,modifiability,pac,packages,1380,"the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnume",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1609,modifiability,pac,packages,1609," title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1912,modifiability,pac,packages,1912,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2117,modifiability,pac,packages,2117,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2356,modifiability,pac,packages,2356,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:368,performance,perform,perform,368,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:405,reliability,Doe,Does,405,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:398,safety,input,input,398,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:575,safety,input,input-,575,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:602,safety,modul,module,602,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2434,safety,except,except,2434,"18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:31,testability,understand,understanding,31,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:531,testability,Trace,Traceback,531,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:368,usability,perform,perform,368,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:398,usability,input,input,398,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:575,usability,input,input-,575,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:708,usability,User,Users,708,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:776,usability,User,Users,776,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right? And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it? ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-44-72c504b15b2e> in <module>. 17 title='{} path'.format(descr),. 18 return_data=True,. ---> 19 show=False). 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1037 if n_avg > 1:. 1038 old_len_x = len(x). -> 1039 x = moving_average(x). 1040 if ikey == 0:. 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:341,interoperability,format,format,341,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:452,interoperability,format,format,452,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:563,interoperability,format,format,563,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:674,interoperability,format,format,674,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:785,interoperability,format,format,785,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:896,interoperability,format,format,896,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1007,interoperability,format,format,1007,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1118,interoperability,format,format,1118,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1229,interoperability,format,format,1229,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1258,usability,indicat,indicates,1258,"@rsggsr I've been dealing with the same ValueError also. Adding a `print(x)` at https://github.com/theislab/scanpy/blob/d47d373e96fed96a40186f258ec81994534fd1cc/scanpy/plotting/_tools/paga.py#L1039. presented the following:. ```{python}. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. with 0 stored elements in Compressed Sparse Row format>]. ```. Which further indicates that the `use_raw=True` is leading to the usage of sparse matrices. I solved that with `sc.pl.paga_path(... use_raw=False)` and making sure that `type(adata.X) != 'Sparse Matrix'`, e.g. converting it to a `np.array` for instance. . (`sc.__version__=1.4.6`, `matplotlib.__version__ = '3.1.3'` )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:61,availability,state,statement,61,@rsggsr what is the print output after inserting the `print` statement at l1039. Make sure that it is before `x=moving_average(...)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:61,integrability,state,statement,61,@rsggsr what is the print output after inserting the `print` statement at l1039. Make sure that it is before `x=moving_average(...)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:79087,deployability,modul,module,79087," matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>]. </details>. <details>. <summary> traceback </summary>. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-41-14c81f38e281> in <module>. 18 return_data=True,. 19 show=True,. ---> 20 use_raw=False). 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1038 old_len_x = len(x). 1039 print(x). -> 1040 x = moving_average(x). 1041 if ikey == 0:. 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:80390,energy efficiency,core,core,80390,"=True,. ---> 20 use_raw=False). 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1038 old_len_x = len(x). 1039 print(x). -> 1040 x = moving_average(x). 1041 if ikey == 0:. 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:80595,energy efficiency,core,core,80595,"=True,. ---> 20 use_raw=False). 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1038 old_len_x = len(x). 1039 print(x). -> 1040 x = moving_average(x). 1041 if ikey == 0:. 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:80834,energy efficiency,core,core,80834,"=True,. ---> 20 use_raw=False). 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1038 old_len_x = len(x). 1039 print(x). -> 1040 x = moving_average(x). 1041 if ikey == 0:. 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:80924,integrability,wrap,wrap,80924,"=True,. ---> 20 use_raw=False). 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1038 old_len_x = len(x). 1039 print(x). -> 1040 x = moving_average(x). 1041 if ikey == 0:. 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:81006,integrability,wrap,wrap,81006,"=True,. ---> 20 use_raw=False). 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)). 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1038 old_len_x = len(x). 1039 print(x). -> 1040 x = moving_average(x). 1041 if ikey == 0:. 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a). 980 . 981 def moving_average(a):. --> 982 return _sc_utils.moving_average(a, n_avg). 983 . 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n). 374 An array view storing the moving average. 375 """""". --> 376 ret = np.cumsum(a, dtype=float). 377 ret[n:] = ret[n:] - ret[:-n]. 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out). 2421 . 2422 """""". -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out). 2424 . 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 56 bound = getattr(obj, method, None). 57 if bound is None:. ---> 58 return _wrapit(obj, method, *args, **kwds). 59 . 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds). 45 except AttributeError:. 46 wrap = None. ---> 47 result = getattr(asarray(obj), method)(*args, **kwds). 48 if wrap:. 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:230,interoperability,format,format,230,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:342,interoperability,format,format,342,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:454,interoperability,format,format,454,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:566,interoperability,format,format,566,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:678,interoperability,format,format,678,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:790,interoperability,format,format,790,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:902,interoperability,format,format,902,"@HypoChloremic,. I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1014,interoperability,format,format,1014,"I put the print(x) in l1039 and got similar outcome as shown below:. <details>. <summary> stdout </summary>. [<1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1126,interoperability,format,format,1126,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1238,interoperability,format,format,1238,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1350,interoperability,format,format,1350,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1462,interoperability,format,format,1462,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1574,interoperability,format,format,1574,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1686,interoperability,format,format,1686,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1798,interoperability,format,format,1798,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:1910,interoperability,format,format,1910,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2022,interoperability,format,format,2022,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2134,interoperability,format,format,2134,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2246,interoperability,format,format,2246,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2358,interoperability,format,format,2358,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2470,interoperability,format,format,2470,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2582,interoperability,format,format,2582,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2694,interoperability,format,format,2694,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2806,interoperability,format,format,2806,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:2918,interoperability,format,format,2918,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3030,interoperability,format,format,3030,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3142,interoperability,format,format,3142,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3254,interoperability,format,format,3254,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3366,interoperability,format,format,3366,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3478,interoperability,format,format,3478,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3590,interoperability,format,format,3590,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3702,interoperability,format,format,3702,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3814,interoperability,format,format,3814,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:3926,interoperability,format,format,3926,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4038,interoperability,format,format,4038,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4150,interoperability,format,format,4150,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4262,interoperability,format,format,4262,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4374,interoperability,format,format,4374,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4486,interoperability,format,format,4486,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4598,interoperability,format,format,4598,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4710,interoperability,format,format,4710,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4822,interoperability,format,format,4822,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:4934,interoperability,format,format,4934,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5046,interoperability,format,format,5046,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5158,interoperability,format,format,5158,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5270,interoperability,format,format,5270,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5382,interoperability,format,format,5382,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5494,interoperability,format,format,5494,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5606,interoperability,format,format,5606,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5718,interoperability,format,format,5718,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5830,interoperability,format,format,5830,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:5942,interoperability,format,format,5942,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6054,interoperability,format,format,6054,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6166,interoperability,format,format,6166,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6278,interoperability,format,format,6278,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6390,interoperability,format,format,6390,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6502,interoperability,format,format,6502,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6614,interoperability,format,format,6614,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6726,interoperability,format,format,6726,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6838,interoperability,format,format,6838,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:6950,interoperability,format,format,6950,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7062,interoperability,format,format,7062,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7174,interoperability,format,format,7174,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7286,interoperability,format,format,7286,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7398,interoperability,format,format,7398,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7510,interoperability,format,format,7510,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7622,interoperability,format,format,7622,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7734,interoperability,format,format,7734,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7846,interoperability,format,format,7846,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:7958,interoperability,format,format,7958,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8070,interoperability,format,format,8070,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8182,interoperability,format,format,8182,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8294,interoperability,format,format,8294,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8406,interoperability,format,format,8406,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8518,interoperability,format,format,8518,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8630,interoperability,format,format,8630,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8742,interoperability,format,format,8742,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8854,interoperability,format,format,8854,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:8966,interoperability,format,format,8966,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9078,interoperability,format,format,9078,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9190,interoperability,format,format,9190,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9302,interoperability,format,format,9302,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9414,interoperability,format,format,9414,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9526,interoperability,format,format,9526,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9638,interoperability,format,format,9638,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9750,interoperability,format,format,9750,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9862,interoperability,format,format,9862,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:9974,interoperability,format,format,9974,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10086,interoperability,format,format,10086,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10198,interoperability,format,format,10198,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10310,interoperability,format,format,10310,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10422,interoperability,format,format,10422,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10534,interoperability,format,format,10534,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10646,interoperability,format,format,10646,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10758,interoperability,format,format,10758,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10870,interoperability,format,format,10870,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:10982,interoperability,format,format,10982,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11094,interoperability,format,format,11094,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11206,interoperability,format,format,11206,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11318,interoperability,format,format,11318,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11430,interoperability,format,format,11430,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11542,interoperability,format,format,11542,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11654,interoperability,format,format,11654,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11766,interoperability,format,format,11766,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11878,interoperability,format,format,11878,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:11990,interoperability,format,format,11990,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12102,interoperability,format,format,12102,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12214,interoperability,format,format,12214,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12326,interoperability,format,format,12326,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12438,interoperability,format,format,12438,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12550,interoperability,format,format,12550,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12662,interoperability,format,format,12662,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12774,interoperability,format,format,12774,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12886,interoperability,format,format,12886,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:12998,interoperability,format,format,12998,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13110,interoperability,format,format,13110,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13222,interoperability,format,format,13222,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13334,interoperability,format,format,13334,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13446,interoperability,format,format,13446,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13558,interoperability,format,format,13558,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13670,interoperability,format,format,13670,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13782,interoperability,format,format,13782,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:13894,interoperability,format,format,13894,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14006,interoperability,format,format,14006,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14118,interoperability,format,format,14118,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14230,interoperability,format,format,14230,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14342,interoperability,format,format,14342,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14454,interoperability,format,format,14454,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14566,interoperability,format,format,14566,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14678,interoperability,format,format,14678,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14790,interoperability,format,format,14790,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:14902,interoperability,format,format,14902,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15014,interoperability,format,format,15014,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15126,interoperability,format,format,15126,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15238,interoperability,format,format,15238,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15350,interoperability,format,format,15350,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15462,interoperability,format,format,15462,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15574,interoperability,format,format,15574,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15686,interoperability,format,format,15686,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15798,interoperability,format,format,15798,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:15910,interoperability,format,format,15910,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16022,interoperability,format,format,16022,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16134,interoperability,format,format,16134,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16246,interoperability,format,format,16246,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16358,interoperability,format,format,16358,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16470,interoperability,format,format,16470,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16582,interoperability,format,format,16582,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16694,interoperability,format,format,16694,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16806,interoperability,format,format,16806,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:16918,interoperability,format,format,16918,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17030,interoperability,format,format,17030,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17142,interoperability,format,format,17142,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17254,interoperability,format,format,17254,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17366,interoperability,format,format,17366,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17478,interoperability,format,format,17478,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17590,interoperability,format,format,17590,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17702,interoperability,format,format,17702,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17814,interoperability,format,format,17814,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:17926,interoperability,format,format,17926,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18038,interoperability,format,format,18038,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18150,interoperability,format,format,18150,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18262,interoperability,format,format,18262,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18374,interoperability,format,format,18374,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18486,interoperability,format,format,18486,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18598,interoperability,format,format,18598,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18710,interoperability,format,format,18710,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18822,interoperability,format,format,18822,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:18934,interoperability,format,format,18934,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19046,interoperability,format,format,19046,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19158,interoperability,format,format,19158,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19270,interoperability,format,format,19270,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19382,interoperability,format,format,19382,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19494,interoperability,format,format,19494,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19606,interoperability,format,format,19606,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19718,interoperability,format,format,19718,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19830,interoperability,format,format,19830,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:19942,interoperability,format,format,19942,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20054,interoperability,format,format,20054,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20166,interoperability,format,format,20166,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20278,interoperability,format,format,20278,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20390,interoperability,format,format,20390,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20502,interoperability,format,format,20502,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20614,interoperability,format,format,20614,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20726,interoperability,format,format,20726,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20838,interoperability,format,format,20838,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:20950,interoperability,format,format,20950,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21062,interoperability,format,format,21062,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21174,interoperability,format,format,21174,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21286,interoperability,format,format,21286,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21398,interoperability,format,format,21398,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21510,interoperability,format,format,21510,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21622,interoperability,format,format,21622,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21734,interoperability,format,format,21734,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21846,interoperability,format,format,21846,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:21958,interoperability,format,format,21958,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22070,interoperability,format,format,22070,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22182,interoperability,format,format,22182,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22294,interoperability,format,format,22294,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22406,interoperability,format,format,22406,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22518,interoperability,format,format,22518,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22630,interoperability,format,format,22630,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22742,interoperability,format,format,22742,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22854,interoperability,format,format,22854,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:22966,interoperability,format,format,22966,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23078,interoperability,format,format,23078,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23190,interoperability,format,format,23190,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23302,interoperability,format,format,23302,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23414,interoperability,format,format,23414,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23526,interoperability,format,format,23526,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23638,interoperability,format,format,23638,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23750,interoperability,format,format,23750,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23862,interoperability,format,format,23862,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:23974,interoperability,format,format,23974,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24086,interoperability,format,format,24086,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24198,interoperability,format,format,24198,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24310,interoperability,format,format,24310,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24422,interoperability,format,format,24422,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24534,interoperability,format,format,24534,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24646,interoperability,format,format,24646,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24758,interoperability,format,format,24758,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24870,interoperability,format,format,24870,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:24982,interoperability,format,format,24982,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25094,interoperability,format,format,25094,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25206,interoperability,format,format,25206,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25318,interoperability,format,format,25318,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25430,interoperability,format,format,25430,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25542,interoperability,format,format,25542,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25654,interoperability,format,format,25654,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25766,interoperability,format,format,25766,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25878,interoperability,format,format,25878,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:25990,interoperability,format,format,25990,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26102,interoperability,format,format,26102,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26214,interoperability,format,format,26214,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26326,interoperability,format,format,26326,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26438,interoperability,format,format,26438,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26550,interoperability,format,format,26550,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26662,interoperability,format,format,26662,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26774,interoperability,format,format,26774,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26886,interoperability,format,format,26886,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:26998,interoperability,format,format,26998,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27110,interoperability,format,format,27110,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27222,interoperability,format,format,27222,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27334,interoperability,format,format,27334,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27446,interoperability,format,format,27446,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27558,interoperability,format,format,27558,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27670,interoperability,format,format,27670,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27782,interoperability,format,format,27782,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:27894,interoperability,format,format,27894,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28006,interoperability,format,format,28006,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28118,interoperability,format,format,28118,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28230,interoperability,format,format,28230,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28342,interoperability,format,format,28342,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28454,interoperability,format,format,28454,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28566,interoperability,format,format,28566,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28678,interoperability,format,format,28678,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28790,interoperability,format,format,28790,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:28902,interoperability,format,format,28902,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:29014,interoperability,format,format,29014,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
https://github.com/scverse/scanpy/issues/1168:29126,interoperability,format,format,29126,"x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 0 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse Row format>, <1x1 sparse matrix of type '<class 'numpy.float32'>'. 	with 1 stored elements in Compressed Sparse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168
