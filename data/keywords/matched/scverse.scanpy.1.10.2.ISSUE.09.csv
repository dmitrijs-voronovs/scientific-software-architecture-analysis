id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/900:832,safety,log,logging,832,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:850,safety,log,logging,850,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:952,safety,log,logging,952,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:967,safety,modul,module,967,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1036,safety,log,logging,1036,"ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.sile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1111,safety,modul,module,1111,"40> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _regi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1358,safety,modul,module,1358,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1581,safety,modul,module,1581,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1814,safety,modul,module,1814,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2014,safety,modul,module,2014,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:628,security,log,logging,628,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:639,security,log,logg,639,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:832,security,log,logging,832,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:850,security,log,logging,850,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:952,security,log,logging,952,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1036,security,log,logging,1036,"ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.sile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:52,testability,Trace,Traceback,52,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:628,testability,log,logging,628,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:639,testability,log,logg,639,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:832,testability,log,logging,832,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:850,testability,log,logging,850,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:952,testability,log,logging,952,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1036,testability,log,logging,1036,"ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.sile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:96,usability,input,input-,96,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/pull/902:6,interoperability,specif,specification,6,"Allow specification of mitochondrial chromosome when retrieving mitochondrial genes; Allow specification of mitochondrial chromosome when retrieving mitochondrial. genes as some organisms use a different chromosome name than ""MT"".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/902
https://github.com/scverse/scanpy/pull/902:91,interoperability,specif,specification,91,"Allow specification of mitochondrial chromosome when retrieving mitochondrial genes; Allow specification of mitochondrial chromosome when retrieving mitochondrial. genes as some organisms use a different chromosome name than ""MT"".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/902
https://github.com/scverse/scanpy/pull/903:83,deployability,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:383,energy efficiency,reduc,reduction,383,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:477,energy efficiency,power,powerful,477,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:83,integrability,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:520,integrability,sub,subtle,520,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:621,integrability,pub,publication,621,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:83,interoperability,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:83,modifiability,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:256,modifiability,variab,variability,256,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:310,modifiability,Extens,Extensive,310,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:83,reliability,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:702,safety,test,test,702,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:83,security,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:83,testability,integr,integrate,83,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/pull/903:702,testability,test,test,702,Adding the Self-Assembling Manifold algorithm to scanpy; This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:. SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903
https://github.com/scverse/scanpy/issues/904:53,energy efficiency,draw,draws,53,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:61,energy efficiency,heat,heatmap,61,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:125,energy efficiency,heat,heatmap,125,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:237,energy efficiency,draw,draw,237,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:579,energy efficiency,heat,heatmap,579,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:93,integrability,compon,component,93,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:93,interoperability,compon,component,93,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:93,modifiability,compon,component,93,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:476,usability,user,user-images,476,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:659,usability,visual,visualizing-marker-genes,659,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/904:708,usability,user,user-images,708,"dimheatmap equlvalence; Dear,. Seurat's dimheatmap() draws a heatmap focusing on a principal component, while scanpy's sc.pl.heatmap focusing on the gene expression matrix. Could you develop a similar function ? I find it's very easy to draw a nice heamap with dimheatmap(), while it is hard in scany as the expression noise. here is the link of dimheatmap:. https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html. And, here is the comparison ( dimheatmap):. ![image](https://user-images.githubusercontent.com/29703450/68374317-a3df0d80-017f-11ea-8a7b-5a16204c5cd3.png). here is heatmap genereated by scanpy (https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html). ![image](https://user-images.githubusercontent.com/29703450/68374417-dab52380-017f-11ea-9775-79a9a443bdd5.png). Obviously, it's more nice with Seurat's dimheatmap().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/904
https://github.com/scverse/scanpy/issues/905:501,deployability,log,log,501,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:643,deployability,log,log,643,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:669,deployability,log,log,669,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:694,deployability,log,log,694,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:718,deployability,scale,scale,718,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:733,deployability,scale,scale,733,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1186,deployability,log,log,1186,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1344,deployability,pipelin,pipelines,1344,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1359,deployability,pipelin,pipeline,1359,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:718,energy efficiency,scale,scale,718,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:733,energy efficiency,scale,scale,733,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:559,integrability,sub,subset,559,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:629,integrability,filter,filtering,629,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:673,integrability,transform,transform,673,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1344,integrability,pipelin,pipelines,1344,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1359,integrability,pipelin,pipeline,1359,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:673,interoperability,transform,transform,673,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1293,interoperability,standard,standard,1293,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:429,modifiability,variab,variable,429,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:718,modifiability,scal,scale,718,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:733,modifiability,scal,scale,733,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:718,performance,scale,scale,718,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:733,performance,scale,scale,733,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:998,performance,perform,performed,998,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:117,reliability,doe,does,117,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:501,safety,log,log,501,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:643,safety,log,log,643,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:669,safety,log,log,669,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:694,safety,log,log,694,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:936,safety,detect,detected,936,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1186,safety,log,log,1186,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:501,security,log,log,501,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:643,security,log,log,643,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:669,security,log,log,669,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:694,security,log,log,694,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:936,security,detect,detected,936,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1186,security,log,log,1186,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1262,security,sign,signal,1262,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:501,testability,log,log,501,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:643,testability,log,log,643,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:669,testability,log,log,669,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:694,testability,log,log,694,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:742,testability,unit,unit,742,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:1186,testability,log,log,1186,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/905:998,usability,perform,performed,998,"sc.pp.recipe_zheng17 function is different from the Zheng et al. (2017) paper; **The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**. ```. sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count. sc.pp.normalize_per_cell( # normalize with total UMI count per cell. adata, key_n_counts='n_counts_all'). filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes. adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False). adata = adata[:, filter_result.gene_subset] # subset the genes. sc.pp.normalize_per_cell(adata) # renormalize after filtering. if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1). sc.pp.scale(adata) # scale to unit variance and shift to zero mean. ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**. Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905
https://github.com/scverse/scanpy/issues/906:131,availability,cluster,clusters,131,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:217,availability,cluster,cluster,217,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:266,availability,cluster,cluster,266,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:333,availability,cluster,cluster,333,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:395,availability,cluster,cluster,395,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:438,availability,cluster,cluster,438,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:493,availability,cluster,cluster,493,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:521,availability,cluster,cluster,521,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:131,deployability,cluster,clusters,131,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:217,deployability,cluster,cluster,217,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:266,deployability,cluster,cluster,266,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:333,deployability,cluster,cluster,333,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:395,deployability,cluster,cluster,395,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:438,deployability,cluster,cluster,438,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:493,deployability,cluster,cluster,493,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:521,deployability,cluster,cluster,521,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:594,usability,help,helps,594,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/issues/906:607,usability,user,user,607,"plot order; Dear,. sc.pl.umap() and other related functions seem to plot the points in a certain order. For example, If I have two clusters A and B, then sc.pl.umap(adata,color=['leiden'],groups='A') would would plot cluster A first in blue color, leaving points of cluster B as grey color, this is perfect. However, if I wanna show cluster B with sc.pl.umap(adata,color=['leiden'],groups='B'), cluster A will be shown in grey color, and cluster B will be shown in yellow color, but points of cluster A will overlap with cluster B, as a result, some points of B are blocked by A. So, it's very helps if the user can set the display order mannually.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906
https://github.com/scverse/scanpy/pull/908:4,security,immut,immutable,4,Use immutable defaults;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/908
https://github.com/scverse/scanpy/pull/910:223,safety,test,test,223,Fix paga root; Paga plotting had some half-finished support for supplying a root by its label. This is supposed to finish this. Fixes #909. @falexwolf needs to say if this corresponds to his intentions and we need to add a test.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/910
https://github.com/scverse/scanpy/pull/910:223,testability,test,test,223,Fix paga root; Paga plotting had some half-finished support for supplying a root by its label. This is supposed to finish this. Fixes #909. @falexwolf needs to say if this corresponds to his intentions and we need to add a test.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/910
https://github.com/scverse/scanpy/pull/910:52,usability,support,support,52,Fix paga root; Paga plotting had some half-finished support for supplying a root by its label. This is supposed to finish this. Fixes #909. @falexwolf needs to say if this corresponds to his intentions and we need to add a test.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/910
https://github.com/scverse/scanpy/issues/911:32,deployability,API,API,32,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:32,integrability,API,API,32,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:12,interoperability,Coordinat,Coordinates,12,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:32,interoperability,API,API,32,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:51,interoperability,coordinat,coordinates,51,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:252,reliability,doe,doesn,252,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/911:0,security,Access,Access,0,"Access UMAP Coordinates through API?; Are the UMAP coordinates stored anywhere in the object? I couldn't find it, so I ended up running `umap` with the `show=False` and then manually extracting it from the matplotlib Axes object. If such functionality doesn't exist, then I suppose you can consider this a feature request.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/911
https://github.com/scverse/scanpy/issues/912:155,energy efficiency,current,currently,155,"plotting overlay of 2 genes on tsne; Hi guys,. I would like to use the sc.pl.tsne to color the TSNE with the expression of 2 genes but in the same plot... currently i ran sc.pl.tsne(adata, color=['geneA','geneB']). This gives 2 tsne's but i would like to visualize things in just one. thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/912:255,usability,visual,visualize,255,"plotting overlay of 2 genes on tsne; Hi guys,. I would like to use the sc.pl.tsne to color the TSNE with the expression of 2 genes but in the same plot... currently i ran sc.pl.tsne(adata, color=['geneA','geneB']). This gives 2 tsne's but i would like to visualize things in just one. thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/912
https://github.com/scverse/scanpy/issues/913:93,deployability,version,version,93,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:193,energy efficiency,cpu,cpus,193,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:357,energy efficiency,core,core,357,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2430,energy efficiency,cpu,cpu,2430,"/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:93,integrability,version,version,93,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:575,integrability,transform,transformation,575,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1179,integrability,transform,transformation,1179,"ith 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1826,integrability,transform,transformation,1826,"-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2637,integrability,transform,transformation,2637,"dates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3242,integrability,transform,transformation,3242,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:558,interoperability,specif,specified,558,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:575,interoperability,transform,transformation,575,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1162,interoperability,specif,specified,1162,"u virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1179,interoperability,transform,transformation,1179,"ith 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1809,interoperability,specif,specified,1809,"/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnos",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1826,interoperability,transform,transformation,1826,"-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2620,interoperability,specif,specified,2620,"eighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there an",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2637,interoperability,transform,transformation,2637,"dates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3225,interoperability,specif,specified,3225,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3242,interoperability,transform,transformation,3242,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:93,modifiability,version,version,93,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:458,modifiability,pac,packages,458,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:834,modifiability,pac,packages,834,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1063,modifiability,pac,packages,1063,"scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1438,modifiability,pac,packages,1438,"/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and giv",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1709,modifiability,pac,packages,1709,"a-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try tur",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2085,modifiability,pac,packages,2085,":92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/minico",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2520,modifiability,pac,packages,2520,"urrent_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2896,modifiability,pac,packages,2896,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3125,modifiability,pac,packages,3125,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3501,modifiability,pac,packages,3501,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3,performance,parallel,parallelization,3,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:193,performance,cpu,cpus,193,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:539,performance,parallel,parallel,539,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:594,performance,parallel,parallel,594,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:659,performance,parallel,parallel,659,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:731,performance,parallel,parallel,731,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:911,performance,parallel,parallel,911,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1143,performance,parallel,parallel,1143,"y 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=Tr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1198,performance,parallel,parallel,1198,"the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1263,performance,parallel,parallel,1263,". ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_ba",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1335,performance,parallel,parallel,1335,"cally uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1486,performance,parallel,parallel,1486,"NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1790,performance,parallel,parallel,1790,"niconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1845,performance,parallel,parallel,1845,"p_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../..",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1910,performance,parallel,parallel,1910,"allel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1982,performance,parallel,parallel,1982,"ate):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2136,performance,parallel,parallel,2136,"t 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2430,performance,cpu,cpu,2430,"/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2601,performance,parallel,parallel,2601,"aph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2656,performance,parallel,parallel,2656,". /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2721,performance,parallel,parallel,2721,"ompiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2793,performance,parallel,parallel,2793,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2973,performance,parallel,parallel,2973,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3206,performance,parallel,parallel,3206,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3261,performance,parallel,parallel,3261,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3326,performance,parallel,parallel,3326,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3398,performance,parallel,parallel,3398,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3552,performance,parallel,parallel,3552,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3658,performance,parallel,parallelization,3658,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3726,performance,bottleneck,bottleneck,3726,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:668,reliability,diagno,diagnostics,668,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:745,reliability,diagno,diagnostics,745,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1272,reliability,diagno,diagnostics,1272,"p.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('thre",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1349,reliability,diagno,diagnostics,1349,"1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1919,reliability,diagno,diagnostics,1919,". def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1996,reliability,diagno,diagnostics,1996,"unc_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2730,reliability,diagno,diagnostics,2730,"602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2807,reliability,diagno,diagnostics,2807,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3335,reliability,diagno,diagnostics,3335,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3412,reliability,diagno,diagnostics,3412,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:668,testability,diagno,diagnostics,668,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:745,testability,diagno,diagnostics,745,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1272,testability,diagno,diagnostics,1272,"p.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('thre",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1349,testability,diagno,diagnostics,1349,"1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1919,testability,diagno,diagnostics,1919,". def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1996,testability,diagno,diagnostics,1996,"unc_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2730,testability,diagno,diagnostics,2730,"602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2807,testability,diagno,diagnostics,2807,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3335,testability,diagno,diagnostics,3335,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3412,testability,diagno,diagnostics,3412,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:726,usability,user,user,726,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:761,usability,help,help,761,"No parallelization during nearest neighbors computation; I'm using scanpy with the following version. - scanpy 1.4.4.post1 . - numba 0.45.1. - numpy 1.17.2. on a Ubuntu virtual machine with 16 cpus. In the example data below, I have 47933 cells  41 genes. When I run. ```. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. it empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1330,usability,user,user,1330," empirically uses only 1 core and takes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1365,usability,help,help,1365,"akes 1 min 35s and I get the warning:. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:1977,usability,user,user,1977," rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parall",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2012,usability,help,help,2012,". /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_proj",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2788,usability,user,user,2788,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:2823,usability,help,help,2823,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3393,usability,user,user,3393,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3428,usability,help,help,3428,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/913:3627,usability,tip,tips,3627,"anceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. when I run. ```. from joblib import parallel_backend. with parallel_backend('threading', n_jobs=15):. sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12). ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:. @numba.njit(fastmath=True, nogil=True, parallel=True). def euclidean_random_projection_split(data, indices, rng_state):. ^. self.func_ir.loc)). /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913
https://github.com/scverse/scanpy/issues/914:879,availability,Cluster,Clustering,879,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:879,deployability,Cluster,Clustering,879,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:648,safety,input,input,648,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:11,security,Ident,Identities,11,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:244,security,ident,identifiers,244,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:1977,security,ident,identify,1977,"Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by day after they have been combined into one anndata object. I don't think this is causing the issue, but if that is part of it, then please let me know if there is a work around.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:356,usability,user,user-images,356,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/issues/914:648,usability,input,input,648,"Gene Names/Identities are showing up as numbers when plotted with highest_expr_genes(); When processing the data in Scanpy I am unable to figure out why my plot of the Highest Expressed Genes shows up with numbers rather than gene names as the identifiers on the Y-axis. . Example:. <img width=""786"" alt=""Screen Shot 2019-11-12 at 5 16 15 PM"" src=""https://user-images.githubusercontent.com/9083834/68719284-411bb680-0571-11ea-8666-ce427fce74df.png"">. I am worried that it may not be reading our file properly, but even when I converted from a txt file to a .loom file I still got the same problem. I am unsure if this is an issue with our original input or if something it is being read improperly. I will include some of my code below, and if there is somewhere you would like me to send copies of raw data to run it on let me know where to send it. I am using the tutorial for Clustering on the Scanpy website as the basis for my code. . ```py. sc.settings.set_figure_params(dpi = 80). day00a = sc.read_text(""/alex_ryan/D0.1500.dge"", first_column_names = True, delimiter = ""\t""). day01 = sc.read_text(""/alex_ryan/D1.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day02 = sc.read_text(""/alex_ryan/D2.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day04 = sc.read_text(""/alex_ryan/D4.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day09 = sc.read_text(""/alex_ryan/D9.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day11 = sc.read_text(""/alex_ryan/D11.txt.500.dge"", first_column_names = True, delimiter = ""\t""). day00a.obs['tech'] = 'Day 0'. day01.obs['tech'] = 'Day 1'. day02.obs['tech'] = 'Day 2'. day04.obs['tech'] = 'Day 4'. day09.obs['tech'] = 'Day 9'. day11.obs['tech'] = 'Day 11'. adata_list = [day01, day02, day04, day09, day11]. adata2 = day00a.concatenate(adata_list, join = 'outer'). adata = adata2. sc.pl.highest_expr_genes(adata, n_top = 20, save = True). ```. I am adding the extra .obs 'tech' tag so that I can identify the cells by d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914
https://github.com/scverse/scanpy/pull/915:498,availability,avail,available,498,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:743,availability,avail,available,743,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:797,availability,avail,available,797,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:13,deployability,modul,module,13,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:79,deployability,modul,module,79,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:411,deployability,modul,modularity,411,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:454,deployability,modul,module,454,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:561,deployability,api,api,561,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:585,deployability,modul,module,585,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1807,deployability,API,API,1807,"ut have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubuserconte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2500,deployability,log,logcounts,2500,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2576,deployability,log,logcounts,2576,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:285,energy efficiency,measur,measurements,285,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:553,energy efficiency,current,current,553,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1294,energy efficiency,heat,heatmap,1294," I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2145,energy efficiency,measur,measure,2145,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2304,energy efficiency,measur,measure,2304,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2591,energy efficiency,CPU,CPU,2591,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2935,energy efficiency,reduc,reductions,2935,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:257,integrability,transform,transformers,257,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:411,integrability,modular,modularity,411,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:561,integrability,api,api,561,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:827,integrability,interfac,interfaces,827,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1807,integrability,API,API,1807,"ut have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubuserconte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1964,integrability,wrap,wrapping,1964," on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: htt",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2906,integrability,compon,components,2906,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:257,interoperability,transform,transformers,257,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:561,interoperability,api,api,561,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:827,interoperability,interfac,interfaces,827,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1807,interoperability,API,API,1807,"ut have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubuserconte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2906,interoperability,compon,components,2906,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:13,modifiability,modul,module,13,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:79,modifiability,modul,module,79,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:411,modifiability,modul,modularity,411,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:454,modifiability,modul,module,454,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:585,modifiability,modul,module,585,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:827,modifiability,interfac,interfaces,827,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2492,modifiability,layer,layers,2492,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2569,modifiability,layer,layer,2569,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2906,modifiability,compon,components,2906,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2158,performance,network,network,2158,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2527,performance,time,time,2527,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2591,performance,CPU,CPU,2591,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2595,performance,time,times,2595,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2651,performance,time,time,2651,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:498,reliability,availab,available,498,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:743,reliability,availab,available,743,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:797,reliability,availab,available,797,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2373,reliability,pra,practice,2373,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:13,safety,modul,module,13,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:79,safety,modul,module,79,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:411,safety,modul,modularity,411,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:454,safety,modul,module,454,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:498,safety,avail,available,498,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:585,safety,modul,module,585,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:743,safety,avail,available,743,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:797,safety,avail,available,797,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2500,safety,log,logcounts,2500,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2576,safety,log,logcounts,2576,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:128,security,modif,modify,128,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:498,security,availab,available,498,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:743,security,availab,available,743,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:797,security,availab,available,797,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2158,security,network,network,2158,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2408,security,ident,identifying,2408,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2500,security,log,logcounts,2500,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2576,security,log,logcounts,2576,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:411,testability,modula,modularity,411,"`sc.metrics` module (add confusion matrix & Geary's C methods); This PR adds a module `sc.metrics` for functions which wouldn't modify an anndata object, but are useful calculations. I'm basing this on `sklearn.metrics`, namely, how `sklearn` has separated transformers (`sc.tl`) from measurements. I've started it with two functions, `confusion_matrix` and `gearys_c` but think there are more use cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2500,testability,log,logcounts,2500,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2576,testability,log,logcounts,2576,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1389,usability,user,user-images,1389,"se cases (e.g. `modularity`). I'm open to this not being a module, but I think these methods should be available and I'm not sure where they'd fit within the current api. My vision for this module is to make it easier to calculate values based on values you'd get using the scanpy ecosystem. Methods that would be included would be either *a)* not available in other libraries (`gearys_c`) or *b)* are available, but have difficult interfaces (`confusion_matrix`). ## `sc.metrics.confusion_matrix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:1883,usability,feedback,feedback,1883,"trix`. Creates a confusion matrix for comparing categorical labels. This is based on `sklearn.metrics.confusion_matrix` but is easier to use, and returns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2602,usability,user,user,2602,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/pull/915:2782,usability,user,user-images,2782,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python. import scanpy as sc. import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(). sc.tl.leiden(pbmc). sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)). ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python. sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]). ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python. import numpy as np. pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""). # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms. # Wall time: 74.9 ms. to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]. sc.pl.umap(pbmc, color=to_plot, ncols=2). ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915
https://github.com/scverse/scanpy/issues/916:421,energy efficiency,GPU,GPU,421,"Remove zappy; [Zappy](https://github.com/lasersonlab/zappy) is an experimental library for distributed processing of chunked NumPy arrays on engines like Pywren, Apache Spark, and Apache Beam. Zappy is not being actively maintained and won't be in the future, so we should consider removing it from Scanpy. Scanpy can use Dask for multithreaded/distributed processing, as well as RAPIDS for running graph algorithms on a GPU.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/916
https://github.com/scverse/scanpy/issues/916:91,interoperability,distribut,distributed,91,"Remove zappy; [Zappy](https://github.com/lasersonlab/zappy) is an experimental library for distributed processing of chunked NumPy arrays on engines like Pywren, Apache Spark, and Apache Beam. Zappy is not being actively maintained and won't be in the future, so we should consider removing it from Scanpy. Scanpy can use Dask for multithreaded/distributed processing, as well as RAPIDS for running graph algorithms on a GPU.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/916
https://github.com/scverse/scanpy/issues/916:345,interoperability,distribut,distributed,345,"Remove zappy; [Zappy](https://github.com/lasersonlab/zappy) is an experimental library for distributed processing of chunked NumPy arrays on engines like Pywren, Apache Spark, and Apache Beam. Zappy is not being actively maintained and won't be in the future, so we should consider removing it from Scanpy. Scanpy can use Dask for multithreaded/distributed processing, as well as RAPIDS for running graph algorithms on a GPU.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/916
https://github.com/scverse/scanpy/issues/916:221,modifiability,maintain,maintained,221,"Remove zappy; [Zappy](https://github.com/lasersonlab/zappy) is an experimental library for distributed processing of chunked NumPy arrays on engines like Pywren, Apache Spark, and Apache Beam. Zappy is not being actively maintained and won't be in the future, so we should consider removing it from Scanpy. Scanpy can use Dask for multithreaded/distributed processing, as well as RAPIDS for running graph algorithms on a GPU.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/916
https://github.com/scverse/scanpy/issues/916:421,performance,GPU,GPU,421,"Remove zappy; [Zappy](https://github.com/lasersonlab/zappy) is an experimental library for distributed processing of chunked NumPy arrays on engines like Pywren, Apache Spark, and Apache Beam. Zappy is not being actively maintained and won't be in the future, so we should consider removing it from Scanpy. Scanpy can use Dask for multithreaded/distributed processing, as well as RAPIDS for running graph algorithms on a GPU.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/916
https://github.com/scverse/scanpy/issues/916:221,safety,maintain,maintained,221,"Remove zappy; [Zappy](https://github.com/lasersonlab/zappy) is an experimental library for distributed processing of chunked NumPy arrays on engines like Pywren, Apache Spark, and Apache Beam. Zappy is not being actively maintained and won't be in the future, so we should consider removing it from Scanpy. Scanpy can use Dask for multithreaded/distributed processing, as well as RAPIDS for running graph algorithms on a GPU.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/916
https://github.com/scverse/scanpy/pull/917:201,availability,state,state,201,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:117,deployability,releas,released,117,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:173,energy efficiency,core,cores,173,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:201,integrability,state,state,201,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:17,performance,parallel,parallel,17,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:6,security,control,control,6,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/pull/917:6,testability,control,control,6,"Allow control of parallel in simplicial_set_embedding; This should not be merged until Scanpy uses UMAP 0.4 (not yet released). It allows UMAP to take advantage of multiple cores by setting the random state to `None`:. ```python. sc.tl.umap(adata, random_state=None). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917
https://github.com/scverse/scanpy/issues/918:477,availability,Error,Error,477,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:566,availability,error,error,566,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:583,deployability,Version,Versions,583,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:616,deployability,log,logging,616,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:583,integrability,Version,Versions,583,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:155,modifiability,paramet,parameter,155,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:583,modifiability,Version,Versions,583,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:477,performance,Error,Error,477,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:566,performance,error,error,566,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:477,safety,Error,Error,477,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:566,safety,error,error,566,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:616,safety,log,logging,616,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:616,security,log,logging,616,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:616,testability,log,logging,616,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:75,usability,clear,clear,75,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:223,usability,user,user-images,223,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:329,usability,minim,minimal,329,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:477,usability,Error,Error,477,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:566,usability,error,error,566,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/918:745,usability,learn,learn,745,"When using umap with init_pos=paga, got strange result; <!-- Please give a clear and concise description of what the bug is: -->. When i use umap with the parameter init_pos='paga', I got a strange result. ![image](https://user-images.githubusercontent.com/20806068/68834273-5703b580-06f0-11ea-9d05-76a66ea9e943.png). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. sc.pl.umap(adata,color='louvain'). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. no error. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.3 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918
https://github.com/scverse/scanpy/issues/919:36,safety,test,test,36,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:93,safety,test,test,93,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:138,safety,test,tests,138,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:175,safety,test,test,175,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:36,testability,test,test,36,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:93,testability,test,test,93,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:138,testability,test,tests,138,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/919:175,testability,test,test,175,"rank_genes_groups one- or two-sided test; Hi! Is it possible to do two-sided (or two-tailed) test in rank_genes_groups? It seems like the tests you can choose from for now (t-test, wilcoxon etc.) are one-sided in that comparing group A to group B produces a list of DE genes that is not the same as in comparison of B to A. Thank you. Sincerely,. Anna Arutyunyan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919
https://github.com/scverse/scanpy/issues/920:60,availability,cluster,clustering,60,Problem when using sc.tl.louvain; Some kind of problem when clustering the neighbourhood graph with louvain. Some collisions with the igraph project. ![Captura](https://user-images.githubusercontent.com/55575784/68854040-89ef7e80-06db-11ea-8994-0bef86e231c6.PNG).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/920:60,deployability,cluster,clustering,60,Problem when using sc.tl.louvain; Some kind of problem when clustering the neighbourhood graph with louvain. Some collisions with the igraph project. ![Captura](https://user-images.githubusercontent.com/55575784/68854040-89ef7e80-06db-11ea-8994-0bef86e231c6.PNG).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/920:169,usability,user,user-images,169,Problem when using sc.tl.louvain; Some kind of problem when clustering the neighbourhood graph with louvain. Some collisions with the igraph project. ![Captura](https://user-images.githubusercontent.com/55575784/68854040-89ef7e80-06db-11ea-8994-0bef86e231c6.PNG).,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920
https://github.com/scverse/scanpy/issues/921:276,availability,operat,operations,276,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:598,availability,slo,slower,598,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1005,availability,operat,operations,1005,"e Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1514,availability,operat,operation,1514,"data sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2630,availability,operat,operations,2630," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3328,availability,slo,slowed,3328," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3335,availability,down,down,3335," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3175,deployability,fail,fails,3175," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:136,energy efficiency,CPU,CPUs,136,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:145,energy efficiency,GPU,GPUs,145,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:210,energy efficiency,core,cores,210,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:258,energy efficiency,GPU,GPUs,258,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:345,energy efficiency,GPU,GPUs,345,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1056,energy efficiency,core,cores,1056,"o speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Da",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1247,energy efficiency,core,cores,1247,"Py (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2018,energy efficiency,core,cores,2018,"to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2374,energy efficiency,GPU,GPUs,2374,"order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2504,energy efficiency,GPU,GPU,2504," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:734,integrability,wrap,wrapper,734,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:896,integrability,protocol,protocol,896,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2396,integrability,wrap,wrapper,2396," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2987,integrability,sub,subset,2987," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3042,integrability,sub,subset,3042," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3086,integrability,sub,subset,3086," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3103,integrability,sub,subset,3103," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:734,interoperability,wrapper,wrapper,734,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:896,interoperability,protocol,protocol,896,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2396,interoperability,wrapper,wrapper,2396," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1332,modifiability,interm,intermediate,1332,"k with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:136,performance,CPU,CPUs,136,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:145,performance,GPU,GPUs,145,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:258,performance,GPU,GPUs,258,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:345,performance,GPU,GPUs,345,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2184,performance,overhead,overhead,2184,"lways faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for row",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2374,performance,GPU,GPUs,2374,"order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2504,performance,GPU,GPU,2504," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:598,reliability,slo,slower,598,"Investigate Dask for speeding up Zheng17; Can we use Dask to speed up the preprocessing phase of Scanpy by taking advantage of multiple CPUs (or GPUs)? **TLDR**: Dask can speed up Zheng17, but it needs lots of cores and a rewritten implementation. CuPy (for GPUs) has missing operations required by Zheng17, so more work is needed for Dask with GPUs. ### Investigation. Dask is mainly used with dense arrays, however the arrays in Scanpy are sparse (for most of the preprocessing phase at least). I tried looking at [pydata sparse](https://sparse.pydata.org/en/latest/) with Dask, but it ran a lot slower than regular [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neuron",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2960,reliability,doe,does,2960," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3175,reliability,fail,fails,3175," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3328,reliability,slo,slowed,3328," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1626,safety,avoid,avoid,1626,"rse`](https://docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1637,safety,compl,complication,1637,"docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations wo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:1637,security,compl,complication,1637,"docs.scipy.org/doc/scipy/reference/sparse.html) (which is what Scanpy uses). So I wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations wo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2119,security,sign,significant,2119,"st tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.ar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2770,security,sign,significant,2770," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:3340,security,sign,significantly,3340," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/921:2998,usability,support,supported,2998," Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,. to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code. See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs. I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it cant be used for Zheng17 yet. It would require significant work in CuPy to get it working:. * `multiply` - not implemented by `cupyx.scipy.sparse.csr.csr_matrix`. * `mean` - no method on `cupyx.scipy.sparse.csr.csr_matrix` (note that it does have `sum`). * column subset not supported, e.g. `xs[:, 1:3]` (note that row subset is). * boolean indexing, i.e. `xs[:, subset]`, where `subset` is e.g. `np.array([True, False, True, False, True])`; note this fails for rows too. #### NumPy 1.16 vs NumPy 1.17. I used NumPy 1.16 for the above experiments. However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921
https://github.com/scverse/scanpy/issues/922:619,deployability,automat,automatically,619,"Create links between scanpy_tutorials and scanpy docs; https://scVelo.rtfd.io has links to its tutorials as external links in the sidebar, pretty nice! @VolkerBergen did something cool with his scvelo_tutorials repo, by including it into the TOC tree of the main repo and vice versa:. scvelo.rtfd.io | scvelo-notebooks.rtfd.io. -------------- | ------------------------. ![scvelo](https://user-images.githubusercontent.com/291575/68943962-f25a6080-07ac-11ea-9a0f-c5e424cba91c.png) | ![scvelo-notebooks](https://user-images.githubusercontent.com/291575/68944029-161da680-07ad-11ea-8d36-191fbd0bf8aa.png). We can do that automatically by using intersphinx!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:180,energy efficiency,cool,cool,180,"Create links between scanpy_tutorials and scanpy docs; https://scVelo.rtfd.io has links to its tutorials as external links in the sidebar, pretty nice! @VolkerBergen did something cool with his scvelo_tutorials repo, by including it into the TOC tree of the main repo and vice versa:. scvelo.rtfd.io | scvelo-notebooks.rtfd.io. -------------- | ------------------------. ![scvelo](https://user-images.githubusercontent.com/291575/68943962-f25a6080-07ac-11ea-9a0f-c5e424cba91c.png) | ![scvelo-notebooks](https://user-images.githubusercontent.com/291575/68944029-161da680-07ad-11ea-8d36-191fbd0bf8aa.png). We can do that automatically by using intersphinx!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:619,testability,automat,automatically,619,"Create links between scanpy_tutorials and scanpy docs; https://scVelo.rtfd.io has links to its tutorials as external links in the sidebar, pretty nice! @VolkerBergen did something cool with his scvelo_tutorials repo, by including it into the TOC tree of the main repo and vice versa:. scvelo.rtfd.io | scvelo-notebooks.rtfd.io. -------------- | ------------------------. ![scvelo](https://user-images.githubusercontent.com/291575/68943962-f25a6080-07ac-11ea-9a0f-c5e424cba91c.png) | ![scvelo-notebooks](https://user-images.githubusercontent.com/291575/68944029-161da680-07ad-11ea-8d36-191fbd0bf8aa.png). We can do that automatically by using intersphinx!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:389,usability,user,user-images,389,"Create links between scanpy_tutorials and scanpy docs; https://scVelo.rtfd.io has links to its tutorials as external links in the sidebar, pretty nice! @VolkerBergen did something cool with his scvelo_tutorials repo, by including it into the TOC tree of the main repo and vice versa:. scvelo.rtfd.io | scvelo-notebooks.rtfd.io. -------------- | ------------------------. ![scvelo](https://user-images.githubusercontent.com/291575/68943962-f25a6080-07ac-11ea-9a0f-c5e424cba91c.png) | ![scvelo-notebooks](https://user-images.githubusercontent.com/291575/68944029-161da680-07ad-11ea-8d36-191fbd0bf8aa.png). We can do that automatically by using intersphinx!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/issues/922:511,usability,user,user-images,511,"Create links between scanpy_tutorials and scanpy docs; https://scVelo.rtfd.io has links to its tutorials as external links in the sidebar, pretty nice! @VolkerBergen did something cool with his scvelo_tutorials repo, by including it into the TOC tree of the main repo and vice versa:. scvelo.rtfd.io | scvelo-notebooks.rtfd.io. -------------- | ------------------------. ![scvelo](https://user-images.githubusercontent.com/291575/68943962-f25a6080-07ac-11ea-9a0f-c5e424cba91c.png) | ![scvelo-notebooks](https://user-images.githubusercontent.com/291575/68944029-161da680-07ad-11ea-8d36-191fbd0bf8aa.png). We can do that automatically by using intersphinx!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/922
https://github.com/scverse/scanpy/pull/923:800,deployability,contain,contains,800,"WIP Links to tutorials; This defines a directive to include other TOCs into your own using intersphinx:. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/conf.py#L90-L93. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/index.rst#L22-L23. ![grafik](https://user-images.githubusercontent.com/291575/68944214-a0660a80-07ad-11ea-8e13-6f7c92732633.png). Fixes #922. Theres three problems however:. - [ ] The names of our tutorials dont work as well as the headers in our [Tutorials section](https://scanpy.readthedocs.io/en/stable/tutorials.html). - [ ] The order isnt preserved. PAGA should definitely come last as its most specific. - [ ] neither the best practices repo nor scanpy_usage is published with nbsphinx. Also it contains a lot of notebooks, no idea what we want in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/923
https://github.com/scverse/scanpy/pull/923:767,integrability,pub,published,767,"WIP Links to tutorials; This defines a directive to include other TOCs into your own using intersphinx:. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/conf.py#L90-L93. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/index.rst#L22-L23. ![grafik](https://user-images.githubusercontent.com/291575/68944214-a0660a80-07ad-11ea-8e13-6f7c92732633.png). Fixes #922. Theres three problems however:. - [ ] The names of our tutorials dont work as well as the headers in our [Tutorials section](https://scanpy.readthedocs.io/en/stable/tutorials.html). - [ ] The order isnt preserved. PAGA should definitely come last as its most specific. - [ ] neither the best practices repo nor scanpy_usage is published with nbsphinx. Also it contains a lot of notebooks, no idea what we want in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/923
https://github.com/scverse/scanpy/pull/923:699,interoperability,specif,specific,699,"WIP Links to tutorials; This defines a directive to include other TOCs into your own using intersphinx:. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/conf.py#L90-L93. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/index.rst#L22-L23. ![grafik](https://user-images.githubusercontent.com/291575/68944214-a0660a80-07ad-11ea-8e13-6f7c92732633.png). Fixes #922. Theres three problems however:. - [ ] The names of our tutorials dont work as well as the headers in our [Tutorials section](https://scanpy.readthedocs.io/en/stable/tutorials.html). - [ ] The order isnt preserved. PAGA should definitely come last as its most specific. - [ ] neither the best practices repo nor scanpy_usage is published with nbsphinx. Also it contains a lot of notebooks, no idea what we want in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/923
https://github.com/scverse/scanpy/pull/923:732,reliability,pra,practices,732,"WIP Links to tutorials; This defines a directive to include other TOCs into your own using intersphinx:. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/conf.py#L90-L93. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/index.rst#L22-L23. ![grafik](https://user-images.githubusercontent.com/291575/68944214-a0660a80-07ad-11ea-8e13-6f7c92732633.png). Fixes #922. Theres three problems however:. - [ ] The names of our tutorials dont work as well as the headers in our [Tutorials section](https://scanpy.readthedocs.io/en/stable/tutorials.html). - [ ] The order isnt preserved. PAGA should definitely come last as its most specific. - [ ] neither the best practices repo nor scanpy_usage is published with nbsphinx. Also it contains a lot of notebooks, no idea what we want in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/923
https://github.com/scverse/scanpy/pull/923:331,usability,user,user-images,331,"WIP Links to tutorials; This defines a directive to include other TOCs into your own using intersphinx:. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/conf.py#L90-L93. https://github.com/theislab/scanpy/blob/23ac33691acfc188d17c2e84bbf8a7750ce5e063/docs/index.rst#L22-L23. ![grafik](https://user-images.githubusercontent.com/291575/68944214-a0660a80-07ad-11ea-8e13-6f7c92732633.png). Fixes #922. Theres three problems however:. - [ ] The names of our tutorials dont work as well as the headers in our [Tutorials section](https://scanpy.readthedocs.io/en/stable/tutorials.html). - [ ] The order isnt preserved. PAGA should definitely come last as its most specific. - [ ] neither the best practices repo nor scanpy_usage is published with nbsphinx. Also it contains a lot of notebooks, no idea what we want in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/923
https://github.com/scverse/scanpy/issues/924:486,availability,error,error,486,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2297,availability,Mask,MaskedArray,2297,"def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:661,deployability,modul,module,661,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:777,deployability,Continu,Continuum,777,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1025,deployability,Continu,Continuum,1025,"uys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Loca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1393,deployability,Continu,Continuum,1393,"'Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1770,deployability,Continu,Continuum,1770,"l\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2032,deployability,Continu,Continuum,2032,"aconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2370,deployability,Continu,Continuum,2370,". ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_arra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2640,deployability,Continu,Continuum,2640,"_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimension",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2957,deployability,Continu,Continuum,2957,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3283,deployability,Continu,Continuum,3283,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2077,energy efficiency,core,core,2077,"in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2415,energy efficiency,core,core,2415,"site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_fai",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2685,energy efficiency,core,core,2685,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3002,energy efficiency,core,core,3002,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3328,energy efficiency,core,core,3328,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3433,integrability,sub,subarr,3433,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3559,integrability,sub,subarr,3559,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:661,modifiability,modul,module,661,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:806,modifiability,pac,packages,806,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:947,modifiability,layer,layers,947,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:954,modifiability,layer,layers,954,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1054,modifiability,pac,packages,1054,".loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1127,modifiability,layer,layers,1127,"b05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1199,modifiability,layer,layers,1199,"loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, di",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1206,modifiability,layer,layers,1206,"le by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. -",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1422,modifiability,pac,packages,1422,"ed_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\intern",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1507,modifiability,layer,layers,1507,"--------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:1799,modifiability,pac,packages,1799,"te-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2061,modifiability,pac,packages,2061,"ndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtyp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2399,modifiability,pac,packages,2399,"anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2669,modifiability,pac,packages,2669,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:2986,modifiability,pac,packages,2986,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3312,modifiability,pac,packages,3312,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:486,performance,error,error,486,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:295,safety,valid,validate,295,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:440,safety,valid,validate,440,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:486,safety,error,error,486,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:580,safety,Except,Exception,580,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:634,safety,input,input-,634,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:661,safety,modul,module,661,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:744,safety,valid,validate,744,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3503,safety,Except,Exception,3503,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:3610,safety,Except,Exception,3610,", 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 break. 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy). 390 dtype=dtype, copy=copy). 391 elif isinstance(data, dict):. --> 392 mgr = init_dict(data, index, columns, dtype=dtype). 393 elif isinstance(data, ma.MaskedArray):. 394 import numpy.ma.mrecords as mrecords. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_dict(data, index, columns, dtype). 210 arrays = [data[k] for k in keys]. 211 . --> 212 return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype). 213 . 214 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype). 54 . 55 # don't force copy because getting jammed in an ndarray anyway. ---> 56 arrays = _homogenize(arrays, index, dtype). 57 . 58 # from BlockManager perspective. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in _homogenize(data, index, dtype). 275 val = lib.fast_multiget(val, oindex.values, default=np.nan). 276 val = sanitize_array(val, index, dtype=dtype, copy=False,. --> 277 raise_cast_failure=False). 278 . 279 homogenized.append(val). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals\construction.py in sanitize_array(data, index, dtype, copy, raise_cast_failure). 656 elif subarr.ndim > 1:. 657 if isinstance(data, np.ndarray):. --> 658 raise Exception('Data must be 1-dimensional'). 659 else:. 660 subarr = com.asarray_tuplesafe(data, dtype=dtype). Exception: Data must be 1-dimensional. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:295,security,validat,validate,295,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:440,security,validat,validate,440,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:744,security,validat,validate,744,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:590,testability,Trace,Traceback,590,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:486,usability,error,error,486,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/924:634,usability,input,input-,634,"issue opening .loom file; Hi guys,. I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py. loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. However i would like to open it with scanpy by:. ```py. loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False). ```. and i get the following error:. ```pytb. ---------------------------------------------------------------------------. Exception Traceback (most recent call last). <ipython-input-26-3a0e0ee3248f> in <module>(). ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs). 184 var=var,. 185 layers=layers,. --> 186 dtype=dtype). 187 return adata. 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 670 layers=layers,. 671 dtype=dtype, shape=shape,. --> 672 filename=filename, filemode=filemode). 673 . 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode). 848 # annotations. 849 self._obs = _gen_dataframe(obs, self._n_obs,. --> 850 ['obs_names', 'row_names', 'smp_names']). 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']). 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names). 285 _anno = pd.DataFrame(. 286 anno, index=anno[index_name],. --> 287 columns=[k for k in anno.keys() if k != index_name]). 288 br",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924
https://github.com/scverse/scanpy/issues/925:17,availability,cluster,clusters,17,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:66,availability,cluster,cluster,66,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:166,availability,cluster,clustering,166,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:260,availability,cluster,cluster,260,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:286,availability,error,error,286,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:17,deployability,cluster,clusters,17,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:66,deployability,cluster,cluster,66,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:166,deployability,cluster,clustering,166,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:260,deployability,cluster,cluster,260,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:13,integrability,sub,sub-clusters,13,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:201,integrability,sub,subclusters,201,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:286,performance,error,error,286,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:286,safety,error,error,286,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:56,security,ident,identical,56,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:103,testability,understand,understanding,103,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:286,usability,error,error,286,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/925:451,usability,help,help,451,"How to merge sub-clusters or rename the categories with identical cluster names?; Hi,. To have a depth understanding, I wanted to set the resolution high for louvain clustering, but now I cannot merge subclusters. When I try to rename the categories with same cluster name, it gives an error about not having unique names. Yet, I could not find a functional merge_clusters function. Is there anyone having the same issue as me? I would appreciate any help. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925
https://github.com/scverse/scanpy/issues/926:355,energy efficiency,current,currently,355,"Scatter plots coloring when a gene in the column specified by `gene_symbols=` corresponds to multiple entries; - [x] Additional function parameters / changed functionality / changed defaults? When coloring by a gene in a column of `.var` specified by `gene_symbols=` and when there are multiple entries in `.var` sharing the same value of that attribute, currently only the first entry is used to coloring. Would you consider allowing a different summarisation, e.g. mean/sum/etc? An existing work around is that the user calculate that summarisation oneself and insert it into `.obs`, but it becomes tedious when the number grows. For example, this occurs when mapping genes across species with one-to-many mappings kept. https://github.com/theislab/scanpy/blob/233aa15adbf565a7cb278d0724e3cb3f566b749f/scanpy/plotting/_tools/scatterplots.py#L844",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:49,interoperability,specif,specified,49,"Scatter plots coloring when a gene in the column specified by `gene_symbols=` corresponds to multiple entries; - [x] Additional function parameters / changed functionality / changed defaults? When coloring by a gene in a column of `.var` specified by `gene_symbols=` and when there are multiple entries in `.var` sharing the same value of that attribute, currently only the first entry is used to coloring. Would you consider allowing a different summarisation, e.g. mean/sum/etc? An existing work around is that the user calculate that summarisation oneself and insert it into `.obs`, but it becomes tedious when the number grows. For example, this occurs when mapping genes across species with one-to-many mappings kept. https://github.com/theislab/scanpy/blob/233aa15adbf565a7cb278d0724e3cb3f566b749f/scanpy/plotting/_tools/scatterplots.py#L844",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:238,interoperability,specif,specified,238,"Scatter plots coloring when a gene in the column specified by `gene_symbols=` corresponds to multiple entries; - [x] Additional function parameters / changed functionality / changed defaults? When coloring by a gene in a column of `.var` specified by `gene_symbols=` and when there are multiple entries in `.var` sharing the same value of that attribute, currently only the first entry is used to coloring. Would you consider allowing a different summarisation, e.g. mean/sum/etc? An existing work around is that the user calculate that summarisation oneself and insert it into `.obs`, but it becomes tedious when the number grows. For example, this occurs when mapping genes across species with one-to-many mappings kept. https://github.com/theislab/scanpy/blob/233aa15adbf565a7cb278d0724e3cb3f566b749f/scanpy/plotting/_tools/scatterplots.py#L844",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:137,modifiability,paramet,parameters,137,"Scatter plots coloring when a gene in the column specified by `gene_symbols=` corresponds to multiple entries; - [x] Additional function parameters / changed functionality / changed defaults? When coloring by a gene in a column of `.var` specified by `gene_symbols=` and when there are multiple entries in `.var` sharing the same value of that attribute, currently only the first entry is used to coloring. Would you consider allowing a different summarisation, e.g. mean/sum/etc? An existing work around is that the user calculate that summarisation oneself and insert it into `.obs`, but it becomes tedious when the number grows. For example, this occurs when mapping genes across species with one-to-many mappings kept. https://github.com/theislab/scanpy/blob/233aa15adbf565a7cb278d0724e3cb3f566b749f/scanpy/plotting/_tools/scatterplots.py#L844",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/issues/926:517,usability,user,user,517,"Scatter plots coloring when a gene in the column specified by `gene_symbols=` corresponds to multiple entries; - [x] Additional function parameters / changed functionality / changed defaults? When coloring by a gene in a column of `.var` specified by `gene_symbols=` and when there are multiple entries in `.var` sharing the same value of that attribute, currently only the first entry is used to coloring. Would you consider allowing a different summarisation, e.g. mean/sum/etc? An existing work around is that the user calculate that summarisation oneself and insert it into `.obs`, but it becomes tedious when the number grows. For example, this occurs when mapping genes across species with one-to-many mappings kept. https://github.com/theislab/scanpy/blob/233aa15adbf565a7cb278d0724e3cb3f566b749f/scanpy/plotting/_tools/scatterplots.py#L844",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926
https://github.com/scverse/scanpy/pull/927:55,deployability,updat,updates,55,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:73,deployability,log,logging,73,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:55,safety,updat,updates,55,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:73,safety,log,logging,73,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:55,security,updat,updates,55,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:73,security,log,logging,73,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:73,testability,log,logging,73,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/pull/927:44,usability,efficien,efficiency,44,"score_genes: use_raw=None instead of False, efficiency updates, prettier logging;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927
https://github.com/scverse/scanpy/issues/929:80,deployability,log,log,80,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:114,deployability,log,logarithm,114,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:186,deployability,log,log,186,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:252,deployability,log,log,252,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:530,energy efficiency,adapt,adapt,530,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:530,integrability,adapt,adapt,530,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:530,interoperability,adapt,adapt,530,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:530,modifiability,adapt,adapt,530,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:478,performance,time,time,478,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:572,performance,time,time,572,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:80,safety,log,log,80,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:114,safety,log,logarithm,114,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:186,safety,log,log,186,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:252,safety,log,log,252,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:80,security,log,log,80,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:114,security,log,logarithm,114,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:186,security,log,log,186,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:252,security,log,log,252,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:80,testability,log,log,80,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:114,testability,log,logarithm,114,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:186,testability,log,log,186,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:252,testability,log,log,252,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:335,testability,simpl,simple,335,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/929:335,usability,simpl,simple,335,"log1p with different bases; `log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929
https://github.com/scverse/scanpy/issues/932:0,availability,Cluster,Cluster,0,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:266,availability,cluster,clustering,266,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:0,deployability,Cluster,Cluster,0,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:25,deployability,stack,stacked,25,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:266,deployability,cluster,clustering,266,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:317,deployability,stack,stacked,317,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:425,energy efficiency,heat,heatmaps,425,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:125,modifiability,paramet,parameters,125,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/932:416,reliability,doe,does,416,"Cluster cols and rows of stacked violin; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. Could you add functionality to allow for clustering of both the columns and the rows of the stacked violin plot (e.g., dendrogram_col = True, dendrogram_row = True), similar to what pheatmap does for heatmaps in R?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/932
https://github.com/scverse/scanpy/issues/933:226,availability,Cluster,Clusterings,226,"interaction with loom object; Hi guys,. I have opened a loom file on scanpy by running: sc.read_loom. Once i try to see what is in there this is what pops up:. AnnData object with n_obs  n_vars = 56902  17473 . obs: 'Age', 'Clusterings', 'Embedding', 'Embeddings_X', 'Embeddings_Y', 'Gender', 'Genotype', 'RegulonsAUC', 'Replicate', 'nGene', 'nUMI'. var: 'ClusterMarkers_0', 'ClusterMarkers_0_sub_0', 'ClusterMarkers_0_sub_1', 'ClusterMarkers_0_sub_10', 'ClusterMarkers_0_sub_11', 'ClusterMarkers_0_sub_12', 'ClusterMarkers_0_sub_13', 'ClusterMarkers_0_sub_14', 'ClusterMarkers_0_sub_15', 'ClusterMarkers_0_sub_16', 'ClusterMarkers_0_sub_17', 'ClusterMarkers_0_sub_18', 'ClusterMarkers_0_sub_19', 'ClusterMarkers_0_sub_2', 'ClusterMarkers_0_sub_20', 'ClusterMarkers_0_sub_21', 'ClusterMarkers_0_sub_22', 'ClusterMarkers_0_sub_23', 'ClusterMarkers_0_sub_24', 'ClusterMarkers_0_sub_25', 'ClusterMarkers_0_sub_26', 'ClusterMarkers_0_sub_27', 'ClusterMarkers_0_sub_28', 'ClusterMarkers_0_sub_29', 'ClusterMarkers_0_sub_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:323,availability,Replic,Replicate,323,"interaction with loom object; Hi guys,. I have opened a loom file on scanpy by running: sc.read_loom. Once i try to see what is in there this is what pops up:. AnnData object with n_obs  n_vars = 56902  17473 . obs: 'Age', 'Clusterings', 'Embedding', 'Embeddings_X', 'Embeddings_Y', 'Gender', 'Genotype', 'RegulonsAUC', 'Replicate', 'nGene', 'nUMI'. var: 'ClusterMarkers_0', 'ClusterMarkers_0_sub_0', 'ClusterMarkers_0_sub_1', 'ClusterMarkers_0_sub_10', 'ClusterMarkers_0_sub_11', 'ClusterMarkers_0_sub_12', 'ClusterMarkers_0_sub_13', 'ClusterMarkers_0_sub_14', 'ClusterMarkers_0_sub_15', 'ClusterMarkers_0_sub_16', 'ClusterMarkers_0_sub_17', 'ClusterMarkers_0_sub_18', 'ClusterMarkers_0_sub_19', 'ClusterMarkers_0_sub_2', 'ClusterMarkers_0_sub_20', 'ClusterMarkers_0_sub_21', 'ClusterMarkers_0_sub_22', 'ClusterMarkers_0_sub_23', 'ClusterMarkers_0_sub_24', 'ClusterMarkers_0_sub_25', 'ClusterMarkers_0_sub_26', 'ClusterMarkers_0_sub_27', 'ClusterMarkers_0_sub_28', 'ClusterMarkers_0_sub_29', 'ClusterMarkers_0_sub_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2497,availability,Cluster,ClusterID,2497,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2532,availability,avail,available,2532,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2709,availability,error,error,2709,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2849,availability,cluster,clustering,2849,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2939,availability,cluster,clusters,2939,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:226,deployability,Cluster,Clusterings,226,"interaction with loom object; Hi guys,. I have opened a loom file on scanpy by running: sc.read_loom. Once i try to see what is in there this is what pops up:. AnnData object with n_obs  n_vars = 56902  17473 . obs: 'Age', 'Clusterings', 'Embedding', 'Embeddings_X', 'Embeddings_Y', 'Gender', 'Genotype', 'RegulonsAUC', 'Replicate', 'nGene', 'nUMI'. var: 'ClusterMarkers_0', 'ClusterMarkers_0_sub_0', 'ClusterMarkers_0_sub_1', 'ClusterMarkers_0_sub_10', 'ClusterMarkers_0_sub_11', 'ClusterMarkers_0_sub_12', 'ClusterMarkers_0_sub_13', 'ClusterMarkers_0_sub_14', 'ClusterMarkers_0_sub_15', 'ClusterMarkers_0_sub_16', 'ClusterMarkers_0_sub_17', 'ClusterMarkers_0_sub_18', 'ClusterMarkers_0_sub_19', 'ClusterMarkers_0_sub_2', 'ClusterMarkers_0_sub_20', 'ClusterMarkers_0_sub_21', 'ClusterMarkers_0_sub_22', 'ClusterMarkers_0_sub_23', 'ClusterMarkers_0_sub_24', 'ClusterMarkers_0_sub_25', 'ClusterMarkers_0_sub_26', 'ClusterMarkers_0_sub_27', 'ClusterMarkers_0_sub_28', 'ClusterMarkers_0_sub_29', 'ClusterMarkers_0_sub_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2497,deployability,Cluster,ClusterID,2497,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2849,deployability,cluster,clustering,2849,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2939,deployability,cluster,clusters,2939,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2522,integrability,pub,publicaly,2522,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2589,integrability,pub,published,2589,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2709,performance,error,error,2709,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2532,reliability,availab,available,2532,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2532,safety,avail,available,2532,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2709,safety,error,error,2709,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2532,security,availab,available,2532,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2651,testability,simpl,simply,2651,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:0,usability,interact,interaction,0,"interaction with loom object; Hi guys,. I have opened a loom file on scanpy by running: sc.read_loom. Once i try to see what is in there this is what pops up:. AnnData object with n_obs  n_vars = 56902  17473 . obs: 'Age', 'Clusterings', 'Embedding', 'Embeddings_X', 'Embeddings_Y', 'Gender', 'Genotype', 'RegulonsAUC', 'Replicate', 'nGene', 'nUMI'. var: 'ClusterMarkers_0', 'ClusterMarkers_0_sub_0', 'ClusterMarkers_0_sub_1', 'ClusterMarkers_0_sub_10', 'ClusterMarkers_0_sub_11', 'ClusterMarkers_0_sub_12', 'ClusterMarkers_0_sub_13', 'ClusterMarkers_0_sub_14', 'ClusterMarkers_0_sub_15', 'ClusterMarkers_0_sub_16', 'ClusterMarkers_0_sub_17', 'ClusterMarkers_0_sub_18', 'ClusterMarkers_0_sub_19', 'ClusterMarkers_0_sub_2', 'ClusterMarkers_0_sub_20', 'ClusterMarkers_0_sub_21', 'ClusterMarkers_0_sub_22', 'ClusterMarkers_0_sub_23', 'ClusterMarkers_0_sub_24', 'ClusterMarkers_0_sub_25', 'ClusterMarkers_0_sub_26', 'ClusterMarkers_0_sub_27', 'ClusterMarkers_0_sub_28', 'ClusterMarkers_0_sub_29', 'ClusterMarkers_0_sub_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2651,usability,simpl,simply,2651,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/issues/933:2709,usability,error,error,2709,"_3', 'ClusterMarkers_0_sub_30', 'ClusterMarkers_0_sub_31', 'ClusterMarkers_0_sub_32', 'ClusterMarkers_0_sub_33', 'ClusterMarkers_0_sub_34', 'ClusterMarkers_0_sub_35', 'ClusterMarkers_0_sub_36', 'ClusterMarkers_0_sub_37', 'ClusterMarkers_0_sub_38', 'ClusterMarkers_0_sub_39', 'ClusterMarkers_0_sub_4', 'ClusterMarkers_0_sub_40', 'ClusterMarkers_0_sub_41', 'ClusterMarkers_0_sub_42', 'ClusterMarkers_0_sub_43', 'ClusterMarkers_0_sub_44', 'ClusterMarkers_0_sub_45', 'ClusterMarkers_0_sub_46', 'ClusterMarkers_0_sub_47', 'ClusterMarkers_0_sub_48', 'ClusterMarkers_0_sub_49', 'ClusterMarkers_0_sub_5', 'ClusterMarkers_0_sub_50', 'ClusterMarkers_0_sub_51', 'ClusterMarkers_0_sub_52', 'ClusterMarkers_0_sub_53', 'ClusterMarkers_0_sub_54', 'ClusterMarkers_0_sub_55', 'ClusterMarkers_0_sub_56', 'ClusterMarkers_0_sub_57', 'ClusterMarkers_0_sub_58', 'ClusterMarkers_0_sub_59', 'ClusterMarkers_0_sub_6', 'ClusterMarkers_0_sub_60', 'ClusterMarkers_0_sub_61', 'ClusterMarkers_0_sub_62', 'ClusterMarkers_0_sub_63', 'ClusterMarkers_0_sub_64', 'ClusterMarkers_0_sub_65', 'ClusterMarkers_0_sub_66', 'ClusterMarkers_0_sub_67', 'ClusterMarkers_0_sub_68', 'ClusterMarkers_0_sub_69', 'ClusterMarkers_0_sub_7', 'ClusterMarkers_0_sub_70', 'ClusterMarkers_0_sub_71', 'ClusterMarkers_0_sub_72', 'ClusterMarkers_0_sub_8', 'ClusterMarkers_0_sub_9', 'ClusterMarkers_1', 'ClusterMarkers_2', 'ClusterMarkers_3', 'ClusterMarkers_4', 'ClusterMarkers_5', 'ClusterMarkers_6', 'ClusterMarkers_7', 'Regulons'. obsm: 'ClusterID'. This is from publicaly available data. so what i would like to do is plot their published tsne or umap and compare a few things from it. If I simply run sc.pl.tsne(loom_file, color=['louvain']) I get error msg: ValueError: no field of name X_tsne. This makes sense as there is not X_tsne on the object. How could I get pass this without re-clustering myself? At the moment I am only interested in pulling out 2 of their annotated clusters... if there is an easy way to do this via scanpy please let me know.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/933
https://github.com/scverse/scanpy/pull/934:0,deployability,Updat,Update,0,"Update release_notes and remove inline links; I got rid of the margins for the floating images. I didnt see much of a difference, and if they should be in the css sheet, not inline.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/934
https://github.com/scverse/scanpy/pull/934:0,safety,Updat,Update,0,"Update release_notes and remove inline links; I got rid of the margins for the floating images. I didnt see much of a difference, and if they should be in the css sheet, not inline.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/934
https://github.com/scverse/scanpy/pull/934:0,security,Updat,Update,0,"Update release_notes and remove inline links; I got rid of the margins for the floating images. I didnt see much of a difference, and if they should be in the css sheet, not inline.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/934
https://github.com/scverse/scanpy/issues/935:158,integrability,batch,batch,158,"no highly variable genes; I ran the newest Scanpy package's . ```. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,. batch_key='batch'). ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:10,modifiability,variab,variable,10,"no highly variable genes; I ran the newest Scanpy package's . ```. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,. batch_key='batch'). ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:50,modifiability,pac,package,50,"no highly variable genes; I ran the newest Scanpy package's . ```. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,. batch_key='batch'). ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:277,modifiability,variab,variable,277,"no highly variable genes; I ran the newest Scanpy package's . ```. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,. batch_key='batch'). ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/935:158,performance,batch,batch,158,"no highly variable genes; I ran the newest Scanpy package's . ```. sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,. batch_key='batch'). ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935
https://github.com/scverse/scanpy/issues/936:208,availability,error,error,208,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:0,deployability,Fail,Failed,0,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:24,deployability,pipelin,pipeline,24,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:234,deployability,Fail,Failed,234,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:258,deployability,pipelin,pipeline,258,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:314,energy efficiency,CPU,CPUDispatcher,314,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:566,energy efficiency,CPU,CPUDispatcher,566,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:845,energy efficiency,current,current,845,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:24,integrability,pipelin,pipeline,24,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:258,integrability,pipelin,pipeline,258,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:370,modifiability,paramet,parameters,370,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:511,modifiability,paramet,parameterized,511,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:678,modifiability,pac,packages,678,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:749,modifiability,pac,packages,749,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:208,performance,error,error,208,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:314,performance,CPU,CPUDispatcher,314,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:566,performance,CPU,CPUDispatcher,566,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:0,reliability,Fail,Failed,0,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:234,reliability,Fail,Failed,234,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:208,safety,error,error,208,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:435,security,sign,signatures,435,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/936:208,usability,error,error,208,"Failed in nopython mode pipeline ; This may be related to this issue:. https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```. TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)). [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936
https://github.com/scverse/scanpy/issues/937:625,availability,error,error,625,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:817,availability,error,error,817,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1847,availability,ERROR,ERROR,1847,":. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1880,availability,Error,Error,1880,"/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:117,deployability,modul,module,117,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:124,deployability,updat,update,124,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:254,deployability,instal,install,254,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:304,deployability,modul,modules,304,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:740,deployability,modul,modules,740,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:755,deployability,updat,updating,755,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2125,deployability,modul,module,2125,".pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(file",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2860,deployability,log,logg,2860,"``. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4748,deployability,Version,Versions,4748,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4781,deployability,log,logging,4781,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3047,energy efficiency,load,load,3047,"rror Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4748,integrability,Version,Versions,4748,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4527,interoperability,format,format,4527,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:117,modifiability,modul,module,117,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:304,modifiability,modul,modules,304,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:740,modifiability,modul,modules,740,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:843,modifiability,pac,package,843,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2125,modifiability,modul,module,2125,".pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(file",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2210,modifiability,pac,packages,2210," = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2587,modifiability,pac,packages,2587,"lcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2951,modifiability,pac,packages,2951,". ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3246,modifiability,pac,packages,3246,"ename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3546,modifiability,pac,packages,3546,"ict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3860,modifiability,pac,packages,3860,".debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4174,modifiability,pac,packages,4174,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4424,modifiability,pac,packages,4424,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4748,modifiability,Version,Versions,4748,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:625,performance,error,error,625,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:817,performance,error,error,817,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1847,performance,ERROR,ERROR,1847,":. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1880,performance,Error,Error,1880,"/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2320,performance,cach,cache,2320,"tegorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2484,performance,cach,cache,2484,". # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2490,performance,cach,cache,2490,"t if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2698,performance,cach,cache,2698,"ataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3047,performance,load,load,3047,"rror Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3068,performance,memor,memory,3068,"ecent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_fro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:117,safety,modul,module,117,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:124,safety,updat,update,124,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:304,safety,modul,modules,304,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:625,safety,error,error,625,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:740,safety,modul,modules,740,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:755,safety,updat,updating,755,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:817,safety,error,error,817,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1432,safety,test,test,1432,"save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1783,safety,test,test,1783," seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1847,safety,ERROR,ERROR,1847,":. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1880,safety,Error,Error,1880,"/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2098,safety,input,input-,2098,"0, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2125,safety,modul,module,2125,".pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(file",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2860,safety,log,logg,2860,"``. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4781,safety,log,logging,4781,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:124,security,updat,update,124,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:755,security,updat,updating,755,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2860,security,log,logg,2860,"``. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4781,security,log,logging,4781,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1432,testability,test,test,1432,"save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1783,testability,test,test,1783," seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2054,testability,Trace,Traceback,2054," existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2860,testability,log,logg,2860,"``. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4781,testability,log,logging,4781,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:152,usability,clear,clear,152,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:625,usability,error,error,625,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:817,usability,error,error,817,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:909,usability,minim,minimal,909,"Reading an h5ad file not working anymore after I run the rank_genes_groups function (used to work fine before python module update); <!-- Please give a clear and concise description of what the bug is: -->. Yesterday I moved to a new server and I had to install miniconda3, Jupiter and all the necessary modules for my scRNA-seq analysis including scanpy. I can read fine an h5ad file and run various steps with scanpy and I can then save the object as an h5ad file and read it back without a problem. However, if I run the rank_genes_groups function, even though I can perfectly fine save my object as an h5ad file I get an error when I am attempting to read it back. I have to say that this exact piece of code used to work with my older modules before updating it. Also, some people seem to have spotted a similar error in the newest numpy package:. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ----------------------------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1847,usability,ERROR,ERROR,1847,":. https://github.com/numpy/numpy/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:1880,usability,Error,Error,1880,"/issues/13431. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. # I have already read in an Ann data object from an h5ad existing file. sc.tl.pca(adata, n_comps=30, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sh",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:2098,usability,input,input-,2098,"0, svd_solver='arpack'). sc.pp.neighbors(adata, n_neighbors=15). sc.tl.umap(adata). k = 15. communities, graph, Q = sc.external.tl.phenograph(pd.DataFrame(adata.obsm['X_pca']),k=k). adata.obs['PhenoGraph_clusters'] = pd.Categorical(communities). adata.uns['PhenoGraph_Q'] = Q. adata.uns['PhenoGraph_k'] = k. path_to_h5ad_file = '~/test.h5ad'. adata.write_h5ad(path_to_h5ad_file) # works. # but if I run. sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'). rcParams['figure.figsize'] = 4,4. rcParams['axes.grid'] = True. sc.pl.rank_genes_groups(adata). pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works. adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:3068,usability,memor,memory,3068,"ecent call last). <ipython-input-23-cb0bc3c267ae> in <module>. ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_fro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/937:4917,usability,learn,learn,4917,"n read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.4 scipy==1.3.3 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937
https://github.com/scverse/scanpy/issues/938:26,availability,cluster,cluster,26,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:191,availability,cluster,cluster,191,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:781,availability,error,error-prone,781,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:911,availability,cluster,cluster,911,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:26,deployability,cluster,cluster,26,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:191,deployability,cluster,cluster,191,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:911,deployability,cluster,cluster,911,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:763,energy efficiency,current,current,763,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:103,modifiability,variab,variable,103,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:252,modifiability,variab,variable,252,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:781,performance,error,error-prone,781,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:781,safety,error,error-prone,781,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/issues/938:781,usability,error,error-prone,781,"Finding a better home for cluster centroids (i.e. _tmp_cluster_pos); We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938
https://github.com/scverse/scanpy/pull/939:246,deployability,releas,release,246,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:9,performance,network,networkx,9,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:165,performance,network,networkx,165,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:206,performance,network,networkx,206,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:9,security,network,networkx,9,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:165,security,network,networkx,165,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:206,security,network,networkx,206,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/939:225,usability,document,documentation,225,Restrict networkx to 1.X; We use Graph.node attributes (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L776) which are removed in networkx 2.X. More details here: https://networkx.github.io/documentation/stable/release/migration_guide_from_1.x_to_2.0.html,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/939
https://github.com/scverse/scanpy/pull/941:278,availability,error,errors,278,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:364,availability,slo,slow,364,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:175,deployability,version,version,175,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:84,energy efficiency,reduc,reduction,84,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:175,integrability,version,version,175,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:934,integrability,compon,components,934,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:971,integrability,compon,components,971,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:934,interoperability,compon,components,934,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:971,interoperability,compon,components,971,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:1539,interoperability,Share,Share,1539,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:175,modifiability,version,version,175,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:767,modifiability,variab,variable,767,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:934,modifiability,compon,components,934,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:971,modifiability,compon,components,971,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:1404,modifiability,scal,scaling,1404,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:278,performance,error,errors,278,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:364,reliability,slo,slow,364,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:1358,reliability,stabil,stability,1358,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:278,safety,error,errors,278,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:41,usability,progress,progress,41,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:278,usability,error,errors,278,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:571,usability,learn,learn,571,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:584,usability,learn,learn,584,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:628,usability,learn,learn,628,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:641,usability,learn,learn,641,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:1046,usability,user,user-images,1046,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:1206,usability,user,user-images,1206,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/pull/941:1518,usability,Document,Documentation,1518,"[WIP] ICA; Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version  entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset  10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>. <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>. <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability. - [ ] Figure out if I should be be scaling the whitening matrix differently. - [ ] More in depth comparison of results with sklearn based ICA. - [ ] Documentation. - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941
https://github.com/scverse/scanpy/issues/942:453,availability,Error,Error,453,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:628,deployability,modul,module,628,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:847,deployability,log,log,847,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:886,deployability,scale,scale,886,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1567,deployability,Version,Versions,1567,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1600,deployability,log,logging,1600,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:886,energy efficiency,scale,scale,886,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1567,integrability,Version,Versions,1567,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:628,modifiability,modul,module,628,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:778,modifiability,pac,packages,778,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:886,modifiability,scal,scale,886,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1219,modifiability,pac,packages,1219,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1276,modifiability,layer,layer,1276,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1431,modifiability,layer,layer,1431,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1437,modifiability,layer,layer,1437,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1567,modifiability,Version,Versions,1567,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:453,performance,Error,Error,453,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:886,performance,scale,scale,886,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:453,safety,Error,Error,453,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:601,safety,input,input-,601,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:628,safety,modul,module,628,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:847,safety,log,log,847,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1600,safety,log,logging,1600,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:847,security,log,log,847,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:927,security,rotat,rotation,927,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1600,security,log,logging,1600,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:557,testability,Trace,Traceback,557,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:847,testability,log,log,847,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1600,testability,log,logging,1600,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:83,usability,clear,clear,83,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:246,usability,minim,minimal,246,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:453,usability,Error,Error,453,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:601,usability,input,input-,601,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/issues/942:1726,usability,learn,learn,1726,"AttributeError: 'AnnData' object has no attribute 'obs_vector'; <!-- Please give a clear and concise description of what the bug is: -->. ... AttributeError: 'AnnData' object has no attribute 'obs_vector', when `pl.umap`, `pl.violin`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ...sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ...AttributeError Traceback (most recent call last). <ipython-input-41-ed9365d2081e> in <module>. ----> 1 sc.pl.violin(adata, ['n_genes', 'n_counts', 'percent_mito'],jitter=0.4, multi_panel=True). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds). 636 obs_df = get.obs_df(adata, keys=[groupby] + keys, use_raw=use_raw). 637 else:. --> 638 obs_df = get.obs_df(adata, keys=keys, use_raw=use_raw). 639 if groupby is None:. 640 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/anaconda3/envs/myenv/lib/python3.7/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 160 for k, l in zip(keys, lookup_keys):. 161 if not use_raw or k in adata.obs.columns:. --> 162 df[k] = adata.obs_vector(l, layer=layer). 163 else:. 164 df[k] = adata.raw.obs_vector(l). AttributeError: 'AnnData' object has no attribute 'obs_vector'. ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ...scanpy==1.4.3 anndata==0.6.20 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.21.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942
https://github.com/scverse/scanpy/pull/943:22,integrability,sub,subsample,22,"Add replace option to subsample and rename function to sample; so that we can do sampling with replacement. It can be useful for bootstrap samples i.e. sc.pp.subsample(adata, n_obs=adata.n_obs, replace=True). We can also sample fractions > 1, but then it's not really subsampling.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/943
https://github.com/scverse/scanpy/pull/943:158,integrability,sub,subsample,158,"Add replace option to subsample and rename function to sample; so that we can do sampling with replacement. It can be useful for bootstrap samples i.e. sc.pp.subsample(adata, n_obs=adata.n_obs, replace=True). We can also sample fractions > 1, but then it's not really subsampling.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/943
https://github.com/scverse/scanpy/pull/943:268,integrability,sub,subsampling,268,"Add replace option to subsample and rename function to sample; so that we can do sampling with replacement. It can be useful for bootstrap samples i.e. sc.pp.subsample(adata, n_obs=adata.n_obs, replace=True). We can also sample fractions > 1, but then it's not really subsampling.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/943
https://github.com/scverse/scanpy/issues/944:44,deployability,scale,scale,44,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:103,deployability,scale,scale,103,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:217,deployability,scale,scale,217,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:698,deployability,Version,Versions,698,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:44,energy efficiency,scale,scale,44,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:103,energy efficiency,scale,scale,103,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:217,energy efficiency,scale,scale,217,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:698,integrability,Version,Versions,698,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:44,modifiability,scal,scale,44,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:103,modifiability,scal,scale,103,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:217,modifiability,scal,scale,217,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:698,modifiability,Version,Versions,698,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:44,performance,scale,scale,44,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:103,performance,scale,scale,103,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:217,performance,scale,scale,217,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:498,security,rotat,rotation,498,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:231,usability,Minim,Minimizing,231,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:256,usability,help,helps,256,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:309,usability,minim,minimal,309,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:597,usability,user,user-images,597,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/issues/944:813,usability,learn,learn,813,"sc.pl.stacked_violin overplotting of y-axis scale; Especially when using `standard_scale = 'var'`, the scale values obscure each other if values are '0' and '1'. Is there a way to allow for off-center plotting of the scale values? Minimizing the font size helps somewhat but is far from ideal. ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```. sc.settings.set_figure_params(dpi=150). sc.pl.stacked_violin(adata, marker_genes, groupby='louvain', rotation = 90, standard_scale = 'var'. ... ```. Output:. <img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/36309128/70078263-5dd76700-15d0-11ea-8c6d-2d809f4a2f7c.png"">. #### Versions:. <scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.15.4 scipy==1.3.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/944
https://github.com/scverse/scanpy/pull/945:26,security,access,accessible,26,Make the optimal_ordering accessible from sc.tl.dendrogram.;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/945
https://github.com/scverse/scanpy/issues/946:0,integrability,Filter,Filtering,0,"Filtering cells by barcodes?; Hello,. If I have a list of cells that I want to look at (and their corresponding barcodes), is it possible to filter to just those cells? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/946
https://github.com/scverse/scanpy/issues/946:141,integrability,filter,filter,141,"Filtering cells by barcodes?; Hello,. If I have a list of cells that I want to look at (and their corresponding barcodes), is it possible to filter to just those cells? Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/946
https://github.com/scverse/scanpy/issues/947:283,energy efficiency,current,currently,283,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:900,integrability,wrap,wraps,900,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1273,interoperability,share,share,1273,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:67,modifiability,extens,extension,67,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1073,modifiability,variab,variables,1073,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:0,performance,Cach,Caching,0,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:59,performance,cach,caching,59,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:231,performance,cach,caching,231,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:310,performance,cach,cache,310,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:521,performance,cach,cache,521,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:653,performance,cach,caching,653,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:714,performance,cach,cached,714,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:840,performance,Cach,Cache,840,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:946,performance,cach,caching,946,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1067,performance,cach,cache,1067,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1192,performance,cach,caching,1192,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1462,performance,cach,cached,1462,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1607,performance,cach,caching,1607,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1574,safety,input,input,1574,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/947:1574,usability,input,input,1574,"Caching in Scanpy; Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:. - write the AnnData object. - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python. import scachepy. c = scachepy.Cache(<directory>) . c.pp.pca(adata). ```. where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to.... - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls. - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object. - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo. .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947
https://github.com/scverse/scanpy/issues/948:12,availability,error,error,12,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:26,availability,error,errors,26,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:474,availability,error,error,474,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1238,availability,error,errors,1238,"b/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/nump",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2292,availability,error,errors,2292," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2483,availability,error,error,2483," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:46,deployability,Fail,Failed,46,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:70,deployability,pipelin,pipeline,70,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:575,deployability,modul,module,575,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1258,deployability,Fail,Failed,1258,"/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1282,deployability,pipelin,pipeline,1282,"hlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2087,deployability,releas,release,2087," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1338,energy efficiency,CPU,CPUDispatcher,1338,"tion providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pyd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1590,energy efficiency,CPU,CPUDispatcher,1590,"r/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1873,energy efficiency,current,current,1873,"n3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:70,integrability,pipelin,pipeline,70,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:480,integrability,messag,message,480,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1282,integrability,pipelin,pipeline,1282,"hlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2489,integrability,messag,message,2489," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:480,interoperability,messag,message,480,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2489,interoperability,messag,message,2489," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:575,modifiability,modul,module,575,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:629,modifiability,pac,packages,629,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:759,modifiability,pac,packages,759,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:887,modifiability,pac,packages,887,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1023,modifiability,pac,packages,1023,"rrors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1154,modifiability,pac,packages,1154,"'m trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/refer",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1394,modifiability,paramet,parameters,1394,"8 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-cod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1535,modifiability,paramet,parameterized,1535,"t):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproduc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1711,modifiability,pac,packages,1711,"ile ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 113",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1777,modifiability,pac,packages,1777,""", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2680,modifiability,pac,package,2680," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:12,performance,error,error,12,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:26,performance,error,errors,26,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:474,performance,error,error,474,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1238,performance,error,errors,1238,"b/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/nump",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1338,performance,CPU,CPUDispatcher,1338,"tion providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pyd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1590,performance,CPU,CPUDispatcher,1590,"r/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2292,performance,error,errors,2292," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2483,performance,error,error,2483," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:46,reliability,Fail,Failed,46,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1258,reliability,Fail,Failed,1258,"/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2401,reliability,doe,doesn-t-compile,2401," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:12,safety,error,error,12,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:26,safety,error,errors,26,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:474,safety,error,error,474,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:575,safety,modul,module,575,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1238,safety,error,errors,1238,"b/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/nump",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2292,safety,error,errors,2292," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2483,safety,error,error,2483," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1459,security,sign,signatures,1459,"ting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:511,testability,Trace,Traceback,511,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2502,testability,trace,traceback,2502," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:12,usability,error,error,12,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:26,usability,error,errors,26,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:474,usability,error,error,474,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:645,usability,tool,tools,645,"Getting the error: ""numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)"" when running sc.tl.umap with init_pos='paga'; Hi,. I'm trying to follow the [Dahlin18 PAGA tutorial](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported feature",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:1238,usability,error,errors,1238,"b/theislab/paga/blob/master/blood/dahlin18/dahlin18.ipynb). And in the part where it calls the UMAP function providing it with the PAGA initial points (line 28 in the notebook: `sc.tl.umap(adata, init_pos='paga')`), I'm getting this error message:. ```. computing UMAP. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap. verbose=settings.verbosity > 3,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding. verbose=verbose,. File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/nump",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2063,usability,support,supported,2063," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2292,usability,error,errors,2292," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2370,usability,user,user,2370," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2483,usability,error,error,2483," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/948:2526,usability,minim,minimal,2526," 401, in _compile_for_args. error_rewrite(e, 'typing'). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite. reraise(type(e), e, None). File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise. raise value.with_traceback(tb). numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend). Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)). Known signatures:. * (array(float32, 1d, A), array(float32, 1d, A)) -> float32. * parameterized. [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)). [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:. def optimize_layout(. <source elided>. dist_squared = rdist(current, other). ^. This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. I saw a relevant [issue](https://github.com/lmcinnes/umap/issues/179) on the umap package and . even changed line 1138 in [umap_.py](https://github.com/lmcinnes/umap/blob/80f1247de0d60eb60d7222a3cdf9aef9452ab38e/umap/umap_.py) from `embedding` to `embedding..astype(np.float32, copy=True)`, but no success. Any idea? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948
https://github.com/scverse/scanpy/issues/949:204,usability,tool,tools,204,sc.tl.rank_genes_groups truncates gene ids to 50 bytes/char; Ran in to this problem becouse I have gene names longer than U50. as found in this line. https://github.com/theislab/scanpy/blob/master/scanpy/tools/_rank_genes_groups.py#L433. This silently truncates rank genes to 50 characters and the gene groups can't be used to plot or otherwise search indices of my data.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/949
https://github.com/scverse/scanpy/issues/950:450,deployability,automat,automated,450,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:54,integrability,sub,submission,54,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:341,integrability,coupl,couple,341,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:139,interoperability,format,format,139,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:161,interoperability,share,share,161,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:391,interoperability,format,formatting,391,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:528,interoperability,format,formatted,528,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:341,modifiability,coupl,couple,341,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:652,reliability,doe,does,652,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:341,testability,coupl,couple,341,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/issues/950:450,testability,automat,automated,450,"Auto-generation of Study files for Single Cell Portal submission; I have recently been using `scanpy` to analyze my data and get it into a format that I want to share with others. I've been doing that using the Broad's [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), which I suspect others use as well. It took me a couple of hours to figure out the details of file formatting, etc, but I realize the process could be easily automated. Given an `adata` object, a list of files can be generated that are formatted correctly for upload. I have written this function and I use it myself. Is `scanpy` interested in a function that does this? Where in the code would such a thing go? I'd be happy to create a pull request, but didn't want to go ahead unless there's interest. Or maybe `anndata` is a better place for something like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950
https://github.com/scverse/scanpy/pull/951:0,deployability,Releas,Release,0,Release notes; - [x] reordered existing ones. - [ ] added new ones. With the new structure we dont even need `:noteversion:` anymore and should just make the headers links to the GitHub releases.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/951
https://github.com/scverse/scanpy/pull/951:187,deployability,releas,releases,187,Release notes; - [x] reordered existing ones. - [ ] added new ones. With the new structure we dont even need `:noteversion:` anymore and should just make the headers links to the GitHub releases.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/951
https://github.com/scverse/scanpy/pull/952:164,deployability,updat,updating,164,"Fixup #566 & #706; I think the reason of issues(#566 & #706) is that the value of x*y*z in sqrt(x*y*z) is too large and overflow encountered in long_scalars. After updating these codes, the ValueError disappeared in my PC.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/952
https://github.com/scverse/scanpy/pull/952:164,safety,updat,updating,164,"Fixup #566 & #706; I think the reason of issues(#566 & #706) is that the value of x*y*z in sqrt(x*y*z) is too large and overflow encountered in long_scalars. After updating these codes, the ValueError disappeared in my PC.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/952
https://github.com/scverse/scanpy/pull/952:164,security,updat,updating,164,"Fixup #566 & #706; I think the reason of issues(#566 & #706) is that the value of x*y*z in sqrt(x*y*z) is too large and overflow encountered in long_scalars. After updating these codes, the ValueError disappeared in my PC.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/952
https://github.com/scverse/scanpy/issues/953:211,availability,error,error,211,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:573,availability,Error,Error,573,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3026,availability,error,error,3026,".7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:816,deployability,modul,module,816,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3331,deployability,modul,module,3331,"lib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5560,deployability,Version,Versions,5560," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5593,deployability,log,logging,5593," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5519,energy efficiency,heat,heatmap,5519," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5539,energy efficiency,heat,heatmap,5539," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1800,integrability,wrap,wrapper,1800,"49c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1997,integrability,wrap,wrapper,1997,"w, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix form",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2081,integrability,wrap,wrapper,2081,", xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2278,integrability,wrap,wrapper,2278,"ve, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2448,integrability,filter,filternorm,2448,"n3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2460,integrability,filter,filterrad,2460,"ckages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_to",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4315,integrability,wrap,wrapper,4315,"49c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4512,integrability,wrap,wrapper,4512,"w, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4596,integrability,wrap,wrapper,4596,", xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4793,integrability,wrap,wrapper,4793,"ve, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4963,integrability,filter,filternorm,4963," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4975,integrability,filter,filterrad,4975," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5560,integrability,Version,Versions,5560," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1698,interoperability,bind,bind,1698,"----------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1800,interoperability,wrapper,wrapper,1800,"49c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1997,interoperability,wrapper,wrapper,1997,"w, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix form",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2081,interoperability,wrapper,wrapper,2081,", xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2278,interoperability,wrapper,wrapper,2278,"ve, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2839,interoperability,format,format,2839,"s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2997,interoperability,format,format,2997,"pper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4213,interoperability,bind,bind,4213,"---------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4315,interoperability,wrapper,wrapper,4315,"49c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4512,interoperability,wrapper,wrapper,4512,"w, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4596,interoperability,wrapper,wrapper,4596,", xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4793,interoperability,wrapper,wrapper,4793,"ve, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5374,interoperability,format,format,5374," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:816,modifiability,modul,module,816,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1463,modifiability,pac,packages,1463,"15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1698,modifiability,bind,bind,1698,"----------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1756,modifiability,pac,packages,1756,"t recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:1852,modifiability,paramet,parameter,1852,"odes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.sha",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2037,modifiability,pac,packages,2037,"notations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2133,modifiability,paramet,parameter,2133,"tsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2318,modifiability,pac,packages,2318," ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2433,modifiability,exten,extent,2433,"aconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Pyth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:2664,modifiability,pac,packages,2664,". 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_nod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3331,modifiability,modul,module,3331,"lib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3978,modifiability,pac,packages,3978," sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4213,modifiability,bind,bind,4213,"---------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4271,modifiability,pac,packages,4271," recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4367,modifiability,paramet,parameter,4367,"odes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4552,modifiability,pac,packages,4552,"notations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4648,modifiability,paramet,parameter,4648,"tsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4833,modifiability,pac,packages,4833," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:4948,modifiability,exten,extent,4948," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5179,modifiability,pac,packages,5179," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5560,modifiability,Version,Versions,5560," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:211,performance,error,error,211,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:573,performance,Error,Error,573,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3026,performance,error,error,3026,".7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:211,safety,error,error,211,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:573,safety,Error,Error,573,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:790,safety,input,input-,790,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:816,safety,modul,module,816,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3026,safety,error,error,3026,".7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3304,safety,input,input-,3304,"on3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecatio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3331,safety,modul,module,3331,"lib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5593,safety,log,logging,5593," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5593,security,log,logging,5593," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:746,testability,Trace,Traceback,746,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3260,testability,Trace,Traceback,3260,". 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5593,testability,log,logging,5593," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:63,usability,clear,clear,63,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:211,usability,error,error,211,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:232,usability,minim,minimal,232,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:573,usability,Error,Error,573,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:790,usability,input,input-,790,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:907,usability,Document,Documents,907,"sc.pl.paga_path not working with TypeError; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. I run scanpy in Python 3.7, matplotlib=3.1.1 - `sc.pl.paga_path` gives me the following error(s). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.dpt(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-5-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3026,usability,error,error,3026,".7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3304,usability,input,input-,3304,"on3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecatio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:3422,usability,Document,Documents,3422,"igin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 688 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):. 689 raise TypeError(""Invalid shape {} for image data"". --> 690 .format(self._A.shape)). 691 . 692 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data. ```. If I convert the `adata.X` to sparse matrix format, I have the following error:. ```python. adata.X = sci.sparse.csr_matrix(adata.X). sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ```. ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-29-a9471349c389> in <module>. ----> 1 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). ~/Documents/Python/scanpy/scanpy/plotting/_tools/paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax). 1057 if as_heatmap:. 1058 img = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/953:5756,usability,learn,learn,5756," = ax.imshow(. -> 1059 X, aspect='auto', interpolation='nearest', cmap=color_map. 1060 ). 1061 if show_yticks:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs). 1599 def inner(ax, *args, data=None, **kwargs):. 1600 if data is None:. -> 1601 return func(ax, *map(sanitize_sequence, args), **kwargs). 1602 . 1603 bound = new_sig.bind(ax, *args, **kwargs). ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs). 367 f""%(removal)s. If any parameter follows {name!r}, they "". 368 f""should be pass as keyword, not positionally.""). --> 369 return func(*args, **kwargs). 370 . 371 return wrapper. ~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs). 5669 resample=resample, **kwargs). 5670 . -> 5671 im.set_data(X). 5672 im.set_alpha(alpha). 5673 if im.get_clip_path() is None:. ~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py in set_data(self, A). 683 not np.can_cast(self._A.dtype, float, ""same_kind"")):. 684 raise TypeError(""Image data of dtype {} cannot be converted to "". --> 685 ""float"".format(self._A.dtype)). 686 . 687 if not (self._A.ndim == 2. TypeError: Image data of dtype object cannot be converted to float. ```. Plotting a heatmap with `sc.pl.heatmap` works. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. ```. scanpy==1.4.5.dev137+ge46f89b anndata==0.6.22.post2.dev73+g00b4b91 umap==0.3.8 numpy==1.17.3 scipy==1.2.1 pandas==0.25.2 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. > .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953
https://github.com/scverse/scanpy/issues/954:292,availability,cluster,clusters,292,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/954:292,deployability,cluster,clusters,292,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/954:585,deployability,continu,continuous,585,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/954:750,energy efficiency,Current,Currently,750,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/954:159,modifiability,paramet,parameters,159,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/954:373,usability,command,command,373,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/954:448,usability,help,help,448,"Ordering of selected discrete colors in scatter plots when using 'groups='; <!-- What kind of feature would you like to request? -->. [x ] Additional function parameters / changed functionality / changed defaults? <!-- Please describe your wishes below: -->. When we have discrete colors for clusters or samples, scatter plots (and therefore umap, tsne plots etc.) using a command like this `sc.pl.scatter(adata, color='sample', groups=['A'])` can help emphasize them by plotting only selected colors and all other cells in gray (see attached). This is very useful. But much like with continuous coloring, the order of colors matters. And particularly here, it would be most useful if all cells with selected colors are ""in front"" of all gray cells. Currently some colored selected cells are often invisible behind gray cells, like in the attached example. [umap-example.pdf](https://github.com/theislab/scanpy/files/3965139/umap-example.pdf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/954
https://github.com/scverse/scanpy/issues/955:25,availability,cluster,clusters,25,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:583,availability,cluster,clustering,583,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:655,availability,cluster,clusters,655,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:25,deployability,cluster,clusters,25,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:583,deployability,cluster,clustering,583,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:655,deployability,cluster,clusters,655,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:119,modifiability,paramet,parameters,119,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:396,modifiability,pac,package,396,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1239,safety,test,test,1239,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1287,safety,test,test,1287,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1310,safety,test,test,1310,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1328,safety,test,test,1328,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1359,safety,test,test,1359,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:201,testability,simpl,simple,201,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:611,testability,simpl,simple,611,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1239,testability,test,test,1239,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1287,testability,test,test,1287,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1310,testability,test,test,1310,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1328,testability,test,test,1328,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1359,testability,test,test,1359,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1655,testability,simpl,simple,1655,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:193,usability,tool,tool,193,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:201,usability,simpl,simple,201,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:217,usability,tool,tool,217,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:265,usability,tool,tools,265,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:365,usability,tool,tools,365,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:611,usability,simpl,simple,611,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1414,usability,user,user-images,1414,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1655,usability,simpl,simple,1655,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/955:1681,usability,help,helper,1681,"Small multiple plots for clusters; <!-- What kind of feature would you like to request? -->. - [ ] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [x] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Hey @fidelram ! I just wrote something to create small multiples to plot cells in a clustering category. Pretty simple and very useful if you have too many clusters. What do you think of this:. ```. def cluster_small_multiples(adata, clust_key, size=60, frameon=False, legend_loc=None, **kwargs):. tmp = adata.copy(). for i,clust in enumerate(adata.obs[clust_key].cat.categories):. tmp.obs[clust] = adata.obs[clust_key].isin([clust]).astype('category'). tmp.uns[clust+'_colors'] = ['#d3d3d3', adata.uns[clust_key+'_colors'][i]]. sc.pl.umap(tmp, groups=tmp.obs[clust].cat.categories[1:].values, color=adata.obs[clust_key].cat.categories.tolist(), size=size, frameon=frameon, legend_loc=legend_loc, **kwargs). ```. Example output from:. ```. test = sc.datasets.pbmc68k_reduced(). sc.pp.pca(test). sc.pp.neighbors(test). sc.tl.umap(test). cluster_small_multiples(test, 'bulk_labels'). ```. ![umap_bulk_lab_sm](https://user-images.githubusercontent.com/13019956/70931843-a19e8780-2038-11ea-8549-2f7820636c41.png). Could generalize this to different bases via `sc.pl.scatter()`. Or is this already implemented somewhere that I'm not aware of? Or maybe it's too simple to have as a small helper function?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955
https://github.com/scverse/scanpy/issues/956:698,availability,cluster,clusters,698,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:875,availability,cluster,clusters,875,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:698,deployability,cluster,clusters,698,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:875,deployability,cluster,clusters,875,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:515,energy efficiency,reduc,reduce,515,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:132,modifiability,paramet,parameters,132,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:409,modifiability,pac,package,409,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:0,safety,Avoid,Avoid,0,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:214,testability,simpl,simple,214,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:206,usability,tool,tool,206,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:214,usability,simpl,simple,214,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:230,usability,tool,tool,230,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:278,usability,tool,tools,278,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/issues/956:378,usability,tool,tools,378,"Avoid argument explosion in plotting functions; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. To reduce the number of arguments that are passed to plotting functions and to agrupate them by type I was considering the following example syntax:. ```PYTHON. sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1). .legend(loc='on data', outline=1). .add_edges(color='black', width=0.1). ```. or . ```. sc.pl.dotplot(adata, ['gene1', 'gene2'], groupby='clusters'). .add_dendrogram(width=0.4,color='grey'). .swap_axes(). .dot_size_legend(title='fraction', location='left'). ```. Any comments? . (I am not sure how to implement something like this)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956
https://github.com/scverse/scanpy/pull/957:44,energy efficiency,cool,cool,44,Add key_added option to sc.tl.paga; It'd be cool to allow multiple abstract representations of the same graph (e.g. w.r.t different groups). What do you think about storing PAGA info under `adata.uns['paga'][key_added]` instead of directly `adata.uns['paga']`?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957
https://github.com/scverse/scanpy/pull/957:67,integrability,abstract,abstract,67,Add key_added option to sc.tl.paga; It'd be cool to allow multiple abstract representations of the same graph (e.g. w.r.t different groups). What do you think about storing PAGA info under `adata.uns['paga'][key_added]` instead of directly `adata.uns['paga']`?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957
https://github.com/scverse/scanpy/pull/957:67,modifiability,abstract,abstract,67,Add key_added option to sc.tl.paga; It'd be cool to allow multiple abstract representations of the same graph (e.g. w.r.t different groups). What do you think about storing PAGA info under `adata.uns['paga'][key_added]` instead of directly `adata.uns['paga']`?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957
https://github.com/scverse/scanpy/issues/958:20,availability,cluster,clusters,20,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:41,availability,cluster,cluster,41,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:76,availability,cluster,cluster,76,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:306,availability,cluster,cluster,306,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:333,availability,cluster,clusters,333,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:538,availability,cluster,cluster,538,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:20,deployability,cluster,clusters,20,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:41,deployability,cluster,cluster,41,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:76,deployability,cluster,cluster,76,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:306,deployability,cluster,cluster,306,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:333,deployability,cluster,clusters,333,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:538,deployability,cluster,cluster,538,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:0,integrability,Filter,Filter,0,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:206,integrability,filter,filter,206,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:11,interoperability,specif,specific,11,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/issues/958:154,performance,time,time,154,"Filter out specific clusters using their cluster number; There is a certain cluster of cells that I would like to remove. They seem to mess up the pseudo time. Is it possible to do this? I know that we can filter cells based on gene counts. But in this case, I would like to remove them based on annotated cluster labels. adata.obs[clusters].cat.categories. Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,. 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,. 25],. dtype=object). The cluster I wan to remove is 25. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/958
https://github.com/scverse/scanpy/pull/959:4,modifiability,layer,layer,4,"Add layer support to sc.tl.score_genes; ... so that we can do crazy things like scoring velocities or scoring ""unspliced expression"".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/959
https://github.com/scverse/scanpy/pull/959:10,usability,support,support,10,"Add layer support to sc.tl.score_genes; ... so that we can do crazy things like scoring velocities or scoring ""unspliced expression"".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/959
https://github.com/scverse/scanpy/pull/960:24,deployability,log,logging,24,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:67,deployability,releas,release,67,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:92,deployability,log,logging,92,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:109,modifiability,paramet,parameters,109,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:24,safety,log,logging,24,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:92,safety,log,logging,92,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:24,security,log,logging,24,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:92,security,log,logging,92,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:24,testability,log,logging,24,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/pull/960:92,testability,log,logging,92,"webpage overhaul, docs, logging, default params for 1.4.5; Prepare release 1.4.5 with docs, logging, default parameters.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/960
https://github.com/scverse/scanpy/issues/961:1425,availability,avail,available,1425,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2241,availability,cluster,clustering,2241,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2270,availability,cluster,clustering,2270,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2313,availability,cluster,clustering,2313,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:0,deployability,Instal,Install,0,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:31,deployability,modul,module,31,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:383,deployability,modul,module,383,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:575,deployability,modul,module,575,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:600,deployability,API,API,600,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:885,deployability,modul,module,885,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1169,deployability,modul,module,1169,"graph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1397,deployability,modul,module,1397,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1675,deployability,modul,module,1675,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1880,deployability,modul,module,1880,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1902,deployability,version,version,1902,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1989,deployability,Contain,Contains,1989,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2084,deployability,modul,module,2084,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2241,deployability,cluster,clustering,2241,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2257,deployability,modul,module,2257,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2270,deployability,cluster,clustering,2270,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2313,deployability,cluster,clustering,2313,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1776,energy efficiency,Optim,Optimiser,1776,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:600,integrability,API,API,600,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1902,integrability,version,version,1902,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:600,interoperability,API,API,600,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:31,modifiability,modul,module,31,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:178,modifiability,pac,package,178,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:383,modifiability,modul,module,383,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:543,modifiability,pac,packages,543,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:575,modifiability,modul,module,575,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:847,modifiability,pac,packages,847,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:885,modifiability,modul,module,885,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1132,modifiability,pac,packages,1132,"low for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1169,modifiability,modul,module,1169,"graph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1362,modifiability,pac,packages,1362,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1397,modifiability,modul,module,1397,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1639,modifiability,pac,packages,1639,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1675,modifiability,modul,module,1675,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1838,modifiability,pac,packages,1838,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1880,modifiability,modul,module,1880,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1902,modifiability,version,version,1902,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2084,modifiability,modul,module,2084,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2257,modifiability,modul,module,2257,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1425,reliability,availab,available,1425,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:31,safety,modul,module,31,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:356,safety,input,input-,356,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:383,safety,modul,module,383,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:575,safety,modul,module,575,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:885,safety,modul,module,885,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1169,safety,modul,module,1169,"graph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1265,safety,except,except,1265,"---------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module unde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1397,safety,modul,module,1397,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1425,safety,avail,available,1425,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1675,safety,modul,module,1675,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1880,safety,modul,module,1880,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2084,safety,modul,module,2084,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:2257,safety,modul,module,2257,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1425,security,availab,available,1425,"ast). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might be wrong, but looks like Scanpy is directly calling igraph.vertexclustering, while vertex clustering is a module under clustering. Shouldn't it be referred as ig.clustering.vertexclustering?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:312,testability,Trace,Traceback,312,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:356,usability,input,input-,356,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:684,usability,tool,tools,684,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:726,usability,tool,tools,726,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:863,usability,tool,tools,863,"Install Issue: AttributeError: module 'igraph' has no attribute 'VertexClustering'; Hello,. Importing scanpy unsuccessful. Please see below for details. This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/issues/961:1148,usability,tool,tools,1148,". This is after the Igraph package is successfully imported. . ```. `---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-13-a0665160cba0> in <module>. 1 import anndata. ----> 2 import scanpy as sc. 3 import igraph. 4 . 5 C6665_new = anndata.AnnData(C6665_encoded). ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\__init__.py in <module>. 30 # the actual API. 31 from ._settings import settings, Verbosity # start with settings as several tools are using it. ---> 32 from . import tools as tl. 33 from . import preprocessing as pp. 34 from . import plotting as pl. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\__init__.py in <module>. 8 from ._rank_genes_groups import rank_genes_groups, filter_rank_genes_groups. 9 from ._dpt import dpt. ---> 10 from ._leiden import leiden. 11 from ._louvain import louvain. 12 from ._sim import sim. ~\anaconda3\envs\ms-sy-code\lib\site-packages\scanpy\tools\_leiden.py in <module>. 13 . 14 try:. ---> 15 from leidenalg.VertexPartition import MutableVertexPartition. 16 except ImportError:. 17 class MutableVertexPartition: pass. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\__init__.py in <module>. 33 not immediately available in :func:`leidenalg.find_partition`. 34 """""". ---> 35 from .functions import ALL_COMMS. 36 from .functions import ALL_NEIGH_COMMS. 37 from .functions import RAND_COMM. ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\functions.py in <module>. 21 return graph.__graph_as_cobject(). 22 . ---> 23 from .VertexPartition import *. 24 from .Optimiser import *. 25 . ~\anaconda3\envs\ms-sy-code\lib\site-packages\leidenalg\VertexPartition.py in <module>. 6 PY3 = (sys.version > '3'). 7 . ----> 8 class MutableVertexPartition(_ig.VertexClustering):. 9 """""" Contains a partition of graph, derives from :class:`ig.VertexClustering`. 10 . AttributeError: module 'igraph' has no attribute 'VertexClustering'`. ```. I might ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961
https://github.com/scverse/scanpy/pull/963:266,availability,error,error,266,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:498,availability,error,error,498,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:621,deployability,log,logic,621,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:266,performance,error,error,266,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:498,performance,error,error,498,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:266,safety,error,error,266,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:498,safety,error,error,498,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:621,safety,log,logic,621,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:684,safety,test,test,684,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:621,security,log,logic,621,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:621,testability,log,logic,621,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:684,testability,test,test,684,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:266,usability,error,error,266,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/963:498,usability,error,error,498,"Fix test_preprocessing_distributed; I don't know what changes caused this, but now there are 2 problems with test_preprocessing_distributed.py. When `adata_dist.X` is a dask array, `adata_dist.X.chunks` is `((2000, 2000, 2000, 2000, 2000), (1000,))`. It leads to an error in `adata.write_zarr(temp_store, chunks)` because zarr chunks should be a tuple with an integer entry per dimension, not a tuple of tuples. The second problem is that `adata_dist.X.to_zarr(temp_store.dir_path(""X""))` causes an error because there is already `'X'` in `temp_store`, it needs to be overwritten. This pr removes these problems but maybe logic of the function should be changed somehow instead of the test.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/963
https://github.com/scverse/scanpy/pull/964:7,testability,simpl,simplification,7,Ingest simplification: remove return_joint;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/964
https://github.com/scverse/scanpy/pull/964:7,usability,simpl,simplification,7,Ingest simplification: remove return_joint;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/964
https://github.com/scverse/scanpy/pull/965:7,testability,simpl,simplification,7,"Ingest simplification, more convenience embedding_density;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/965
https://github.com/scverse/scanpy/pull/965:7,usability,simpl,simplification,7,"Ingest simplification, more convenience embedding_density;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/965
https://github.com/scverse/scanpy/issues/967:1135,availability,error,error,1135,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:42,deployability,log,log-transformed,42,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:236,deployability,log,log-transformed,236,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:462,deployability,log,log-transformed,462,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:606,deployability,log,log-transform,606,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1101,deployability,log,log,1101,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1327,deployability,Version,Versions,1327,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1360,deployability,log,logging,1360,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:46,integrability,transform,transformed,46,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:240,integrability,transform,transformed,240,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:466,integrability,transform,transformed,466,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:610,integrability,transform,transform,610,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1049,integrability,transform,transformations,1049,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1327,integrability,Version,Versions,1327,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:46,interoperability,transform,transformed,46,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:240,interoperability,transform,transformed,240,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:466,interoperability,transform,transformed,466,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:610,interoperability,transform,transform,610,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1049,interoperability,transform,transformations,1049,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1281,modifiability,layer,layer,1281,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1327,modifiability,Version,Versions,1327,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1135,performance,error,error,1135,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:486,reliability,doe,does,486,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1187,reliability,pra,practice,1187,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:42,safety,log,log-transformed,42,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:236,safety,log,log-transformed,236,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:462,safety,log,log-transformed,462,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:497,safety,test,test,497,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:606,safety,log,log-transform,606,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1101,safety,log,log,1101,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1135,safety,error,error,1135,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1360,safety,log,logging,1360,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:42,security,log,log-transformed,42,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:236,security,log,log-transformed,236,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:462,security,log,log-transformed,462,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:606,security,log,log-transform,606,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1101,security,log,log,1101,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1360,security,log,logging,1360,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:42,testability,log,log-transformed,42,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:236,testability,log,log-transformed,236,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:462,testability,log,log-transformed,462,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:497,testability,test,test,497,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:606,testability,log,log-transform,606,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1101,testability,log,log,1101,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1360,testability,log,logging,1360,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:83,usability,clear,clear,83,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:171,usability,document,documentation,171,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:588,usability,undo,undo,588,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1135,usability,error,error,1135,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1197,usability,guid,guide,1197,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1307,usability,document,documentation,1307,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/issues/967:1496,usability,learn,learn,1496,"missing docs: rank_genes_groups expecting log-transformed data; <!-- Please give a clear and concise description of what the bug is: -->. Hi,. not really a bug, more of a documentation issue: . `sc.tl.rank_genes_groups` seems to expect log-transformed data (be it in `.X` or `.raw.X`). To my knowledge this is **not mentioned in the docs**. I just ran into trouble and then found out via #671 and #517 . . It's not a problem for the p-values (if the data is not log-transformed it just does the t-test etc on the counts), but the resulting fold-changes are wrong (it essentially tries to undo the expected log-transform):. ```python. foldchanges = (np.expm1(mean_group) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. I was totally unaware of this (been using scanpy for quite a while), especially since I usually store the plain raw counts in the `adata.raw` field, which is used *by default* in `rank_genes_groups`. We should at least mention it in the docstring, but these things are easy to overlook too... Is there any way that scanpy records the transformations you've done to the data (and if the log is missing, just spits out an error in rank_genes_groups)? Also, is there a ""best-practice"" guide of what data to store in which parts of the AnnData objects? `.raw` and the `.layer` dont have a lot of documentation. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967
https://github.com/scverse/scanpy/pull/968:195,availability,state,states,195,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:470,deployability,stack,stackoverflow,470,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:114,energy efficiency,current,currently,114,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:195,integrability,state,states,195,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:104,modifiability,paramet,parameter,104,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:328,performance,time,times,328,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:228,safety,valid,valid,228,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:181,usability,document,documentation,181,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/pull/968:305,usability,prefer,preferable,305,"fixed legend_loc being ignored if not 'on data' or 'right margin'; as mentioned in #761, the legend_loc parameter currently is ignored unless 'on data' or 'right margin', while the documentation states that it should handle any valid argument to `plt.legend(loc=...)`. Rarely useful (`right margin` seems preferable most of the times), but unfortunately/annoyingly matplotlib's `plt.savefig()` wont save the legend if its plotted with `right margin` (see [here](https://stackoverflow.com/questions/8971834/matplotlib-savefig-with-a-legend-outside-the-plot)). Hence for saving plots , it might be useful!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/968
https://github.com/scverse/scanpy/issues/969:438,deployability,log,log-transformed,438,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/969:442,integrability,transform,transformed,442,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/969:442,interoperability,transform,transformed,442,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/969:438,safety,log,log-transformed,438,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/969:438,security,log,log-transformed,438,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/969:438,testability,log,log-transformed,438,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/969:668,testability,plan,plans,668,"cell_ranger HVG flavor inconsistent with 10x code; It appears that in the cell ranger code, the dispersion is calculated using the negative binomial relationship between mean and dispersion, see. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L44. Furthermore, these summary statistics are calculated on the count matrix normalized by library size, but not log-transformed. https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/pca.py#L91-L95. As a follow-up, the ""Seurat"" flavor seems to be no longer used in Seurat. Any plans to implement their ""vst"" method?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969
https://github.com/scverse/scanpy/issues/970:2,safety,test,test,2,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:345,safety,test,test,345,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:89,security,sign,significance,89,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:317,security,sign,significant,317,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:2,testability,test,test,2,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:336,testability,simpl,simple,336,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:345,testability,test,test,345,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:336,usability,simpl,simple,336,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:393,usability,user,user-images,393,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/issues/970:509,usability,help,help,509,"t test with violin plot; Hi islab,. I was wondering if there is a way to get statistical significance for a given gene expression between 2 groups on a violin plot or make a calculation separately? for example:. here is the expression of GZMB between 2 groups and O would like to see if the difference is statistical significant with a simple t-test. <img width=""164"" alt=""image"" src=""https://user-images.githubusercontent.com/29153026/71746338-7631e000-2e21-11ea-95db-0aef5fe6dba7.png"">. Thank you fore your help,. Shen.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/970
https://github.com/scverse/scanpy/pull/971:81,interoperability,format,format,81,A few minor style changes via `pyupgrade`; I ran:. ```. pyupgrade --keep-percent-format --py3-plus scanpy/*py scanpy/*/*py scanpy/*/*/*py. ```,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/971
https://github.com/scverse/scanpy/issues/972:87,availability,restor,restored,87,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:759,availability,restor,restored,759,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:129,deployability,fail,failing,129,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:250,deployability,build,builds,250,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:87,reliability,restor,restored,87,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:129,reliability,fail,failing,129,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:759,reliability,restor,restored,759,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:690,safety,prevent,prevented,690,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:812,safety,review,reviewer,812,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:690,security,preven,prevented,690,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:812,testability,review,reviewer,812,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/972:112,usability,behavi,behavior,112,"Fix docs; PR #966 broke the docs but we missed it because it set `nitpicky = False`. I restored the intentional behavior of docs failing when theyre broken, so we can now see the breakage caused by #966:. https://readthedocs.com/projects/icb-scanpy/builds/273340/. `/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/checkouts/latest/scanpy/neighbors/__init__.py:docstring of scanpy.Neighbors:6:py:class reference target not found: anndata._core.anndata.AnnData`. This can easily be fixed by adding the right mapping to `qualname_overrides` in conf.py, or changing scanpydocs default for `qualname_overrides`. Im also pretty sure there were branch protection rules in place that prevented such changes happening to master. Why were they removed? I restored one that reflects what we agreed upon:. - 1 reviewer has to accept a PR. - Travis needs to succeed. - No merge commits. Please do not remove or change the branch protection rules before discussing that change.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/972
https://github.com/scverse/scanpy/issues/973:210,deployability,modul,module,210,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2194,deployability,Version,Versions,2194,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2227,deployability,log,logging,2227,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:297,integrability,batch,batch,297,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:336,integrability,batch,batch,336,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2150,integrability,batch,batch,2150,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2159,integrability,batch,batch,2159,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2170,integrability,batch,batch,2170,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2194,integrability,Version,Versions,2194,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:210,modifiability,modul,module,210,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:458,modifiability,pac,packages,458,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:869,modifiability,pac,packages,869,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:1317,modifiability,Pac,Packing,1317,"ta[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:1402,modifiability,pac,packages,1402,"a(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2194,modifiability,Version,Versions,2194,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:226,performance,time,time,226,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:231,performance,time,time,231,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:297,performance,batch,batch,297,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:336,performance,batch,batch,336,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:365,performance,time,time,365,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:370,performance,time,time,370,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:398,performance,time,timedelta,398,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2150,performance,batch,batch,2150,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2159,performance,batch,batch,2159,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2170,performance,batch,batch,2170,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:183,safety,input,input-,183,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:210,safety,modul,module,210,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2227,safety,log,logging,2227,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2227,security,log,logging,2227,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:139,testability,Trace,Traceback,139,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2227,testability,log,logging,2227,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:73,usability,clear,clear,73,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:183,usability,input,input-,183,"When I run sce.pp.mnn_correct, there is a IndexError; <!-- Please give a clear and concise description of what the bug is: -->. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!--",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2007,usability,minim,minimal,2007,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/973:2354,usability,learn,learn,2354,"). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. ... ```. sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > ... scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/973
https://github.com/scverse/scanpy/issues/974:278,deployability,modul,module,278,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2275,deployability,Version,Versions,2275,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2308,deployability,log,logging,2308,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:365,integrability,batch,batch,365,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:404,integrability,batch,batch,404,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2226,integrability,batch,batch,2226,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2235,integrability,batch,batch,2235,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2246,integrability,batch,batch,2246,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2275,integrability,Version,Versions,2275,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:278,modifiability,modul,module,278,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:526,modifiability,pac,packages,526,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:937,modifiability,pac,packages,937,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:1385,modifiability,Pac,Packing,1385,"ta[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:1470,modifiability,pac,packages,1470,"a(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-ig",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2275,modifiability,Version,Versions,2275,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:294,performance,time,time,294,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:299,performance,time,time,299,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:365,performance,batch,batch,365,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:404,performance,batch,batch,404,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:433,performance,time,time,433,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:438,performance,time,time,438,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:466,performance,time,timedelta,466,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2226,performance,batch,batch,2226,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2235,performance,batch,batch,2235,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2246,performance,batch,batch,2246,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:251,safety,input,input-,251,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:278,safety,modul,module,278,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2308,safety,log,logging,2308,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2308,security,log,logging,2308,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:207,testability,Trace,Traceback,207,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2308,testability,log,logging,2308,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:77,usability,clear,clear,77,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:251,usability,input,input-,251,"When I ran sce.pp.mnn_correct(), there was an IndexError; <!-- Please give a clear and concise description of what the bug is: -->. ```pytb. -----------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-22-408c81d5d845> in <module>. 1 t1 = time.time(). ----> 2 corrected = sce.pp.mnn_correct(scdata[scdata.obs['batch']==batch_[0]],scdata[scdata.obs['batch']==batch_[1]]). 3 t2 = time.time(). 4 print('Took '+str(timedelta(seconds=t2-t1))). ~/miniconda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexErro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2080,usability,minim,minimal,2080,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/issues/974:2431,usability,learn,learn,2431,"onda3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 152 save_raw=save_raw,. 153 n_jobs=n_jobs,. --> 154 **kwargs,. 155 ). 156 return datas, mnn_list, angle_list. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. ~/miniconda3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->. ```python. corrected = sce.pp.mnn_correct(*[scdata[scdata.obs['batch']==batch] for batch in batch_]). ```. #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. > scanpy==1.4.5 anndata==0.7rc1 umap==0.3.7 numpy==1.18.0 scipy==1.3.1 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974
https://github.com/scverse/scanpy/pull/975:75,performance,time,time,75,Harmony_timeseries - framework for connecting scRNA-seq data from discrete time points; Harmony time series framework for connecting scRNA-seq data from discrete time points,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/975
https://github.com/scverse/scanpy/pull/975:96,performance,time,time,96,Harmony_timeseries - framework for connecting scRNA-seq data from discrete time points; Harmony time series framework for connecting scRNA-seq data from discrete time points,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/975
https://github.com/scverse/scanpy/pull/975:162,performance,time,time,162,Harmony_timeseries - framework for connecting scRNA-seq data from discrete time points; Harmony time series framework for connecting scRNA-seq data from discrete time points,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/975
https://github.com/scverse/scanpy/pull/976:75,performance,time,time,75,"Harmony_timeseries - framework for connecting scRNA-seq data from discrete time points; Harmony_timeseries, a framework for connecting scRNA-seq data from discrete time points",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/976
https://github.com/scverse/scanpy/pull/976:164,performance,time,time,164,"Harmony_timeseries - framework for connecting scRNA-seq data from discrete time points; Harmony_timeseries, a framework for connecting scRNA-seq data from discrete time points",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/976
https://github.com/scverse/scanpy/issues/977:219,performance,time,times,219,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:366,performance,cach,cache,366,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:797,safety,test,test,797,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:833,safety,test,test,833,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:864,safety,Test,Tests,864,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:797,testability,test,test,797,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:833,testability,test,test,833,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:864,testability,Test,Tests,864,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/977:879,usability,user,user-images,879,"Stochastic effects in violin plotting function with jitter; Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`. `adata.var_names_make_unique()`. `sc.pp.filter_cells(adata, min_genes=200)`. `sc.pp.filter_genes(adata, min_cells=3)`. `mito_genes = adata.var_names.str.startswith('MT-')`. `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`. `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem? Thank you in advance,. Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977
https://github.com/scverse/scanpy/issues/978:21,availability,error,error,21,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:115,availability,error,error,115,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1895,availability,error,errors,1895,"282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3065,availability,error,error,3065,"mba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anacon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4971,availability,error,errors,4971,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5162,availability,error,error,5162,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5318,availability,error,error,5318,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:94,deployability,version,version,94,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:564,deployability,modul,module,564,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2444,deployability,Fail,Failed,2444,"ython3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2468,deployability,pipelin,pipeline,2468,"anpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2508,deployability,Fail,Failed,2508,"t_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2532,deployability,pipelin,pipeline,2532,"64 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4766,deployability,releas,release,4766,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:94,integrability,version,version,94,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2468,integrability,pipelin,pipeline,2468,"anpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2532,integrability,pipelin,pipeline,2532,"64 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5168,integrability,messag,message,5168,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5168,interoperability,messag,message,5168,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:94,modifiability,version,version,94,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:564,modifiability,modul,module,564,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:664,modifiability,pac,packages,664,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:721,modifiability,pac,packages,721,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:832,modifiability,layer,layer,832,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:976,modifiability,pac,packages,976,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1033,modifiability,pac,packages,1033,"1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1136,modifiability,layer,layer,1136,". ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 els",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1404,modifiability,pac,packages,1404,"------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1461,modifiability,pac,packages,1461,"---------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1688,modifiability,pac,packages,1688,"scanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1745,modifiability,pac,packages,1745,"g/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2000,modifiability,pac,packages,2000,"scanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2057,modifiability,pac,packages,2057,"g/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. T",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2205,modifiability,pac,packages,2205,"op = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2262,modifiability,pac,packages,2262,"nt_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2663,modifiability,paramet,parameterized,2663,"7 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2775,modifiability,pac,packages,2775,"ompile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2832,modifiability,pac,packages,2832,"msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2966,modifiability,pac,packages,2966,"e user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3023,modifiability,pac,packages,3023,"tscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3277,modifiability,pac,packages,3277,"py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3334,modifiability,pac,packages,3334,"lue.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3399,modifiability,pac,packages,3399,"tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3456,modifiability,pac,packages,3456,"on mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3787,modifiability,pac,packages,3787,"a3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3844,modifiability,pac,packages,3844,"rray_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3920,modifiability,pac,packages,3920,"ndex must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3977,modifiability,pac,packages,3977,"da3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4053,modifiability,pac,packages,4053,"2. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/tro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4110,modifiability,pac,packages,4110," a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4186,modifiability,pac,packages,4186,"lee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4243,modifiability,pac,packages,4243,"yping of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4319,modifiability,pac,packages,4319,"on3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4376,modifiability,pac,packages,4376,"le ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4471,modifiability,pac,packages,4471,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4528,modifiability,pac,packages,4528,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:21,performance,error,error,21,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:115,performance,error,error,115,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:857,performance,parallel,parallel,857,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1164,performance,parallel,parallel,1164,"=0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1895,performance,error,errors,1895,"282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3065,performance,error,error,3065,"mba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anacon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4971,performance,error,errors,4971,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5162,performance,error,error,5162,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5318,performance,error,error,5318,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2444,reliability,Fail,Failed,2444,"ython3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:2508,reliability,Fail,Failed,2508,"t_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5080,reliability,doe,doesn-t-compile,5080,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:21,safety,error,error,21,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:115,safety,error,error,115,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:538,safety,input,input-,538,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:564,safety,modul,module,564,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1888,safety,except,except,1888,"_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. Valu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1895,safety,error,errors,1895,"282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3065,safety,error,error,3065,"mba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anacon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4971,safety,error,errors,4971,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5162,safety,error,error,5162,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5318,safety,error,error,5318,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:494,testability,Trace,Traceback,494,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5181,testability,trace,traceback,5181,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:21,usability,error,error,21,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:115,usability,error,error,115,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:238,usability,learn,learn,238,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:538,usability,input,input-,538,"calculate_qc_metrics error in scanpy 1.4.5 and above; Hi, I was trying to use the most recent version but saw this error in 1.4.5 and above. ```. scanpy==1.4.5 anndata==0.7rc2 umap==0.3.10 numpy==1.17.3 scipy==1.4.1 pandas==0.25.3 scikit-learn==0.22.1 statsmodels==0.11.0rc1 python-igraph==0.7.1. ```. ```. adata = sc.datasets.pbmc3k(). sc.pp.calculate_qc_metrics(adata, inplace=True). ```. output:. ```. ---------------------------------------------------------------------------. TypingError Traceback (most recent call last). <ipython-input-5-0d8cf2779f18> in <module>. 1 adata = sc.datasets.pbmc3k(). ----> 2 sc.pp.calculate_qc_metrics(adata, inplace=True). ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel). 281 percent_top=percent_top,. 282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1895,usability,error,errors,1895,"282 inplace=inplace,. --> 283 X=X,. 284 ). 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1972,usability,user,user,1972,"~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:1987,usability,help,help,1987,"onda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel). 107 if percent_top:. 108 percent_top = sorted(percent_top). --> 109 proportions = top_segment_proportions(X, percent_top). 110 for i, n in enumerate(percent_top):. 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns). 364 mtx = csr_matrix(mtx). 365 return top_segment_proportions_sparse_csr(. --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int). 367 ). 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws). 399 e.patch_message(msg). 400 . --> 401 error_rewrite(e, 'typing'). 402 except errors.UnsupportedError as e:. 403 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:3065,usability,error,error,3065,"mba/dispatcher.py in error_rewrite(e, issue_type). 342 raise e. 343 else:. --> 344 reraise(type(e), e, None). 345 . 346 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb). 666 value = tp(). 667 if value.__traceback__ is not tb:. --> 668 raise value.with_traceback(tb). 669 raise value. 670 . TypingError: Failed in nopython mode pipeline (step: nopython mode backend). Failed in nopython mode pipeline (step: nopython frontend). Invalid use of Function(<intrinsic wrap_index>) with argument(s) of type(s): (int32, int64). * parameterized. In definition 0:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. In definition 1:. ValueError: Argument types for wrap_index must match. raised from /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/array_analysis.py:72. This error is usually caused by passing an argument of a type that is unsupported by the named function. [1] During: resolving callee type: Function(<intrinsic wrap_index>). [2] During: typing of call at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (399). File ""../../../../../packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anacon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4742,usability,support,supported,4742,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:4971,usability,error,errors,4971,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5049,usability,user,user,5049,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5162,usability,error,error,5162,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5205,usability,minim,minimal,5205,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5318,usability,error,error,5318,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/978:5412,usability,help,help,5412,"ib/python3.6/site-packages/scanpy/preprocessing/_qc.py"", line 399:. def top_segment_proportions_sparse_csr(data, indptr, ns):. <source elided>. start, end = indptr[i], indptr[i + 1]. sums[i] = np.sum(data[start:end]). ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.271, range = (0, $100.6, 1))]{386: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (403)>, 388: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (404)>, 264: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (401)>, 306: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (402)>, 118: <ir.Block at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397)>}Var(parfor_index.271, _qc.py:397)"" at /home/gzhang/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py (397). This is not usually a problem with Numba itself but instead often caused by. the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:. http://numba.pydata.org/numba-doc/latest/reference/pysupported.html. and. http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:. http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message. and traceback, along with a minimal reproducer at:. https://github.com/numba/numba/issues/new. ```. numba is 0.47.0 but 0.43.1 gave the same error. It seems that ```top_segment_proportions_sparse_csr``` is new for scanpy 1.4.5. Please help. Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978
https://github.com/scverse/scanpy/issues/979:483,deployability,API,API,483,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:954,deployability,Version,Versions,954,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:987,deployability,log,logging,987,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:270,integrability,sub,subtle,270,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:483,integrability,API,API,483,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:954,integrability,Version,Versions,954,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:483,interoperability,API,API,483,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:954,modifiability,Version,Versions,954,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:987,safety,log,logging,987,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:551,security,modif,modify,551,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:648,security,access,access,648,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:987,security,log,logging,987,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:987,testability,log,logging,987,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:66,usability,document,documentation,66,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:102,usability,indicat,indicates,102,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:505,usability,interact,interact,505,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:788,usability,command,commands,788,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/issues/979:1120,usability,learn,learn,1120,"editing output of scanpy dotplot and other figures; In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```. ax = fig.add_subplot(gs[1,0]). ```. but I can't seem to overwrite the default axis labels or add new lines as commands like. ```. ax.set_ylabel('new label'). ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks! #### Versions:. <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1. > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979
https://github.com/scverse/scanpy/pull/980:129,deployability,version,versions,129,"Work around numba bugs. Fixes #978; That hits the spot. @zhangguy suspected in #978 that the enabling of parallel in newer numba versions triggered the numba bugs, which lay dormant until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/980
https://github.com/scverse/scanpy/pull/980:129,integrability,version,versions,129,"Work around numba bugs. Fixes #978; That hits the spot. @zhangguy suspected in #978 that the enabling of parallel in newer numba versions triggered the numba bugs, which lay dormant until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/980
https://github.com/scverse/scanpy/pull/980:129,modifiability,version,versions,129,"Work around numba bugs. Fixes #978; That hits the spot. @zhangguy suspected in #978 that the enabling of parallel in newer numba versions triggered the numba bugs, which lay dormant until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/980
https://github.com/scverse/scanpy/pull/980:105,performance,parallel,parallel,105,"Work around numba bugs. Fixes #978; That hits the spot. @zhangguy suspected in #978 that the enabling of parallel in newer numba versions triggered the numba bugs, which lay dormant until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/980
https://github.com/scverse/scanpy/issues/981:162,deployability,version,versions,162,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/issues/981:242,deployability,version,versions,242,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/issues/981:162,integrability,version,versions,162,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/issues/981:242,integrability,version,versions,242,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/issues/981:255,interoperability,incompatib,incompatible,255,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/issues/981:162,modifiability,version,versions,162,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/issues/981:242,modifiability,version,versions,242,Numba issues - and a potential fix; @flying-sheep I think this could likely be the issue. https://github.com/numba/numba/issues/5035. You could likely only allow versions before numba 0.47.0 and llvmlite 0.31.0. It also seems like the newest versions are incompatible with mnn_correct (#974),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/981
https://github.com/scverse/scanpy/pull/982:28,deployability,version,version,28,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:82,deployability,version,version,82,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:149,deployability,version,versions,149,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:28,integrability,version,version,28,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:82,integrability,version,version,82,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:149,integrability,version,versions,149,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:28,modifiability,version,version,28,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:82,modifiability,version,version,82,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/982:149,modifiability,version,versions,149,"Migrate to actually working version comparison.; We have to get rid of `distutils.version`, neither `LooseVersion` nor `StrictVersion` parse PEP 404 versions as we use them and are therefore useless for us.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/982
https://github.com/scverse/scanpy/pull/983:134,deployability,version,versions,134,Require umap>=0.3.10 to prevent reports like #948; With all the numba breakage going around we should make sure people use the newest versions.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/983
https://github.com/scverse/scanpy/pull/983:134,integrability,version,versions,134,Require umap>=0.3.10 to prevent reports like #948; With all the numba breakage going around we should make sure people use the newest versions.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/983
https://github.com/scverse/scanpy/pull/983:134,modifiability,version,versions,134,Require umap>=0.3.10 to prevent reports like #948; With all the numba breakage going around we should make sure people use the newest versions.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/983
https://github.com/scverse/scanpy/pull/983:24,safety,prevent,prevent,24,Require umap>=0.3.10 to prevent reports like #948; With all the numba breakage going around we should make sure people use the newest versions.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/983
https://github.com/scverse/scanpy/pull/983:24,security,preven,prevent,24,Require umap>=0.3.10 to prevent reports like #948; With all the numba breakage going around we should make sure people use the newest versions.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/983
https://github.com/scverse/scanpy/issues/984:617,availability,avail,available,617,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:137,modifiability,paramet,parameters,137,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:414,modifiability,pac,package,414,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:617,reliability,availab,available,617,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:38,safety,test,testing,38,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:522,safety,test,testing,522,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:617,safety,avail,available,617,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:617,security,availab,available,617,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:38,testability,test,testing,38,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:219,testability,simpl,simple,219,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:522,testability,test,testing,522,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:211,usability,tool,tool,211,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:219,usability,simpl,simple,219,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:235,usability,tool,tool,235,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:283,usability,tool,tools,283,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:383,usability,tool,tools,383,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/984:692,usability,help,helpful,692,"Use multiple groups as reference when testing genes; <!-- What kind of feature would you like to request? -->. - [x] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. When testing for differential genes among groups with `rank_genes_groups` function, two options are available for `reference`: `'rest'` or any other single group. It would be helpful to have the possibility to choose different groups as reference (`reference: Union[Literal['rest'], Iterable[str]] = 'rest'`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984
https://github.com/scverse/scanpy/issues/985:245,deployability,manag,manage,245,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,deployability,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:245,energy efficiency,manag,manage,245,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,integrability,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:205,interoperability,coordinat,coordinates,205,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,interoperability,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,modifiability,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:27,performance,perform,performed,27,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:255,performance,perform,performed,255,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:153,reliability,doe,doesn,153,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,reliability,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:245,safety,manag,manage,245,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,security,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:277,testability,integr,integrated,277,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:27,usability,perform,performed,27,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/985:255,usability,perform,performed,255,"Ingest Concat; Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985
https://github.com/scverse/scanpy/issues/987:13,deployability,observ,observations,13,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:617,deployability,observ,observations,617,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:659,deployability,observ,observation,659,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:0,integrability,Sub,Subsample,0,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:521,integrability,sub,subsample,521,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:561,integrability,sub,subsampling,561,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:583,integrability,sub,subsamples,583,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:856,integrability,sub,subsample,856,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:120,modifiability,paramet,parameters,120,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:397,modifiability,pac,package,397,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:13,testability,observ,observations,13,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:202,testability,simpl,simple,202,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:617,testability,observ,observations,617,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:659,testability,observ,observation,659,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:194,usability,tool,tool,194,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:202,usability,simpl,simple,202,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:218,usability,tool,tool,218,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:266,usability,tool,tools,266,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:366,usability,tool,tools,366,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/issues/987:573,usability,tool,tool,573,"Subsample by observations grouping; <!-- What kind of feature would you like to request? -->. - [X] Additional function parameters / changed functionality / changed defaults? - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`? - [ ] External tools: Do you know an existing package that should go into `sc.external.*`? - [ ] Other? <!-- Please describe your wishes below: -->. Related to scanpy.pp.subsample, it would be useful to have a subsampling tool that subsamples based on the key of an observations grouping. E.g., if I have an observation key 'MyGroup' with possible values ['A', 'B'], and there are 10,000 cells of type 'A' and 2,000 cells of type 'B' and I want only max 5,000 cells of each type, then this function would subsample 5,000 cells of type 'A' but retain all 2,000 cells of type 'B'.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987
https://github.com/scverse/scanpy/pull/988:0,deployability,Updat,Update,0,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:13,deployability,API,API,13,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:78,deployability,updat,updated,78,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:249,deployability,version,version,249,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:13,integrability,API,API,13,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:249,integrability,version,version,249,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:13,interoperability,API,API,13,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:55,modifiability,paramet,parameter,55,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:100,modifiability,paramet,parameters,100,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:249,modifiability,version,version,249,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:0,safety,Updat,Update,0,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:78,safety,updat,updated,78,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:151,safety,test,tests,151,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:0,security,Updat,Update,0,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:78,security,updat,updated,78,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:151,testability,test,tests,151,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/988:187,usability,interact,interacts,187,Update MAGIC API to 2.x; Changeset:. * added new MAGIC parameter `knn_max`. * updated MAGIC default parameters to match KrishnaswamyLab/MAGIC. * added tests to check MAGIC implementation interacts with AnnData object correctly. * increased required version of `magic-impute` to 2.0,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988
https://github.com/scverse/scanpy/pull/989:147,reliability,doe,does,147,Fix 10x mtx reading functions for anndata 0.7; Example of this kind of bug in https://github.com/theislab/anndata/issues/293. * Now `read_10x_mtx` does not set an integer name for obs_name/ var_name indices. * Additionally improved code re-use and things tested in 10x reading tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989
https://github.com/scverse/scanpy/pull/989:255,safety,test,tested,255,Fix 10x mtx reading functions for anndata 0.7; Example of this kind of bug in https://github.com/theislab/anndata/issues/293. * Now `read_10x_mtx` does not set an integer name for obs_name/ var_name indices. * Additionally improved code re-use and things tested in 10x reading tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989
https://github.com/scverse/scanpy/pull/989:277,safety,test,tests,277,Fix 10x mtx reading functions for anndata 0.7; Example of this kind of bug in https://github.com/theislab/anndata/issues/293. * Now `read_10x_mtx` does not set an integer name for obs_name/ var_name indices. * Additionally improved code re-use and things tested in 10x reading tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989
https://github.com/scverse/scanpy/pull/989:255,testability,test,tested,255,Fix 10x mtx reading functions for anndata 0.7; Example of this kind of bug in https://github.com/theislab/anndata/issues/293. * Now `read_10x_mtx` does not set an integer name for obs_name/ var_name indices. * Additionally improved code re-use and things tested in 10x reading tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989
https://github.com/scverse/scanpy/pull/989:277,testability,test,tests,277,Fix 10x mtx reading functions for anndata 0.7; Example of this kind of bug in https://github.com/theislab/anndata/issues/293. * Now `read_10x_mtx` does not set an integer name for obs_name/ var_name indices. * Additionally improved code re-use and things tested in 10x reading tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989
https://github.com/scverse/scanpy/issues/990:26,availability,error,error,26,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:13,deployability,instal,installation,13,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:42,deployability,instal,install,42,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:222,deployability,fail,failed,222,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:491,deployability,fail,failed,491,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:643,deployability,version,version,643,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:720,deployability,version,version,720,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:896,deployability,version,version,896,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:979,deployability,version,version,979,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1311,deployability,version,version,1311,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1489,deployability,version,version,1489,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1600,deployability,version,version,1600,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1675,deployability,version,version,1675,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1841,deployability,version,version,1841,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1995,deployability,version,version,1995,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:643,integrability,version,version,643,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:720,integrability,version,version,720,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:896,integrability,version,version,896,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:979,integrability,version,version,979,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1311,integrability,version,version,1311,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1489,integrability,version,version,1489,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1600,integrability,version,version,1600,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1675,integrability,version,version,1675,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1841,integrability,version,version,1841,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1995,integrability,version,version,1995,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:391,interoperability,conflict,conflicts,391,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:414,interoperability,incompatib,incompatible,414,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:534,interoperability,specif,specifications,534,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:566,interoperability,incompatib,incompatible,566,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:611,interoperability,conflict,conflicts,611,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:683,interoperability,conflict,conflicts,683,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:757,interoperability,conflict,conflicts,757,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:814,interoperability,conflict,conflicts,814,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:864,interoperability,conflict,conflicts,864,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:940,interoperability,conflict,conflicts,940,"scanpy conda installation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[versi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
https://github.com/scverse/scanpy/issues/990:1015,interoperability,conflict,conflicts,1015,"llation error; I cannot install scanpy successfully. (conda v. 4.7.12). $ conda create -n scanpy_scRNA -c bioconda scanpy . Collecting package metadata (current_repodata.json): done. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done. Solving environment: / . Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package numba conflicts for:. scanpy -> numba[version='>=0.41.0']. Package matplotlib conflicts for:. scanpy -> matplotlib[version='3.0.*|>=2.2']. Package h5py conflicts for:. scanpy -> h5py!=2.10.0. Package networkx conflicts for:. scanpy -> networkx. Package scipy conflicts for:. scanpy -> scipy[version='<1.3|>=1.3']. Package scikit-learn conflicts for:. scanpy -> scikit-learn[version='>=0.21.2']. Package joblib conflicts for:. scanpy -> joblib. Package natsort conflicts for:. scanpy -> natsort. Package seaborn conflicts for:. scanpy -> seaborn. Package setuptools conflicts for:. scanpy -> setuptools. Package pytables conflicts for:. scanpy -> pytables. Package anndata conflicts for:. scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']. Package importlib-metadata conflicts for:. scanpy -> importlib-metadata. Package importlib_metadata conflicts for:. scanpy -> importlib_metadata[version='>=0.7']. Package tqdm conflicts for:. scanpy -> tqdm. Package pandas conflicts for:. scanpy -> pandas[version='>=0.21']. Package umap-learn conflicts for:. scanpy -> umap-learn[version='>=0.3.0']. Package patsy conflicts for:. scanpy -> patsy. Package louvain conflicts for:. scanpy -> louvain. Package python conflicts for:. scanpy -> python[version='>=3.6|>=3.6,<3.7.0a0']. Package python-igraph conflicts for:. scanpy -> python-igraph. Package statsmodels conflicts for:. scanpy -> statsmodels[version='>=0.10.0rc2'].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990
