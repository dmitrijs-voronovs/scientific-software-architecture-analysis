id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/allenai/scispacy/issues/336:90,deployability,updat,update,90,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:127,energy efficiency,model,model,127,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:684,energy efficiency,model,model,684,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1678,energy efficiency,model,model,1678,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:7,integrability,version,version,7,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:62,integrability,version,version,62,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:324,interoperability,specif,specific,324,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:7,modifiability,version,version,7,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:62,modifiability,version,version,62,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:192,performance,memor,memorization,192,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:90,safety,updat,update,90,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:90,security,updat,update,90,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:127,security,model,model,127,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:684,security,model,model,684,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1678,security,model,model,1678,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:766,testability,context,context,766,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1707,testability,context,context,1707,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1841,testability,context,contexts,1841,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:192,usability,memor,memorization,192,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:261,usability,help,help,261,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:333,usability,guid,guidelines,333,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/337:2,integrability,coupl,couple,2,"A couple of questions:. 1) What exactly have you tried/doesn't work? I would hope you can still do something similar, even though pipes are added by name now. 2) Is the only difference in your files that you point to custom data files? Or did you make other changes to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2,modifiability,coupl,couple,2,"A couple of questions:. 1) What exactly have you tried/doesn't work? I would hope you can still do something similar, even though pipes are added by name now. 2) Is the only difference in your files that you point to custom data files? Or did you make other changes to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:55,reliability,doe,doesn,55,"A couple of questions:. 1) What exactly have you tried/doesn't work? I would hope you can still do something similar, even though pipes are added by name now. 2) Is the only difference in your files that you point to custom data files? Or did you make other changes to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2,testability,coupl,couple,2,"A couple of questions:. 1) What exactly have you tried/doesn't work? I would hope you can still do something similar, even though pipes are added by name now. 2) Is the only difference in your files that you point to custom data files? Or did you make other changes to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:217,usability,custom,custom,217,"A couple of questions:. 1) What exactly have you tried/doesn't work? I would hope you can still do something similar, even though pipes are added by name now. 2) Is the only difference in your files that you point to custom data files? Or did you make other changes to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:581,deployability,configurat,configuration,581,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:372,energy efficiency,load,load,372,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:581,integrability,configur,configuration,581,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1042,interoperability,architectur,architectures,1042,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:581,modifiability,configur,configuration,581,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:372,performance,load,load,372,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:581,security,configur,configuration,581,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:240,usability,close,closer,240,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:338,usability,custom,custom,338,"Yes, the only changes I made to candidate_generation.py and linking_utils.py was to replace the UmlsLinkerPaths and DEFAULT_UMLS_PATHs/DEFAULT_UMLS_TYPES_PATHs, respective;y, with local file references. . In attempt to make it work (or get closer) I've tried mainly adjusting the config. The following works (if I **don't** try to use my custom files):. ```. _nlp = spacy.load(_data_model). config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=config). _nlp.add_pipe(""abbreviation_detector""). ```. I've tried variations on that configuration such as:. ```. config = {""resolve_abbreviations"": True,. ""candidate_generator""=_candidate_generator. ""linker_name"": ""umls""}. ```. That didn't work because the cg object wasn't serisalizable. I've tried replacing the object instance with the name of the class ""annotators.ScispacyAnnotator.candidate_generation.CandidateGenerator"". I've tried the following which also didn't work:. ```. config = {""resolve_abbreviations"": True,. ""misc""={""misc"": {""@architectures"": ""spacy.CandidateGenerator.v1""}}. ""linker_name"": ""umls""}. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:824,deployability,instal,install,824,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so. ```. In [5]: nlp.get_pipe('scispacy_linker'). Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[8]: <function __main__.<lambda>(x)>. ```. so you would do. ```. nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>. nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb. ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:121,security,modif,modify,121,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so. ```. In [5]: nlp.get_pipe('scispacy_linker'). Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[8]: <function __main__.<lambda>(x)>. ```. so you would do. ```. nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>. nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb. ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:41,testability,simpl,simplest,41,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so. ```. In [5]: nlp.get_pipe('scispacy_linker'). Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[8]: <function __main__.<lambda>(x)>. ```. so you would do. ```. nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>. nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb. ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:41,usability,simpl,simplest,41,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so. ```. In [5]: nlp.get_pipe('scispacy_linker'). Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[8]: <function __main__.<lambda>(x)>. ```. so you would do. ```. nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>. nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb. ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:60,usability,minim,minimal,60,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so. ```. In [5]: nlp.get_pipe('scispacy_linker'). Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator. Out[8]: <function __main__.<lambda>(x)>. ```. so you would do. ```. nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>. nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb. ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:628,availability,error,error,628,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:85,energy efficiency,load,load,85,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:499,integrability,Compon,Component,499,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:499,interoperability,Compon,Component,499,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:499,modifiability,Compon,Component,499,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:85,performance,load,load,85,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:628,performance,error,error,628,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:478,safety,except,exception,478,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:628,safety,error,error,628,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:399,usability,document,document,399,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:628,usability,error,error,628,"That's certainly straightforward (thanks), so this is what I did:. ```. _nlp = spacy.load(_data_model). _config = {""resolve_abbreviations"": True,. ""linker_name"": ""umls""}. _nlp.add_pipe(""scispacy_linker"", config=_config). _candidate_generator = CandidateGenerator(). _entity_linker = _nlp.get_pipe('scispacy_linker'). _entity_linker.candidate_generator = _candidate_generator. . response = self._nlp(document.normalized_text). ```. and when that last line is executed I get this exception:. > [E109] Component scispacy_linker could not be run. Did you forget to call initialize()? . If I omit the cg assignment, I don't get that error (nor do I accomplish what I set out to do). I don't see an initialize method to call.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:21,availability,error,error,21,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:61,availability,error,error,61,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:21,performance,error,error,21,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:61,performance,error,error,61,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:21,safety,error,error,21,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:61,safety,error,error,61,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:21,usability,error,error,21,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:61,usability,error,error,61,I suspect that spacy error is swallowing whatever the actual error is. Can you try adding the second line from my example too? `_entity_linker.kb = _candidate_generator.kb`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:5,availability,error,error,5,"Same error, even with that line included.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:5,performance,error,error,5,"Same error, even with that line included.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:5,safety,error,error,5,"Same error, even with that line included.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:5,usability,error,error,5,"Same error, even with that line included.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:222,availability,error,error,222,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:245,availability,error,error,245,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:283,availability,error,error,283,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2066,availability,Error,Errors,2066,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1663,deployability,modul,module,1663,"nker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:119,energy efficiency,load,loading,119,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:429,energy efficiency,load,load,429,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2300,energy efficiency,load,load,2300,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:164,integrability,sub,substituting,164,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2011,integrability,compon,component,2011,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2193,integrability,Compon,Component,2193,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:306,interoperability,mismatch,mismatched,306,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2011,interoperability,compon,component,2011,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2078,interoperability,format,format,2078,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2193,interoperability,Compon,Component,2193,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1031,modifiability,pac,packages,1031,"and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initial",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1284,modifiability,pac,packages,1284," resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1663,modifiability,modul,module,1663,"nker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1874,modifiability,pac,packages,1874,"-----------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2011,modifiability,compon,component,2011,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2193,modifiability,Compon,Component,2193,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:119,performance,load,loading,119,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:222,performance,error,error,222,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:245,performance,error,error,245,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:283,performance,error,error,283,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:429,performance,load,load,429,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2066,performance,Error,Errors,2066,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2300,performance,load,load,2300,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:222,safety,error,error,222,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:245,safety,error,error,245,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:283,safety,error,error,283,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1177,safety,except,except,1177," mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1521,safety,except,exception,1521," ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _e",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1569,safety,except,exception,1569,"nfig=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker').",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1636,safety,input,input-,1636,"me=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1663,safety,modul,module,1663,"nker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1954,safety,except,except,1954,"corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.78859841823",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2066,safety,Error,Errors,2066,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2109,safety,except,except,2109,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2116,safety,Except,Exception,2116,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:95,testability,simul,simulate,95,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:912,testability,Trace,Traceback,912,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1592,testability,Trace,Traceback,1592,"andidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.ca",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:222,usability,error,error,222,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:245,usability,error,error,245,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:283,usability,error,error,283,"Any chance its because `self._nlp` and `_nlp` are different in the code you pasted? I tried to simulate your setup, by loading the linker with umls first, and then substituting the mesh candidate generator. I get the same error as you (the full error actually shows that it is a key error resulting from a mismatched candidate generator and kb) without the kb line, but adding the kb line resolves it. ```. In [26]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True, . ...: ""linker_name"": ""umls""} . ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config) . ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically ha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:1636,usability,input,input-,1636,"me=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: ...: response = _nlp(""This text has a disease.""). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:2066,usability,Error,Errors,2066,"cispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 988 try:. --> 989 doc = proc(doc, **component_cfg.get(name, {})). 990 except KeyError as e:. /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/scispacy/linking.py in __call__(self, doc). 120 self.filter_for_definitions. --> 121 and self.kb.cui_to_entity[cand.concept_id].definition is None. 122 and score < self.no_definition_threshold. KeyError: 'D000078623'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last). <ipython-input-26-808026450e1f> in <module>. 8 _entity_linker.candidate_generator = _candidate_generator. 9 . ---> 10 response = _nlp(""This text has a disease.""). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_new/lib/python3.8/site-packages/spacy/language.py in __call__(self, text, disable, component_cfg). 990 except KeyError as e:. 991 # This typically happens if a component is not initialized. --> 992 raise ValueError(Errors.E109.format(name=name)) from e. 993 except Exception as e:. 994 error_handler(name, proc, [doc], e). ValueError: [E109] Component 'scispacy_linker' could not be run. Did you forget to call `initialize()`? In [27]: _nlp = spacy.load(_data_model). ...: _config = {""resolve_abbreviations"": True,. ...: ""linker_name"": ""umls""}. ...: . ...: _nlp.add_pipe(""scispacy_linker"", config=_config). ...: _candidate_generator = CandidateGenerator(name=""mesh""). ...: _entity_linker = _nlp.get_pipe('scispacy_linker'). ...: _entity_linker.candidate_generator = _candidate_generator. ...: _entity_linker.kb = _candidate_generator.kb. ...: . ...: response = _nlp(""This text has a disease.""). In [28]: response.ents. Out[28]: (text, disease). In [29]: response.ents[1]._.kb_ents. Out[29]: . [('D004194', 1.0),. ('D008575', 0.9078139662742615),. ('D057973', 0.8003128170967102),. ('D016538', 0.7885984182357788),. ('D007644', 0.7878221273422241)]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:161,performance,time,time,161,"The line `response = self._nlp(document.normalized_text) ` is executed in a method that gets called by user action. The other assignments are done at class init time. Let me try your example, although the code defaults to umls if you omit the name. Umls is what I want. Thanks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:31,usability,document,document,31,"The line `response = self._nlp(document.normalized_text) ` is executed in a method that gets called by user action. The other assignments are done at class init time. Let me try your example, although the code defaults to umls if you omit the name. Umls is what I want. Thanks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:103,usability,user,user,103,"The line `response = self._nlp(document.normalized_text) ` is executed in a method that gets called by user action. The other assignments are done at class init time. Let me try your example, although the code defaults to umls if you omit the name. Umls is what I want. Thanks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:93,availability,down,download,93,"Daniel, Thank you very much. I finally got it working and determined that I had neglected to download a recent copy of concept_aliases.json and was using an older copy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/338:488,deployability,api,api,488,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:488,integrability,api,api,488,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:488,interoperability,api,api,488,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:626,performance,parallel,parallelization,626,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:707,testability,simpl,simpler,707,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:707,usability,simpl,simpler,707,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:30,reliability,doe,does,30,"Removing n_process definitely does work. I just popped off the argument and everything just works. I only need text itself so I think I can remove the spans. I am not sure what's the way to do it because it seems like that would be deeply embedded into the implementation of the class. I am guessing I would have to fork the project and dig at the implementation of abbreviation detector to remove the span stuff. It seems like converting things to json on user land wouldn't do anything since it's an the implementation of processes that's the problem. But like you said I could just do things myself and that solves the problem. With my project, it does look like I have to do that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:651,reliability,doe,does,651,"Removing n_process definitely does work. I just popped off the argument and everything just works. I only need text itself so I think I can remove the spans. I am not sure what's the way to do it because it seems like that would be deeply embedded into the implementation of the class. I am guessing I would have to fork the project and dig at the implementation of abbreviation detector to remove the span stuff. It seems like converting things to json on user land wouldn't do anything since it's an the implementation of processes that's the problem. But like you said I could just do things myself and that solves the problem. With my project, it does look like I have to do that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:379,safety,detect,detector,379,"Removing n_process definitely does work. I just popped off the argument and everything just works. I only need text itself so I think I can remove the spans. I am not sure what's the way to do it because it seems like that would be deeply embedded into the implementation of the class. I am guessing I would have to fork the project and dig at the implementation of abbreviation detector to remove the span stuff. It seems like converting things to json on user land wouldn't do anything since it's an the implementation of processes that's the problem. But like you said I could just do things myself and that solves the problem. With my project, it does look like I have to do that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:379,security,detect,detector,379,"Removing n_process definitely does work. I just popped off the argument and everything just works. I only need text itself so I think I can remove the spans. I am not sure what's the way to do it because it seems like that would be deeply embedded into the implementation of the class. I am guessing I would have to fork the project and dig at the implementation of abbreviation detector to remove the span stuff. It seems like converting things to json on user land wouldn't do anything since it's an the implementation of processes that's the problem. But like you said I could just do things myself and that solves the problem. With my project, it does look like I have to do that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:457,usability,user,user,457,"Removing n_process definitely does work. I just popped off the argument and everything just works. I only need text itself so I think I can remove the spans. I am not sure what's the way to do it because it seems like that would be deeply embedded into the implementation of the class. I am guessing I would have to fork the project and dig at the implementation of abbreviation detector to remove the span stuff. It seems like converting things to json on user land wouldn't do anything since it's an the implementation of processes that's the problem. But like you said I could just do things myself and that solves the problem. With my project, it does look like I have to do that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:242,deployability,pipelin,pipeline,242,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:278,deployability,pipelin,pipeline,278,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:242,integrability,pipelin,pipeline,242,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:278,integrability,pipelin,pipeline,278,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:184,testability,simpl,simply,184,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:184,usability,simpl,simply,184,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:309,usability,document,documents,309,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2,deployability,manag,managed,2,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:128,deployability,pipelin,pipeline,128,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2,energy efficiency,manag,managed,2,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1081,energy efficiency,load,load,1081,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:71,integrability,transform,transforming,71,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:94,integrability,compon,component,94,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:128,integrability,pipelin,pipeline,128,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:300,integrability,compon,component,300,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:484,integrability,compon,component,484,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1038,integrability,compon,component,1038,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:71,interoperability,transform,transforming,71,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:94,interoperability,compon,component,94,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:300,interoperability,compon,component,300,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:484,interoperability,compon,component,484,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1038,interoperability,compon,component,1038,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:94,modifiability,compon,component,94,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:300,modifiability,compon,component,300,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:484,modifiability,compon,component,484,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1038,modifiability,compon,component,1038,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1546,modifiability,exten,extend,1546,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1081,performance,load,load,1081,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2,safety,manag,managed,2,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1325,safety,test,test,1325,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:232,security,modif,modified,232,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:462,security,modif,modified,462,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1325,testability,test,test,1325,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1553,usability,behavi,behavior,1553,"I managed to get use the function that @danielkingai2 wrote in #205 by transforming it into a component and attaching it to the pipeline. Now multiprocessing works. Note that this introduces breaking changes to code and needs to be modified. ```python. from spacy.language import Language. @Language.component(""serialize_abbreviation""). def replace_abbrev_with_json(spacy_doc):. # https://github.com/allenai/scispacy/issues/205#issuecomment-597273144. # Code is modified to work as a component. new_abbrevs = []. for short in spacy_doc._.abbreviations:. short_text = short.text. short_start = short.start. short_end = short.end. long = short._.long_form. long_text = long.text. long_start = long.start. long_end = long.end. serializable_abbr = {""short_text"": short_text, ""short_start"": short_start, ""short_end"": short_end,. ""long_text"": long_text, ""long_start"": long_start, ""long_end"": long_end}. short._.long_form = None. new_abbrevs.append(serializable_abbr). spacy_doc._.abbreviations = new_abbrevs. return spacy_doc. ```. To call the component do this:. ```python. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""serialize_abbreviation"", after=""abbreviation_detector""). ```. Changes need to be made;. You need to change `abbrev._.long_text` into `abbrev[""long_text""]` and `abbrev.test` into `abbrev[""short_text""]`. This is because the serialization works by making everything into a dict so you need to refer to that instead. Now multiprocessing works perfectly. It's nice that Spacy makes it easy to extend behavior.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:152,deployability,pipelin,pipeline,152,"How far is this from being solved, and is there something I might do to help get this working? Would really like to use the abbreviation detector in my pipeline, but not being able to use it with multiprocessing is a deal breaker. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:152,integrability,pipelin,pipeline,152,"How far is this from being solved, and is there something I might do to help get this working? Would really like to use the abbreviation detector in my pipeline, but not being able to use it with multiprocessing is a deal breaker. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:137,safety,detect,detector,137,"How far is this from being solved, and is there something I might do to help get this working? Would really like to use the abbreviation detector in my pipeline, but not being able to use it with multiprocessing is a deal breaker. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:137,security,detect,detector,137,"How far is this from being solved, and is there something I might do to help get this working? Would really like to use the abbreviation detector in my pipeline, but not being able to use it with multiprocessing is a deal breaker. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:72,usability,help,help,72,"How far is this from being solved, and is there something I might do to help get this working? Would really like to use the abbreviation detector in my pipeline, but not being able to use it with multiprocessing is a deal breaker. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:228,deployability,automat,automatically,228,"@ldorigo . I solved it already using the code I written above. It's basically just adding another component to serialize the output of abbr. Maybe the code could be upstreamed into the library and then you add a flag to do this automatically. So far, if you want to use multiprocessing to have to add that piece of code to your code and add it manually.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:98,integrability,compon,component,98,"@ldorigo . I solved it already using the code I written above. It's basically just adding another component to serialize the output of abbr. Maybe the code could be upstreamed into the library and then you add a flag to do this automatically. So far, if you want to use multiprocessing to have to add that piece of code to your code and add it manually.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:98,interoperability,compon,component,98,"@ldorigo . I solved it already using the code I written above. It's basically just adding another component to serialize the output of abbr. Maybe the code could be upstreamed into the library and then you add a flag to do this automatically. So far, if you want to use multiprocessing to have to add that piece of code to your code and add it manually.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:98,modifiability,compon,component,98,"@ldorigo . I solved it already using the code I written above. It's basically just adding another component to serialize the output of abbr. Maybe the code could be upstreamed into the library and then you add a flag to do this automatically. So far, if you want to use multiprocessing to have to add that piece of code to your code and add it manually.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:228,testability,automat,automatically,228,"@ldorigo . I solved it already using the code I written above. It's basically just adding another component to serialize the output of abbr. Maybe the code could be upstreamed into the library and then you add a flag to do this automatically. So far, if you want to use multiprocessing to have to add that piece of code to your code and add it manually.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:102,integrability,sub,submit,102,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:157,integrability,compon,component,157,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:157,interoperability,compon,component,157,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:157,modifiability,compon,component,157,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:148,safety,detect,detector,148,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:148,security,detect,detector,148,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:170,security,control,control,170,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:170,testability,control,control,170,"As @f0lie said, you should be able to work around it in the manner described. If anyone would like to submit a PR adding a flag to the abbreviation detector component to control this, I would accept that PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/339:32,deployability,fail,fails,32,here is an example case when it fails . ![image](https://user-images.githubusercontent.com/40484210/111787318-db131200-88cf-11eb-99b4-12bc9ffa2fc3.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:32,reliability,fail,fails,32,here is an example case when it fails . ![image](https://user-images.githubusercontent.com/40484210/111787318-db131200-88cf-11eb-99b4-12bc9ffa2fc3.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:57,usability,user,user-images,57,here is an example case when it fails . ![image](https://user-images.githubusercontent.com/40484210/111787318-db131200-88cf-11eb-99b4-12bc9ffa2fc3.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/340:23,deployability,instal,installing,23,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:97,deployability,instal,install,97,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:119,deployability,instal,install,119,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:178,deployability,releas,releases,178,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:38,energy efficiency,model,model,38,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:44,modifiability,pac,package,44,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:38,security,model,model,38,"@phil-oxenberg are you installing the model package in your Dockerfile? You need both:. ```. pip install scispacy. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. in there somewhere. If not, can you post the dockerfile?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:63,usability,stop,stopped,63,"I'll post it shortly, but it has been working all day until it stopped working.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1332,availability,Error,Errors,1332,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:28,deployability,contain,container,28,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:359,deployability,contain,container,359,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:750,deployability,fail,fails,750,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1061,deployability,modul,module,1061,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:66,energy efficiency,core,core-sci-lg,66,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:90,energy efficiency,core,core-sci-md,90,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:114,energy efficiency,core,core-sci-scibert,114,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:143,energy efficiency,core,core-sci-sm,143,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:167,energy efficiency,core,core-web-lg,167,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:191,energy efficiency,core,core-web-sm,191,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:215,energy efficiency,core,core-web-trf,215,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:282,energy efficiency,load,loading,282,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:299,energy efficiency,model,model,299,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:559,energy efficiency,load,load,559,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:667,energy efficiency,load,load,667,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:988,energy efficiency,load,load,988,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1147,energy efficiency,load,load,1147,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1391,energy efficiency,model,model,1391,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1532,energy efficiency,load,load,1532,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1344,interoperability,format,format,1344,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1061,modifiability,modul,module,1061,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1106,modifiability,pac,packages,1106,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1268,modifiability,pac,packages,1268,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1447,modifiability,pac,package,1447,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:282,performance,load,loading,282,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:559,performance,load,load,559,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:667,performance,load,load,667,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:988,performance,load,load,988,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1147,performance,load,load,1147,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1332,performance,Error,Errors,1332,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1532,performance,load,load,1532,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:750,reliability,fail,fails,750,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1419,reliability,doe,doesn,1419,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1061,safety,modul,module,1061,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1332,safety,Error,Errors,1332,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1460,safety,valid,valid,1460,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:299,security,model,model,299,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1391,security,model,model,1391,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:997,testability,Trace,Traceback,997,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:40,usability,command,command,40,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:330,usability,command,command,330,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:466,usability,help,help,466,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:847,usability,help,help,847,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:1332,usability,Error,Errors,1332,"Executing pip list from the container's command line shows:. > en-core-sci-lg 0.4.0. > en-core-sci-md 0.4.0. > en-core-sci-scibert 0.4.0. > en-core-sci-sm 0.4.0. > en-core-web-lg 3.0.0. > en-core-web-sm 3.0.0. > en-core-web-trf 3.0.0. > ... > scispacy 0.4.0. ... > spacy 3.0.5. and loading the data model succeeds from the python command line from inside the container's cli:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7fba820fd090>. >>> x='en_core_sci_sm'. >>> spacy.load(x). <spacy.lang.en.English object at 0x7fba820eed90>. >>>. ```. However, this fails:. ```. $ python. Python 3.7.9 (default, Feb 9 2021, 08:33:04). [GCC 8.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import os,spacy. >>> x=os.environ.get('SCISPACY_DATA_MODEL'). >>> spacy.load(x). Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.7/site-packages/spacy/__init__.py"", line 47, in load. return util.load_model(name, disable=disable, exclude=exclude, config=config). File ""/usr/local/lib/python3.7/site-packages/spacy/util.py"", line 329, in load_model. raise IOError(Errors.E050.format(name=name)). OSError: [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory. >>> print(x). en_core_sci_sm. >>> spacy.load('en_core_sci_sm'). <spacy.lang.en.English object at 0x7f96016cc390>. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:102,energy efficiency,model,model,102,"My fix, even though its should be unnecessary, was essentially to not use a variable to hold the data model name passed to spacy.load() Might this be a bug in spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:129,energy efficiency,load,load,129,"My fix, even though its should be unnecessary, was essentially to not use a variable to hold the data model name passed to spacy.load() Might this be a bug in spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:76,modifiability,variab,variable,76,"My fix, even though its should be unnecessary, was essentially to not use a variable to hold the data model name passed to spacy.load() Might this be a bug in spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:129,performance,load,load,129,"My fix, even though its should be unnecessary, was essentially to not use a variable to hold the data model name passed to spacy.load() Might this be a bug in spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:102,security,model,model,102,"My fix, even though its should be unnecessary, was essentially to not use a variable to hold the data model name passed to spacy.load() Might this be a bug in spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:32,usability,help,help,32,Ha! Good catch. Thanks for your help. Duh - it's there even in the title to my post. :-),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/342:90,availability,down,download,90,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:46,deployability,version,version,46,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:258,deployability,version,version,258,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:353,deployability,version,version,353,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:408,deployability,instal,install,408,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:445,deployability,version,versions,445,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:461,deployability,instal,install,461,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:488,deployability,automat,automatically,488,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:585,deployability,instal,install,585,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:607,deployability,version,version,607,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:80,energy efficiency,current,currently,80,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:364,energy efficiency,model,models,364,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:46,integrability,version,version,46,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:152,integrability,coupl,couple,152,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:258,integrability,version,version,258,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:353,integrability,version,version,353,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:445,integrability,version,versions,445,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:607,integrability,version,version,607,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:186,interoperability,incompatib,incompatibilities,186,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:344,interoperability,specif,specific,344,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:437,interoperability,specif,specify,437,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:598,interoperability,specif,specific,598,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:46,modifiability,version,version,46,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:152,modifiability,coupl,couple,152,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:258,modifiability,version,version,258,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:353,modifiability,version,version,353,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:445,modifiability,version,versions,445,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:607,modifiability,version,version,607,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:162,performance,time,times,162,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:364,security,model,models,364,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:152,testability,coupl,couple,152,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:488,testability,automat,automatically,488,"The streamlit seems to be running scispacy_lg version 0.2.4 and the one you can currently download is 0.4.0, that might be the issue (I ran into this a couple of times when dealing with incompatibilities with spacy 2.X):. ""lang"":""en"". ""name"":""core_sci_lg"". ""version"":""0.2.4"". ""spacy_version"":"">=2.2.1"". EDIT: (The only way I know of to get the specific version of models is to wget from the old link and pip install from file, you can't specify versions in pip install from the links, it automatically gets overwritten with the most up-to-date one). Second EDIT: (I just tried the pip install with specific version link and it works)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:542,deployability,depend,depends,542,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:46,energy efficiency,model,model,46,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:149,energy efficiency,load,loads,149,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:165,energy efficiency,model,models,165,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:542,integrability,depend,depends,542,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:78,interoperability,specif,specifically,78,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:542,modifiability,depend,depends,542,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:149,performance,load,loads,149,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:542,safety,depend,depends,542,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:46,security,model,model,46,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:165,security,model,models,165,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:542,testability,depend,depends,542,"You are not referring to the ""specialized NER model"" here though, right? (and specifically, `""en_ner_jnlpba_md""`). I can see that the streamlit demo loads two spacy models, `spacy_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L47-L50)) and `ner_model` ([here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L59-L63)). My question is about `ner_model`, which should be unaffected by `spacy_model`. . AFAICT, the results of the specialized NER in the streamlit demo depends only on `ner_model` (see [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L69) and [here](https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42#file-app-py-L110-L127)).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:44,deployability,version,version,44,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:217,deployability,version,version,217,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:44,integrability,version,version,44,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:217,integrability,version,version,217,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:297,interoperability,cot,cotransporter,297,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:44,modifiability,version,version,44,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:217,modifiability,version,version,217,"You're totally right. I couldn't find which version of en_ner_jnlpba_md they are using on streamlit demo, but given that en_core_sci_lg was older, it wouldn't surprise me if the en_ner_jnlpba_md was too. EDIT: . with version 0.3.0 of en_ner_jnlpba_md and spacy 2.3.2 I got:. secretory Na+-K+-2Cl- cotransporter PROTEIN. NKCC1 PROTEIN. homodimer PROTEIN. while with 0.4.0 (and spacy 3.0.5) I got:. NKCC1 DNA. homodimer PROTEIN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:67,deployability,version,version,67,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:241,deployability,version,version,241,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:50,energy efficiency,load,load,50,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:78,energy efficiency,model,model,78,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:249,energy efficiency,model,model,249,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:67,integrability,version,version,67,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:241,integrability,version,version,241,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:67,modifiability,version,version,67,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:241,modifiability,version,version,241,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:50,performance,load,load,50,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:78,security,model,model,78,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:249,security,model,model,249,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:113,usability,close,closely,113,"@MichalMalyska Yeah you are totally right. When I load the `0.3.0` version of model and repo, I get results that closely match the streamlit demo for both of my examples. Weird, because at least for these examples, the output of the `0.4.0` version model/code is worse (IMO).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:110,deployability,version,versions,110,@danielkingai2 I guess the bigger underlying problem is why are the 0.4.0 models so much worse than the older versions.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:74,energy efficiency,model,models,74,@danielkingai2 I guess the bigger underlying problem is why are the 0.4.0 models so much worse than the older versions.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:110,integrability,version,versions,110,@danielkingai2 I guess the bigger underlying problem is why are the 0.4.0 models so much worse than the older versions.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:110,modifiability,version,versions,110,@danielkingai2 I guess the bigger underlying problem is why are the 0.4.0 models so much worse than the older versions.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:74,security,model,models,74,@danielkingai2 I guess the bigger underlying problem is why are the 0.4.0 models so much worse than the older versions.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:45,deployability,version,versions,45,"I did my best to match everything to the old versions, and our reported accuracy didn't drop much I don't think, but there are a bunch of hyperparams that we haven't really done any search over, just tried to use whatever spacy is using. If you wanted to play around with retraining with different hyperparameters or something, all the training scripts should be clear from project.yml",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:45,integrability,version,versions,45,"I did my best to match everything to the old versions, and our reported accuracy didn't drop much I don't think, but there are a bunch of hyperparams that we haven't really done any search over, just tried to use whatever spacy is using. If you wanted to play around with retraining with different hyperparameters or something, all the training scripts should be clear from project.yml",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:45,modifiability,version,versions,45,"I did my best to match everything to the old versions, and our reported accuracy didn't drop much I don't think, but there are a bunch of hyperparams that we haven't really done any search over, just tried to use whatever spacy is using. If you wanted to play around with retraining with different hyperparameters or something, all the training scripts should be clear from project.yml",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:363,usability,clear,clear,363,"I did my best to match everything to the old versions, and our reported accuracy didn't drop much I don't think, but there are a bunch of hyperparams that we haven't really done any search over, just tried to use whatever spacy is using. If you wanted to play around with retraining with different hyperparameters or something, all the training scripts should be clear from project.yml",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:169,availability,down,downstream,169,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:300,availability,consist,consistent,300,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:450,energy efficiency,predict,predicts,450,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:502,energy efficiency,predict,predicts,502,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:653,energy efficiency,predict,predicted,653,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:241,safety,valid,validation,241,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:450,safety,predict,predicts,450,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:502,safety,predict,predicts,502,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:557,safety,test,test,557,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:653,safety,predict,predicted,653,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:15,security,auth,author,15,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:241,security,validat,validation,241,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:388,security,trust,trustees,388,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:421,security,TRUST,TRUST,421,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:557,testability,test,test,557,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:62,usability,close,closed,62,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:293,usability,clear,clear,293,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:300,usability,consist,consistent,300,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:562,usability,document,documents,562,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:766,usability,behavi,behavior,766,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed. I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:. JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST. the Spacy-2 correctly predicts all 3 entities above. whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents. Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/343:113,deployability,patch,patch,113,"running into the same issue. it would be great to have those paths cacheable in `~/.scispacy` w/o need to monkey patch. my initial guess was also that `file_cache.py` would check the `~/.scispacy` cache, but it is doing something different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:67,performance,cach,cacheable,67,"running into the same issue. it would be great to have those paths cacheable in `~/.scispacy` w/o need to monkey patch. my initial guess was also that `file_cache.py` would check the `~/.scispacy` cache, but it is doing something different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:197,performance,cach,cache,197,"running into the same issue. it would be great to have those paths cacheable in `~/.scispacy` w/o need to monkey patch. my initial guess was also that `file_cache.py` would check the `~/.scispacy` cache, but it is doing something different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:113,safety,patch,patch,113,"running into the same issue. it would be great to have those paths cacheable in `~/.scispacy` w/o need to monkey patch. my initial guess was also that `file_cache.py` would check the `~/.scispacy` cache, but it is doing something different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:113,security,patch,patch,113,"running into the same issue. it would be great to have those paths cacheable in `~/.scispacy` w/o need to monkey patch. my initial guess was also that `file_cache.py` would check the `~/.scispacy` cache, but it is doing something different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:25,security,hash,hashed,25,is there a reason to use hashed filenames? would you be open to a PR that rids of additional hash?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:93,security,hash,hash,93,is there a reason to use hashed filenames? would you be open to a PR that rids of additional hash?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:111,performance,cach,cache,111,"I think the file names do need to be hashed, because it is possible for the files to change, in which case the cache needs to be invalidated. I think the way to do this is for you to define your own `LocalLinkerPaths` and `LocalKnowledgeBase`, and then add those to `DEFAULT_PATHS` and `DEFAULT_KNOWLEDGE_BASES` respectively. Untested, but that is the cleanest way in my mind.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:37,security,hash,hashed,37,"I think the file names do need to be hashed, because it is possible for the files to change, in which case the cache needs to be invalidated. I think the way to do this is for you to define your own `LocalLinkerPaths` and `LocalKnowledgeBase`, and then add those to `DEFAULT_PATHS` and `DEFAULT_KNOWLEDGE_BASES` respectively. Untested, but that is the cleanest way in my mind.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/344:99,deployability,version,version,99,I think the reason for this difference might be the same as in #342 - they are running a different version in the demo and if you use an older one you might get a different ordering / result,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/344
https://github.com/allenai/scispacy/issues/344:99,integrability,version,version,99,I think the reason for this difference might be the same as in #342 - they are running a different version in the demo and if you use an older one you might get a different ordering / result,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/344
https://github.com/allenai/scispacy/issues/344:99,modifiability,version,version,99,I think the reason for this difference might be the same as in #342 - they are running a different version in the demo and if you use an older one you might get a different ordering / result,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/344
https://github.com/allenai/scispacy/issues/345:68,availability,Error,Error,68,"Hey, seems like it works as expected (i.e. doesn't crash) on linux? Error above was from running on OSX 10.14.6. (FYI I suspect it might something to do with multiprocessing using spawn rather than fork by default on OSX as of py3.8 [[doc link]](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) but IDK)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:68,performance,Error,Error,68,"Hey, seems like it works as expected (i.e. doesn't crash) on linux? Error above was from running on OSX 10.14.6. (FYI I suspect it might something to do with multiprocessing using spawn rather than fork by default on OSX as of py3.8 [[doc link]](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) but IDK)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:43,reliability,doe,doesn,43,"Hey, seems like it works as expected (i.e. doesn't crash) on linux? Error above was from running on OSX 10.14.6. (FYI I suspect it might something to do with multiprocessing using spawn rather than fork by default on OSX as of py3.8 [[doc link]](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) but IDK)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:68,safety,Error,Error,68,"Hey, seems like it works as expected (i.e. doesn't crash) on linux? Error above was from running on OSX 10.14.6. (FYI I suspect it might something to do with multiprocessing using spawn rather than fork by default on OSX as of py3.8 [[doc link]](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) but IDK)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:301,testability,context,contexts-and-start-methods,301,"Hey, seems like it works as expected (i.e. doesn't crash) on linux? Error above was from running on OSX 10.14.6. (FYI I suspect it might something to do with multiprocessing using spawn rather than fork by default on OSX as of py3.8 [[doc link]](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) but IDK)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:68,usability,Error,Error,68,"Hey, seems like it works as expected (i.e. doesn't crash) on linux? Error above was from running on OSX 10.14.6. (FYI I suspect it might something to do with multiprocessing using spawn rather than fork by default on OSX as of py3.8 [[doc link]](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) but IDK)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:151,performance,parallel,parallelization,151,"Interesting, not sure off the top of my head. Leaving this open for now, let me know if you happen to resolve anything. At a minimum, you could do the parallelization yourself, but ideally it would work with spacy's parallelization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:216,performance,parallel,parallelization,216,"Interesting, not sure off the top of my head. Leaving this open for now, let me know if you happen to resolve anything. At a minimum, you could do the parallelization yourself, but ideally it would work with spacy's parallelization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:125,usability,minim,minimum,125,"Interesting, not sure off the top of my head. Leaving this open for now, let me know if you happen to resolve anything. At a minimum, you could do the parallelization yourself, but ideally it would work with spacy's parallelization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:143,availability,error,error,143,"I actually initially tried doing the parallelization myself with joblib, calling `nlp()` inside the parallelized code, and it gave me the same error as the spacy `nlp.pipe` snippet I posted. . Will let you know if I come across anything, but it seems to work fine on linux fwiw.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:37,performance,parallel,parallelization,37,"I actually initially tried doing the parallelization myself with joblib, calling `nlp()` inside the parallelized code, and it gave me the same error as the spacy `nlp.pipe` snippet I posted. . Will let you know if I come across anything, but it seems to work fine on linux fwiw.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:100,performance,parallel,parallelized,100,"I actually initially tried doing the parallelization myself with joblib, calling `nlp()` inside the parallelized code, and it gave me the same error as the spacy `nlp.pipe` snippet I posted. . Will let you know if I come across anything, but it seems to work fine on linux fwiw.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:143,performance,error,error,143,"I actually initially tried doing the parallelization myself with joblib, calling `nlp()` inside the parallelized code, and it gave me the same error as the spacy `nlp.pipe` snippet I posted. . Will let you know if I come across anything, but it seems to work fine on linux fwiw.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:143,safety,error,error,143,"I actually initially tried doing the parallelization myself with joblib, calling `nlp()` inside the parallelized code, and it gave me the same error as the spacy `nlp.pipe` snippet I posted. . Will let you know if I come across anything, but it seems to work fine on linux fwiw.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:143,usability,error,error,143,"I actually initially tried doing the parallelization myself with joblib, calling `nlp()` inside the parallelized code, and it gave me the same error as the spacy `nlp.pipe` snippet I posted. . Will let you know if I come across anything, but it seems to work fine on linux fwiw.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/346:4,deployability,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:4,integrability,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:4,interoperability,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:4,modifiability,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:4,reliability,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:4,security,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:4,testability,integr,integration,4,"Hi, integration with wikidata would definitely be nice. You should just need to do what @DeNeutoy says here (https://github.com/allenai/scispacy/issues/331#issuecomment-803291644)!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/347:0,deployability,Instal,Installing,0,Installing things in a new environment solved the issue. As was mentioned [here](https://github.com/allenai/scispacy/issues/318),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/348:64,energy efficiency,model,models,64,"This is how you would add the probabilities from the base spacy models. ```. import spacy. from spacy.lookups import load_lookups. nlp = spacy.load(""en_core_web_sm""). lookups = load_lookups(""en"", [""lexeme_prob""]). nlp.vocab.lookups.add_table(""lexeme_prob"", lookups.get_table(""lexeme_prob"")). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/348:143,energy efficiency,load,load,143,"This is how you would add the probabilities from the base spacy models. ```. import spacy. from spacy.lookups import load_lookups. nlp = spacy.load(""en_core_web_sm""). lookups = load_lookups(""en"", [""lexeme_prob""]). nlp.vocab.lookups.add_table(""lexeme_prob"", lookups.get_table(""lexeme_prob"")). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/348:143,performance,load,load,143,"This is how you would add the probabilities from the base spacy models. ```. import spacy. from spacy.lookups import load_lookups. nlp = spacy.load(""en_core_web_sm""). lookups = load_lookups(""en"", [""lexeme_prob""]). nlp.vocab.lookups.add_table(""lexeme_prob"", lookups.get_table(""lexeme_prob"")). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/348:64,security,model,models,64,"This is how you would add the probabilities from the base spacy models. ```. import spacy. from spacy.lookups import load_lookups. nlp = spacy.load(""en_core_web_sm""). lookups = load_lookups(""en"", [""lexeme_prob""]). nlp.vocab.lookups.add_table(""lexeme_prob"", lookups.get_table(""lexeme_prob"")). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/349:38,availability,down,download,38,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:85,deployability,instal,install,85,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:144,deployability,releas,releases,144,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:281,deployability,version,versions,281,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:281,integrability,version,versions,281,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:19,interoperability,specif,specify,19,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:281,modifiability,version,versions,281,"Yeah, you can just specify the old S3 download paths, they still work. . E.g. . `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz` will get you the scispacy science large vocab for scispacy 0.3.0 which works with spacy 2.X versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:80,availability,down,download,80,"Amazing! I think this model should work for my purposes, but is there a list of download paths somewhere in case I want to try other models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:22,energy efficiency,model,model,22,"Amazing! I think this model should work for my purposes, but is there a list of download paths somewhere in case I want to try other models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:133,energy efficiency,model,models,133,"Amazing! I think this model should work for my purposes, but is there a list of download paths somewhere in case I want to try other models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:22,security,model,model,22,"Amazing! I think this model should work for my purposes, but is there a list of download paths somewhere in case I want to try other models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:133,security,model,models,133,"Amazing! I think this model should work for my purposes, but is there a list of download paths somewhere in case I want to try other models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:39,deployability,releas,release,39,"The way I find them is to go to tagged release for 0.3.0 in this repo, and the old links are in the old readme. I am pretty sure they are identical as the new ones except you replace v0.4.0 with 0.3.0 everywhere.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:164,safety,except,except,164,"The way I find them is to go to tagged release for 0.3.0 in this repo, and the old links are in the old readme. I am pretty sure they are identical as the new ones except you replace v0.4.0 with 0.3.0 everywhere.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:138,security,ident,identical,138,"The way I find them is to go to tagged release for 0.3.0 in this repo, and the old links are in the old readme. I am pretty sure they are identical as the new ones except you replace v0.4.0 with 0.3.0 everywhere.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:61,deployability,releas,release,61,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:133,deployability,version,version,133,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:200,energy efficiency,model,models,200,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:133,integrability,version,version,133,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:211,interoperability,compatib,compatible,211,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:133,modifiability,version,version,133,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:200,security,model,models,200,"Yup! As @MichalMalyska says, you can either go to the tagged release you want and grab the link from the readme, or just replace the version in the url, they are the same other than that. The `0.3.0` models are compatible with spacy 2.3 and later. For 2.2.1 up to 2.3, you'll want `0.2.4`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/350:14,deployability,instal,installing,14,Could you try installing everything clean in a new environment?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:20,deployability,instal,install,20,"This was in a clean install, I just tried a different machine and I get the same problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:90,energy efficiency,model,model,90,"Yeah sure I can do that, but the deprecation warning is being triggered by the `scispacy` model. If I use a normal core spacy model like `en_core_web_sm` then I don't get a warning / exception.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:115,energy efficiency,core,core,115,"Yeah sure I can do that, but the deprecation warning is being triggered by the `scispacy` model. If I use a normal core spacy model like `en_core_web_sm` then I don't get a warning / exception.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:126,energy efficiency,model,model,126,"Yeah sure I can do that, but the deprecation warning is being triggered by the `scispacy` model. If I use a normal core spacy model like `en_core_web_sm` then I don't get a warning / exception.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:183,safety,except,exception,183,"Yeah sure I can do that, but the deprecation warning is being triggered by the `scispacy` model. If I use a normal core spacy model like `en_core_web_sm` then I don't get a warning / exception.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:90,security,model,model,90,"Yeah sure I can do that, but the deprecation warning is being triggered by the `scispacy` model. If I use a normal core spacy model like `en_core_web_sm` then I don't get a warning / exception.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:126,security,model,model,126,"Yeah sure I can do that, but the deprecation warning is being triggered by the `scispacy` model. If I use a normal core spacy model like `en_core_web_sm` then I don't get a warning / exception.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:103,performance,time,time,103,"Ahh, I see, thanks @adrianeboyd! @gautierdag I'm going to leave this open for now, hopefully I'll have time to fix it soon. Apologies if having to ignore those warnings is a hassle for you.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:40,usability,close,close,40,Ok great! Thank you @adrianeboyd - I'll close the issue on the spaCy side.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:45,energy efficiency,model,model,45,@adrianeboyd do you know if I can modify the model file easily without retraining? or is full retraining with the fixed config necessary?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:34,security,modif,modify,34,@adrianeboyd do you know if I can modify the model file easily without retraining? or is full retraining with the fixed config necessary?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:45,security,model,model,45,@adrianeboyd do you know if I can modify the model file easily without retraining? or is full retraining with the fixed config necessary?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:272,energy efficiency,model,model,272,"Hmm, this is one of those cases where I hesitate to describe a workaround, even though I think it's technically possible (you'd have to replace `MultiHashEmbed` and `StaticVectors` with custom code). But I really wouldn't recommend it, especially for a widely distributed model, so my official answer is: no, I would retrain the model with the corrected config.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:329,energy efficiency,model,model,329,"Hmm, this is one of those cases where I hesitate to describe a workaround, even though I think it's technically possible (you'd have to replace `MultiHashEmbed` and `StaticVectors` with custom code). But I really wouldn't recommend it, especially for a widely distributed model, so my official answer is: no, I would retrain the model with the corrected config.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:260,interoperability,distribut,distributed,260,"Hmm, this is one of those cases where I hesitate to describe a workaround, even though I think it's technically possible (you'd have to replace `MultiHashEmbed` and `StaticVectors` with custom code). But I really wouldn't recommend it, especially for a widely distributed model, so my official answer is: no, I would retrain the model with the corrected config.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:272,security,model,model,272,"Hmm, this is one of those cases where I hesitate to describe a workaround, even though I think it's technically possible (you'd have to replace `MultiHashEmbed` and `StaticVectors` with custom code). But I really wouldn't recommend it, especially for a widely distributed model, so my official answer is: no, I would retrain the model with the corrected config.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:329,security,model,model,329,"Hmm, this is one of those cases where I hesitate to describe a workaround, even though I think it's technically possible (you'd have to replace `MultiHashEmbed` and `StaticVectors` with custom code). But I really wouldn't recommend it, especially for a widely distributed model, so my official answer is: no, I would retrain the model with the corrected config.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:186,usability,custom,custom,186,"Hmm, this is one of those cases where I hesitate to describe a workaround, even though I think it's technically possible (you'd have to replace `MultiHashEmbed` and `StaticVectors` with custom code). But I really wouldn't recommend it, especially for a widely distributed model, so my official answer is: no, I would retrain the model with the corrected config.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/351:133,availability,down,download,133,"I think this is basically the same as #343. Does that work for you? Also, if you have access to the internet, but just don't want to download the large files, you can just put a command in your dockerfile that triggers the download, and then copy the folder they get written to (/home/.scispacy/datasets) into your docker image.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:223,availability,down,download,223,"I think this is basically the same as #343. Does that work for you? Also, if you have access to the internet, but just don't want to download the large files, you can just put a command in your dockerfile that triggers the download, and then copy the folder they get written to (/home/.scispacy/datasets) into your docker image.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:44,reliability,Doe,Does,44,"I think this is basically the same as #343. Does that work for you? Also, if you have access to the internet, but just don't want to download the large files, you can just put a command in your dockerfile that triggers the download, and then copy the folder they get written to (/home/.scispacy/datasets) into your docker image.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:86,security,access,access,86,"I think this is basically the same as #343. Does that work for you? Also, if you have access to the internet, but just don't want to download the large files, you can just put a command in your dockerfile that triggers the download, and then copy the folder they get written to (/home/.scispacy/datasets) into your docker image.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:178,usability,command,command,178,"I think this is basically the same as #343. Does that work for you? Also, if you have access to the internet, but just don't want to download the large files, you can just put a command in your dockerfile that triggers the download, and then copy the folder they get written to (/home/.scispacy/datasets) into your docker image.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:35,usability,feedback,feedback,35,"@danielkingai2 , thank you for the feedback. This advice works for me!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/352:89,availability,robust,robust,89,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:39,energy efficiency,current,current,39,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:257,energy efficiency,predict,predict,257,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:450,energy efficiency,current,current,450,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:89,reliability,robust,robust,89,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:115,reliability,doe,does,115,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:89,safety,robust,robust,89,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:257,safety,predict,predict,257,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:159,testability,context,context,159,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:305,testability,simpl,simple,305,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:305,usability,simpl,simple,305,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/353:10,deployability,depend,depend,10,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:24,energy efficiency,model,model,24,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:48,energy efficiency,model,models,48,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:64,energy efficiency,core,core-sci-lg,64,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:10,integrability,depend,depend,10,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:10,modifiability,depend,depend,10,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:10,safety,depend,depend,10,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:24,security,model,model,24,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:48,security,model,models,48,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:10,testability,depend,depend,10,"This will depend on the model you use - I think models like ""en-core-sci-lg"" will have been trained on biomedical texts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:167,energy efficiency,model,model,167,"Hi @MichalMalyska , . Thank you for responding. I'm sorry I wasn't able to find the corresponding section on your paper. Would you please let me know if the embedding model is described anywhere? Thanks again,. Yifu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:167,security,model,model,167,"Hi @MichalMalyska , . Thank you for responding. I'm sorry I wasn't able to find the corresponding section on your paper. Would you please let me know if the embedding model is described anywhere? Thanks again,. Yifu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:281,availability,avail,available,281,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:524,availability,robust,robust,524,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:592,availability,avail,available,592,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:812,availability,robust,robustness,812,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:201,deployability,Depend,Dependencies,201,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:233,deployability,Depend,Dependency,233,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:70,energy efficiency,model,models,70,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:649,energy efficiency,model,models,649,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:201,integrability,Depend,Dependencies,201,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:233,integrability,Depend,Dependency,233,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:362,integrability,Pub,Pubmed,362,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:389,integrability,Sub,Subset,389,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:581,integrability,pub,publically,581,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:201,modifiability,Depend,Dependencies,201,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:233,modifiability,Depend,Dependency,233,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:281,reliability,availab,available,281,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:524,reliability,robust,robust,524,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:592,reliability,availab,available,592,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:812,reliability,robust,robustness,812,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:201,safety,Depend,Dependencies,201,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:233,safety,Depend,Dependency,233,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:281,safety,avail,available,281,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:465,safety,detect,detector,465,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:524,safety,robust,robust,524,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:592,safety,avail,available,592,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:812,safety,robust,robustness,812,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:70,security,model,models,70,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:281,security,availab,available,281,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:382,security,Access,Access,382,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:465,security,detect,detector,465,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:592,security,availab,available,592,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:649,security,model,models,649,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:201,testability,Depend,Dependencies,201,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:233,testability,Depend,Dependency,233,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:940,usability,help,helpful,940,"Hey, . In the descriptions for all their vocabs they have:. ""scispaCy models are trained on data from a variety of sources. In particular, we use:. The GENIA 1.0 Treebank, converted to basic Universal Dependencies using the Stanford Dependency Converter. We have made this dataset available along with the original raw data. word2vec word vectors trained on the Pubmed Central Open Access Subset. The MedMentions Entity Linking dataset, used for training a mention detector. Ontonotes 5.0 to make the parser and tagger more robust to non-biomedical text. Unfortunately this is not publically available. "". To me, this means that all of the language models listed on https://allenai.github.io/scispacy/ should be trained on these datasets and the word embeddings should be tailored to biomedical texts (with some robustness to non-biomed text via Ontonotes). btw, I am not part of allenai - I just use the libraries and try to be moderately helpful in the issues on GH",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1019,availability,avail,available,1019,"verything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""ty",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1802,availability,avail,available,1802,"ing one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2751,deployability,depend,depends,2751,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:158,energy efficiency,load,load,158,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:205,energy efficiency,load,load,205,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:252,energy efficiency,load,load,252,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:299,energy efficiency,load,load,299,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:595,energy efficiency,model,model,595,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:634,energy efficiency,load,loaded,634,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:828,energy efficiency,model,models,828,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1001,energy efficiency,model,models,1001,"ks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1378,energy efficiency,model,model,1378,"7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1417,energy efficiency,load,loaded,1417,"iate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because th",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1611,energy efficiency,model,models,1611,"has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vect",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1784,energy efficiency,model,models,1784," happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I have",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2214,energy efficiency,model,model,2214,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2240,energy efficiency,model,model,2240,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2515,energy efficiency,model,models,2515,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2721,energy efficiency,model,models,2721,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2345,integrability,messag,message,2345,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2633,integrability,pub,pubmed,2633,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2751,integrability,depend,depends,2751,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2345,interoperability,messag,message,2345,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2751,modifiability,depend,depends,2751,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:158,performance,load,load,158,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:205,performance,load,load,205,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:252,performance,load,load,252,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:299,performance,load,load,299,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:634,performance,load,loaded,634,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1417,performance,load,loaded,1417,"iate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because th",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1019,reliability,availab,available,1019,"verything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""ty",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1802,reliability,availab,available,1802,"ing one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1019,safety,avail,available,1019,"verything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""ty",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1802,safety,avail,available,1802,"ing one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2751,safety,depend,depends,2751,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:595,security,model,model,595,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:828,security,model,models,828,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1001,security,model,models,1001,"ks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1019,security,availab,available,1019,"verything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""ty",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1378,security,model,model,1378,"7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1611,security,model,models,1611,"has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vect",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1784,security,model,models,1784," happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I have",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1802,security,availab,available,1802,"ing one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2214,security,model,model,2214,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2240,security,model,model,2240,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2515,security,model,models,2515,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2721,security,model,models,2721,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:907,testability,context,context-sensitive,907,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1690,testability,context,context-sensitive,1690,"on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:2751,testability,depend,depends,2751,"43. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a difference for these models. Which to use probably depends on your use case, and I haven't played with the similarity function much myself, but my example shows about how I would expect it to go. That is, ""tylenol"" and ""ibuprofen"" are more similar according to scispacy vectors than spacy vectors, but the full sentence is scored to be more similar by spacy than scispacy, maybe because the spacy vectors are better at similarity of ""normal"" text. This part is mostly I guess, but I hope that answers your question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:571,usability,User,UserWarning,571,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:907,usability,context-sensit,context-sensitive,907,"Thanks @MichalMalyska, everything you said is correct, and I'll add some more details. Here is an example:. ```. In [1]: import spacy. In [2]: sci_sm = spacy.load('en_core_sci_sm'). In [3]: sci_md = spacy.load('en_core_sci_md'). In [4]: web_sm = spacy.load('en_core_web_sm'). In [5]: web_md = spacy.load('en_core_web_md'). In [6]: text1 = ""Tylenol is used to treat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1354,usability,User,UserWarning,1354,"reat headaches"". In [7]: text2 = ""Ibuprofen is used to alleviate migraines"". In [8]: sci_sm(text1).similarity(sci_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. T",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:1690,usability,context-sensit,context-sensitive,1690,"on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[8]: 0.8506796430737505. In [9]: sci_md(text1).similarity(sci_md(text2)). Out[9]: 0.8859446651287843. In [10]: web_sm(text1).similarity(web_sm(text2)). /net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/ipython:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available. #!/net/nfs2.corp/s2-research/daniel/miniconda3/envs/scispacy_3_released/bin/python. Out[10]: 0.8650837072546965. In [11]: web_md(text1).similarity(web_md(text2)). Out[11]: 0.9257868773729903. In [12]: sci_md(""tylenol"").similarity(sci_md(""ibuprofen"")). Out[12]: 0.821704759629651. In [13]: web_md(""tylenol"").similarity(web_md(""ibuprofen"")). Out[13]: 0.6202061460191125. ```. Neither the scispacy small model nor the spacy small model have word vectors, so they compute similarity based on the tagger, parser, and NER, as the warning message says. This will be different between spacy and scispacy, because the parser, tagger, and NER are trained on biomedical data in scispacy. For the medium and large models, the spacy vectors are trained on Ontonotes (general domain web data), and the scispacy vectors are trained on pubmed papers (academic, biomedical data). So there will also be a",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:126,interoperability,Specif,Specifically,126,"Hi Daniel, . Thank you for posting the detailed explanation. Would you please let me know how the embeddings are constructed? Specifically, I wonder if the embeddings are generated from neural networks? Thanks so much again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:193,performance,network,networks,193,"Hi Daniel, . Thank you for posting the detailed explanation. Would you please let me know how the embeddings are constructed? Specifically, I wonder if the embeddings are generated from neural networks? Thanks so much again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:193,security,network,networks,193,"Hi Daniel, . Thank you for posting the detailed explanation. Would you please let me know how the embeddings are constructed? Specifically, I wonder if the embeddings are generated from neural networks? Thanks so much again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/354:84,energy efficiency,model,models,84,"Hey @dgg32, this is the same issue as #342 - the demo is using spacy 2.0 compatible models it seems, which for some reason perform better",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:73,interoperability,compatib,compatible,73,"Hey @dgg32, this is the same issue as #342 - the demo is using spacy 2.0 compatible models it seems, which for some reason perform better",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:123,performance,perform,perform,123,"Hey @dgg32, this is the same issue as #342 - the demo is using spacy 2.0 compatible models it seems, which for some reason perform better",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:84,security,model,models,84,"Hey @dgg32, this is the same issue as #342 - the demo is using spacy 2.0 compatible models it seems, which for some reason perform better",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:123,usability,perform,perform,123,"Hey @dgg32, this is the same issue as #342 - the demo is using spacy 2.0 compatible models it seems, which for some reason perform better",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:112,availability,degrad,degradation,112,"Thanks @MichalMalyska, that is correct. I unfortunately have not had time to look into the possible performance degradation. Will close this issue as duplicate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:69,performance,time,time,69,"Thanks @MichalMalyska, that is correct. I unfortunately have not had time to look into the possible performance degradation. Will close this issue as duplicate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:100,performance,perform,performance,100,"Thanks @MichalMalyska, that is correct. I unfortunately have not had time to look into the possible performance degradation. Will close this issue as duplicate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:112,reliability,degrad,degradation,112,"Thanks @MichalMalyska, that is correct. I unfortunately have not had time to look into the possible performance degradation. Will close this issue as duplicate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:100,usability,perform,performance,100,"Thanks @MichalMalyska, that is correct. I unfortunately have not had time to look into the possible performance degradation. Will close this issue as duplicate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:130,usability,close,close,130,"Thanks @MichalMalyska, that is correct. I unfortunately have not had time to look into the possible performance degradation. Will close this issue as duplicate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/355:16,modifiability,paramet,parameter,16,"ahh, the config parameter is called `linker_name`, not `name`. If you set `linker_name` instead, it should work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:22,availability,error,error,22,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:57,deployability,configurat,configurator,57,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:187,energy efficiency,load,load,187,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:607,energy efficiency,model,model,607,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:626,energy efficiency,load,load,626,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:57,integrability,configur,configurator,57,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:57,modifiability,configur,configurator,57,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:22,performance,error,error,22,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:187,performance,load,load,187,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:626,performance,load,load,626,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:22,safety,error,error,22,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:57,security,configur,configurator,57,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:607,security,model,model,607,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:22,usability,error,error,22,"I am getting the same error eve using linker_name in the configurator:. config = {. ""resolve_abbreviations"": True, . ""linker_name"": ""mesh"", . ""max_entities_per_mention"":5. }. nlp = spacy.load(""en_core_sci_md""). nlp.add_pipe(""scispacy_linker"", config=config). linker = nlp.get_pipe(""scispacy_linker""). doc = nlp(""Pre-diabetes Obesity Type-2 Diabetes Mellitus Obesity Overweight""). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). . and I get: . Pre-diabetes C0362046. Obesity C0028754. Diabetes Mellitus C0011849. Obesity C0028754. Overweight C0497406. I also used other Scispacy model: nlp = spacy.load(""en_ner_bionlp13cg_md"") in the same script, I don't know if it matters",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:215,deployability,releas,release,215,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:394,deployability,version,version,394,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:349,energy efficiency,current,current,349,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:94,integrability,sub,subset,94,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:238,integrability,sub,subset,238,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:394,integrability,version,version,394,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:394,modifiability,version,version,394,"Hi, it looks like the original mesh linker was created with a separate kb, rather than just a subset of UMLS. The process for creating the linker may have been lost. When I recreated the linkers for the latest UMLS release, I just used a subset of UMLS to produce the mesh linker. I'll have to look into this and decide whether to just stick to the current UMLS ids, or try to recreate the old version of the linker. Sorry about that. For now you will need to map between UMLS id and mesh id yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:52,deployability,version,version,52,"I see, maybe I will try using the previous scispacy version (0.5.1) that should work. Thank you very much for answering my question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:52,integrability,version,version,52,"I see, maybe I will try using the previous scispacy version (0.5.1) that should work. Thank you very much for answering my question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:52,modifiability,version,version,52,"I see, maybe I will try using the previous scispacy version (0.5.1) that should work. Thank you very much for answering my question!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:102,availability,down,download,102,"Also facing this problem, but I am able to map to MeSH from UMLS CUIs using the [MRCONSO.RRF](https://download.nlm.nih.gov/umls/kss/2023AA/umls-2023AA-mrconso.zip) file",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/356:125,availability,error,error,125,I will need some more information than that to help. How did you install scispacy? What code are you running that causes the error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:65,deployability,instal,install,65,I will need some more information than that to help. How did you install scispacy? What code are you running that causes the error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:125,performance,error,error,125,I will need some more information than that to help. How did you install scispacy? What code are you running that causes the error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:125,safety,error,error,125,I will need some more information than that to help. How did you install scispacy? What code are you running that causes the error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:47,usability,help,help,47,I will need some more information than that to help. How did you install scispacy? What code are you running that causes the error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:125,usability,error,error,125,I will need some more information than that to help. How did you install scispacy? What code are you running that causes the error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/357:205,deployability,stack,stackoverflow,205,Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? Maybe this answer works? https://stackoverflow.com/a/60352993/10459457,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:165,interoperability,prox,proxy,165,Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? Maybe this answer works? https://stackoverflow.com/a/60352993/10459457,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:213,deployability,stack,stackoverflow,213,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:404,deployability,modul,module,404,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:905,energy efficiency,load,load,905,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1012,energy efficiency,load,load,1012,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1164,energy efficiency,load,loads,1164,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:167,interoperability,prox,proxy,167,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:287,interoperability,prox,proxy,287,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:404,modifiability,modul,module,404,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:512,modifiability,pac,packages,512,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:686,modifiability,pac,packages,686,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:838,modifiability,pac,packages,838,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1195,modifiability,deco,decode,1195,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1250,modifiability,deco,decoder,1250,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1276,modifiability,deco,decode,1276,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1379,modifiability,deco,decoder,1379,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1457,modifiability,deco,decoder,1457,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:905,performance,load,load,905,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1012,performance,load,load,1012,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1164,performance,load,loads,1164,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:404,safety,modul,module,404,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:337,testability,Trace,Traceback,337,"> Are you able to run this line `https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/file_cache.py#L112` on that file with your proxy? > . > Maybe this answer works? https://stackoverflow.com/a/60352993/10459457. After using fresh environment, the proxy issue gone. However, now I go this issue:. `Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/home/xx/anaconda3/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/xx/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/xx/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648). `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:56,energy efficiency,load,load,56,Were you able to resolve this? Could you see if you can load the json file just in a ipython notebook or something?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:56,performance,load,load,56,Were you able to resolve this? Could you see if you can load the json file just in a ipython notebook or something?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:42,energy efficiency,load,load,42,Unfortunately still the same issue. I can load and work with json files. I want to share my environment: . Spacy 2.3.5. Scispacy 0.2.4. en_core_sci_sm 0.2.0 also 0.3.0,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:83,interoperability,share,share,83,Unfortunately still the same issue. I can load and work with json files. I want to share my environment: . Spacy 2.3.5. Scispacy 0.2.4. en_core_sci_sm 0.2.0 also 0.3.0,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:42,performance,load,load,42,Unfortunately still the same issue. I can load and work with json files. I want to share my environment: . Spacy 2.3.5. Scispacy 0.2.4. en_core_sci_sm 0.2.0 also 0.3.0,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:246,availability,avail,available-models,246,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:133,deployability,releas,release,133,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:231,deployability,releas,release-,231,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:50,energy efficiency,model,models,50,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:256,energy efficiency,model,models,256,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:246,reliability,availab,available-models,246,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:246,safety,avail,available-models,246,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:50,security,model,models,50,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:246,security,availab,available-models,246,with scispacy 0.2.4 you should be using the 0.2.3 models. Could you try that? The way to figure this out is to go to the tag for the release you are using and check the links in the readme (https://github.com/allenai/scispacy/tree/release-v0.2.4#available-models),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:224,availability,error,error,224,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:179,deployability,version,version,179,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:212,deployability,stack,stack,212,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:256,deployability,version,versions,256,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:265,deployability,instal,installed,265,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:432,deployability,version,version,432,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:458,deployability,version,version,458,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:716,deployability,version,version,716,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:742,deployability,version,version,742,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:914,deployability,modul,module,914,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2146,deployability,instal,installed,2146," CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When repla",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2174,deployability,Version,Version,2174," ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2942,deployability,instal,installed,2942,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2956,deployability,version,version,2956,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3292,deployability,releas,release,3292,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:400,energy efficiency,estimat,estimator,400,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:685,energy efficiency,estimat,estimator,685,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1481,energy efficiency,load,load,1481,"t lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1589,energy efficiency,load,load,1589,"r/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1742,energy efficiency,load,loads,1742,"sion 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2380,energy efficiency,core,core-sci-sm,2380,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3269,energy efficiency,current,current,3269,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:179,integrability,version,version,179,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:256,integrability,version,versions,256,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:432,integrability,version,version,432,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:458,integrability,version,version,458,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:716,integrability,version,version,716,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:742,integrability,version,version,742,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2174,integrability,Version,Version,2174," ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2956,integrability,version,version,2956,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3219,interoperability,specif,specify,3219,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:170,modifiability,pac,packages,170,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:179,modifiability,version,version,179,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:247,modifiability,pac,packages,247,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:256,modifiability,version,versions,256,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:338,modifiability,pac,packages,338,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:432,modifiability,version,version,432,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:458,modifiability,version,version,458,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:623,modifiability,pac,packages,623,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:716,modifiability,version,version,716,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:742,modifiability,version,version,742,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:914,modifiability,modul,module,914,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1044,modifiability,pac,packages,1044,"ed your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSO",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1240,modifiability,pac,packages,1240,"of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. boto",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1414,modifiability,pac,packages,1414,"nsformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1773,modifiability,deco,decode,1773," breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spac",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1829,modifiability,deco,decoder,1829,"UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1855,modifiability,deco,decode,1855,"most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extens",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1959,modifiability,deco,decoder,1959,"iations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2037,modifiability,deco,decoder,2037,"te-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2156,modifiability,pac,package,2156,"eGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2166,modifiability,Pac,Package,2166,"(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2174,modifiability,Version,Version,2174," ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2852,modifiability,extens,extensions,2852,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2956,modifiability,version,version,2956,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:224,performance,error,error,224,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1481,performance,load,load,1481,"t lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1589,performance,load,load,1589,"r/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1742,performance,load,loads,1742,"sion 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:224,safety,error,error,224,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:543,safety,risk,risk,543,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:827,safety,risk,risk,827,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:914,safety,modul,module,914,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3135,safety,safe,safe,3135,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3257,safety,review,review,3257,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:543,security,risk,risk,543,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:827,security,risk,risk,827,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2274,security,certif,certifi,2274,".py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current u",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2662,security,rsa,rsa,2662,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3108,security,modif,modify,3108,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:218,testability,trace,trace,218,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:847,testability,Trace,Traceback,847,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3257,testability,review,review,3257,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:224,usability,error,error,224,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:368,usability,User,UserWarning,368,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:549,usability,User,UserWarning,549,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:653,usability,User,UserWarning,653,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:833,usability,User,UserWarning,833,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(). File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__. self.umls = umls or UmlsKnowledgeBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2698,usability,learn,learn,2698,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3119,usability,behavi,behavior,3119,"geBase(). File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__. raw = json.load(open(cached_path(file_path))). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load. parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw). File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads. return _default_decoder.decode(s). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode. obj, end = self.raw_decode(s, idx=_w(s, 0).end()). File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode. obj, end = self.scan_once(s, idx). json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version. ------------------ ---------. awscli 1.20.5. blis 0.4.1. botocore 1.21.5. catalogue 1.0.0. certifi 2021.5.30. charset-normalizer 2.0.3. colorama 0.4.3. conllu 4.4. cymem 2.0.5. docutils 0.15.2. en-core-sci-sm 0.2.4. idna 3.2. importlib-metadata 4.6.1. jmespath 0.10.0. joblib 1.0.1. murmurhash 1.0.5. nmslib 2.1.1. numpy 1.21.1. pip 21.1.3. plac 0.9.6. preshed 3.0.5. psutil 5.8.0. pyasn1 0.4.8. pybind11 2.6.1. pysbd 0.3.4. python-dateutil 2.8.2. PyYAML 5.4.1. requests 2.26.0. rsa 4.7.2. s3transfer 0.5.0. scikit-learn 0.22.2. scipy 1.7.0. scispacy 0.2.4. setuptools 39.0.1. six 1.16.0. spacy 2.2.1. srsly 1.0.5. thinc 7.1.1. threadpoolctl 2.2.0. tqdm 4.61.2. typing-extensions 3.10.0.0. urllib3 1.26.6. wasabi 0.8.2. zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/358:207,reliability,doe,doesn,207,"As far as I understand the Entity Linking + Candidate Generation, the reason this happens is probably that:. 1. all of these entities have the same alias (hence the equal scores). 2. the candidate generator doesn't care about aliases at first, so the order returned will be based on the sparse 3-char TF-IDF cosine similarity. . So I think these are sorted by score and within score by the cosine sim using their candidate generation (or rather they come in that order and since the scores are the same they don't get moved around).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/358:12,testability,understand,understand,12,"As far as I understand the Entity Linking + Candidate Generation, the reason this happens is probably that:. 1. all of these entities have the same alias (hence the equal scores). 2. the candidate generator doesn't care about aliases at first, so the order returned will be based on the sparse 3-char TF-IDF cosine similarity. . So I think these are sorted by score and within score by the cosine sim using their candidate generation (or rather they come in that order and since the scores are the same they don't get moved around).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/358:584,usability,help,helped,584,"The canonical name and aliases should all be treated the same. The candidates are initially sorted by concept id (https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L354), and then by score (https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/linking.py#L127). Although when we evaluated this, the line sorting by concept id was not there, and they were just taken in whatever order the approximate nearest neighbors returned them in. It'd be pretty weird if sorting by concept id helped on medmentions somehow...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/358:85,deployability,releas,release,85,"It looks like that change was added ~4 months ago, so if this is ran under the 0.3.0 release this would make sense",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/359:49,energy efficiency,model,models,49,"Scibert is used for parsing and tagging in those models, while ner is the same as the other `en_core_sci_` models. Sorry, but I cannot provide a timeline for any future improvements. We welcome contributions, and the whole training procedure is documented in `https://github.com/allenai/scispacy/blob/main/project.yml`, if you would like to do it yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:107,energy efficiency,model,models,107,"Scibert is used for parsing and tagging in those models, while ner is the same as the other `en_core_sci_` models. Sorry, but I cannot provide a timeline for any future improvements. We welcome contributions, and the whole training procedure is documented in `https://github.com/allenai/scispacy/blob/main/project.yml`, if you would like to do it yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:145,performance,time,timeline,145,"Scibert is used for parsing and tagging in those models, while ner is the same as the other `en_core_sci_` models. Sorry, but I cannot provide a timeline for any future improvements. We welcome contributions, and the whole training procedure is documented in `https://github.com/allenai/scispacy/blob/main/project.yml`, if you would like to do it yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:49,security,model,models,49,"Scibert is used for parsing and tagging in those models, while ner is the same as the other `en_core_sci_` models. Sorry, but I cannot provide a timeline for any future improvements. We welcome contributions, and the whole training procedure is documented in `https://github.com/allenai/scispacy/blob/main/project.yml`, if you would like to do it yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:107,security,model,models,107,"Scibert is used for parsing and tagging in those models, while ner is the same as the other `en_core_sci_` models. Sorry, but I cannot provide a timeline for any future improvements. We welcome contributions, and the whole training procedure is documented in `https://github.com/allenai/scispacy/blob/main/project.yml`, if you would like to do it yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:245,usability,document,documented,245,"Scibert is used for parsing and tagging in those models, while ner is the same as the other `en_core_sci_` models. Sorry, but I cannot provide a timeline for any future improvements. We welcome contributions, and the whole training procedure is documented in `https://github.com/allenai/scispacy/blob/main/project.yml`, if you would like to do it yourself.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/360:18,energy efficiency,model,models,18,"The different NER models are listed here with their entity types (https://allenai.github.io/scispacy/). The base models do not categorize beyond ""ENTITY"". You'll have to pick which NER model has the entity types you are looking for.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:113,energy efficiency,model,models,113,"The different NER models are listed here with their entity types (https://allenai.github.io/scispacy/). The base models do not categorize beyond ""ENTITY"". You'll have to pick which NER model has the entity types you are looking for.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:185,energy efficiency,model,model,185,"The different NER models are listed here with their entity types (https://allenai.github.io/scispacy/). The base models do not categorize beyond ""ENTITY"". You'll have to pick which NER model has the entity types you are looking for.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:18,security,model,models,18,"The different NER models are listed here with their entity types (https://allenai.github.io/scispacy/). The base models do not categorize beyond ""ENTITY"". You'll have to pick which NER model has the entity types you are looking for.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:113,security,model,models,113,"The different NER models are listed here with their entity types (https://allenai.github.io/scispacy/). The base models do not categorize beyond ""ENTITY"". You'll have to pick which NER model has the entity types you are looking for.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:185,security,model,model,185,"The different NER models are listed here with their entity types (https://allenai.github.io/scispacy/). The base models do not categorize beyond ""ENTITY"". You'll have to pick which NER model has the entity types you are looking for.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/362:70,interoperability,specif,specific,70,"The problem is I'm getting CUI matches with vocabularies that are not specific to the scope of my document (i.e. I can expect the document to be of a specific UMLS vocabulary type). An underlying idea might be an enhancement, where vocabulary type can be an arg of list type with one or more sources, from those found here https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/362
https://github.com/allenai/scispacy/issues/362:150,interoperability,specif,specific,150,"The problem is I'm getting CUI matches with vocabularies that are not specific to the scope of my document (i.e. I can expect the document to be of a specific UMLS vocabulary type). An underlying idea might be an enhancement, where vocabulary type can be an arg of list type with one or more sources, from those found here https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/362
https://github.com/allenai/scispacy/issues/362:98,usability,document,document,98,"The problem is I'm getting CUI matches with vocabularies that are not specific to the scope of my document (i.e. I can expect the document to be of a specific UMLS vocabulary type). An underlying idea might be an enhancement, where vocabulary type can be an arg of list type with one or more sources, from those found here https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/362
https://github.com/allenai/scispacy/issues/362:130,usability,document,document,130,"The problem is I'm getting CUI matches with vocabularies that are not specific to the scope of my document (i.e. I can expect the document to be of a specific UMLS vocabulary type). An underlying idea might be an enhancement, where vocabulary type can be an arg of list type with one or more sources, from those found here https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/362
https://github.com/allenai/scispacy/issues/365:32,availability,error,error,32,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:103,availability,error,error,103,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:38,integrability,messag,message,38,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:109,integrability,messag,message,109,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:38,interoperability,messag,message,38,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:109,interoperability,messag,message,109,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:32,performance,error,error,32,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:103,performance,error,error,103,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:32,safety,error,error,32,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:103,safety,error,error,103,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:32,usability,error,error,32,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:103,usability,error,error,103,Pretty hard to debug without an error message. Can you run that code as a script and see if you get an error message?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:95,usability,user,user-images,95,"Thanks. Here, it is:. <img width=""537"" alt=""Screenshot 2021-06-15 at 12 47 00 PM"" src=""https://user-images.githubusercontent.com/40318353/122009589-db8c4d80-cdd7-11eb-960e-629b6d80409e.png"">.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:167,deployability,instal,install,167,"My guess is that something is not working on 20.0.4, but its a bit hard to know without digging further. The prime candidate is nmslib. Can you see if you are able to install and import nmslib separately from scispacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:28,deployability,instal,install,28,"Thanks again. I was able to install the nmslib library and was able to import the EntityLinker. All seems fine now. :). But, I get a warning when I add the spacy linker pipe to my trained NER model. ![image](https://user-images.githubusercontent.com/40318353/122203288-80cc2200-cebb-11eb-85a7-95041be1980d.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:192,energy efficiency,model,model,192,"Thanks again. I was able to install the nmslib library and was able to import the EntityLinker. All seems fine now. :). But, I get a warning when I add the spacy linker pipe to my trained NER model. ![image](https://user-images.githubusercontent.com/40318353/122203288-80cc2200-cebb-11eb-85a7-95041be1980d.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:192,security,model,model,192,"Thanks again. I was able to install the nmslib library and was able to import the EntityLinker. All seems fine now. :). But, I get a warning when I add the spacy linker pipe to my trained NER model. ![image](https://user-images.githubusercontent.com/40318353/122203288-80cc2200-cebb-11eb-85a7-95041be1980d.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:216,usability,user,user-images,216,"Thanks again. I was able to install the nmslib library and was able to import the EntityLinker. All seems fine now. :). But, I get a warning when I add the spacy linker pipe to my trained NER model. ![image](https://user-images.githubusercontent.com/40318353/122203288-80cc2200-cebb-11eb-85a7-95041be1980d.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/366:0,energy efficiency,Cool,Cool,0,"Cool, I would accept a PR renaming this param to something more meaningful and/or adding a comment explaining what it is, if you would like to submit one.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:143,integrability,sub,submit,143,"Cool, I would accept a PR renaming this param to something more meaningful and/or adding a comment explaining what it is, if you would like to submit one.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/pull/367:16,deployability,fail,fail,16,"I think it will fail on linting again, do you have a script to run linting or should I just manually do it ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/367:16,reliability,fail,fail,16,"I think it will fail on linting again, do you have a script to run linting or should I just manually do it ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/367:173,interoperability,format,formatted,173,"The CI workflow is here (https://github.com/allenai/scispacy/blob/main/.github/workflows/main.yml). We use `black` for autoformatting, but the line length of comments isn't formatted by `black`, so fixing that is manual.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/367:7,usability,workflow,workflow,7,"The CI workflow is here (https://github.com/allenai/scispacy/blob/main/.github/workflows/main.yml). We use `black` for autoformatting, but the line length of comments isn't formatted by `black`, so fixing that is manual.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/367:79,usability,workflow,workflows,79,"The CI workflow is here (https://github.com/allenai/scispacy/blob/main/.github/workflows/main.yml). We use `black` for autoformatting, but the line length of comments isn't formatted by `black`, so fixing that is manual.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/368:154,deployability,depend,dependency,154,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:154,integrability,depend,dependency,154,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:154,modifiability,depend,dependency,154,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:105,safety,test,test,105,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:154,safety,depend,dependency,154,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:105,testability,test,test,105,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:154,testability,depend,dependency,154,"@danielkingai2 I fixed a bunch of issues that somehow were not picked up in previous PRs:. 1. One of the test cases was using the old way of checking for dependency parsing. 2. Reformatted all with black with line-length = 88 (like in your CI checks). 3. Fixed "","" without following whitespace in a list to pass linting",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:155,reliability,doe,doesn,155,"The only change that is outstanding there is switch from. `assert doc.is_parsed`. to. `assert doc.has_annotation(""DEP"")`. in one of the tests, this way it doesn't produce a warning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:136,safety,test,tests,136,"The only change that is outstanding there is switch from. `assert doc.is_parsed`. to. `assert doc.has_annotation(""DEP"")`. in one of the tests, this way it doesn't produce a warning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:59,testability,assert,assert,59,"The only change that is outstanding there is switch from. `assert doc.is_parsed`. to. `assert doc.has_annotation(""DEP"")`. in one of the tests, this way it doesn't produce a warning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:87,testability,assert,assert,87,"The only change that is outstanding there is switch from. `assert doc.is_parsed`. to. `assert doc.has_annotation(""DEP"")`. in one of the tests, this way it doesn't produce a warning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:136,testability,test,tests,136,"The only change that is outstanding there is switch from. `assert doc.is_parsed`. to. `assert doc.has_annotation(""DEP"")`. in one of the tests, this way it doesn't produce a warning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:83,safety,detect,detector,83,While making sure that all works in the linking with the serializable abbreviation detector I noticed a TODO for adding short forms to entity linking so i just added it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:83,security,detect,detector,83,While making sure that all works in the linking with the serializable abbreviation detector I noticed a TODO for adding short forms to entity linking so i just added it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:180,availability,error,errors,180,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:224,deployability,releas,releases,224,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:438,deployability,depend,dependency,438,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:452,deployability,updat,updated,452,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:346,energy efficiency,current,current,346,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:438,integrability,depend,dependency,438,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:324,interoperability,incompatib,incompatible,324,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:438,modifiability,depend,dependency,438,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:180,performance,error,errors,180,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:59,safety,test,test,59,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:180,safety,error,errors,180,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:438,safety,depend,dependency,438,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:452,safety,updat,updated,452,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:452,security,updat,updated,452,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:59,testability,test,test,59,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:438,testability,depend,dependency,438,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:180,usability,error,errors,180,"@danielkingai2 I just did a ton of digging - your proposed test will not succeed (it will just hang forever) on spacy <3.1.0 since that's when they added a fix for multiprocessing errors:. https://github.com/explosion/spaCy/releases/tag/v3.1.0. - in particular this PR https://github.com/explosion/spaCy/pull/8004. Which is incompatible with the current requirements file for scispacy. . We can add it as a commented out option until the dependency is updated, and keep the xfail in the meantime? .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:90,deployability,fail,fail,90,"Weird, ok, thanks. Yeah, you can just leave it as xfail, note in a comment that it should fail, and check and make sure it actually fails for this PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:132,deployability,fail,fails,132,"Weird, ok, thanks. Yeah, you can just leave it as xfail, note in a comment that it should fail, and check and make sure it actually fails for this PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:90,reliability,fail,fail,90,"Weird, ok, thanks. Yeah, you can just leave it as xfail, note in a comment that it should fail, and check and make sure it actually fails for this PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:132,reliability,fail,fails,132,"Weird, ok, thanks. Yeah, you can just leave it as xfail, note in a comment that it should fail, and check and make sure it actually fails for this PR.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:39,deployability,upgrad,upgrading,39,Should I also add the test to add when upgrading to 3.1.0 ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:39,modifiability,upgrad,upgrading,39,Should I also add the test to add when upgrading to 3.1.0 ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:22,safety,test,test,22,Should I also add the test to add when upgrading to 3.1.0 ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:22,testability,test,test,22,Should I also add the test to add when upgrading to 3.1.0 ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:40,deployability,updat,updated,40,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:83,deployability,fail,fails,83,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:83,reliability,fail,fails,83,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:10,safety,test,test,10,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:40,safety,updat,updated,40,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:40,security,updat,updated,40,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:10,testability,test,test,10,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:177,usability,user,user-images,177,"The xfail test also hangs now (with the updated fixtures etc), but we know that it fails if I manually execute it inside of pdb. ![Screen Shot 2021-07-14 at 3 44 50 PM](https://user-images.githubusercontent.com/12971408/125683363-3cba0ba9-a152-47aa-9198-41a6dc876eb0.png)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:42,deployability,fail,fail,42,I figured out which of the internals will fail even without multiprocessing if the doc is not serializable and added that as a test instead.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:42,reliability,fail,fail,42,I figured out which of the internals will fail even without multiprocessing if the doc is not serializable and added that as a test instead.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:127,safety,test,test,127,I figured out which of the internals will fail even without multiprocessing if the doc is not serializable and added that as a test instead.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/368:127,testability,test,test,127,I figured out which of the internals will fail even without multiprocessing if the doc is not serializable and added that as a test instead.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/368
https://github.com/allenai/scispacy/pull/369:9,deployability,version,version,9,"My black version was different which was causing the reformatting changes, fixed now! Thank you.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/369
https://github.com/allenai/scispacy/pull/369:9,integrability,version,version,9,"My black version was different which was causing the reformatting changes, fixed now! Thank you.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/369
https://github.com/allenai/scispacy/pull/369:9,modifiability,version,version,9,"My black version was different which was causing the reformatting changes, fixed now! Thank you.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/369
https://github.com/allenai/scispacy/issues/370:9,testability,trace,traceback,9,"From the traceback, it looks like you are calling `extract_entity` from another loop. If that is the case, you really don't want to be recreating the `nlp` object inside of a loop. You should create it once outside of whatever your main loop is, and then pass it in as an argument to the `extract_entity` function.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/371:52,testability,plan,planning,52,@danielkingai2 Thank you for your reply . . Are you planning to add ICD10 with scispacy in future ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/371
https://github.com/allenai/scispacy/issues/371:7,energy efficiency,current,currently,7,"Sorry, currently have no plans to add ICD10, although if you do find a mapping between UMLS and ICD10, please let me know!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/371
https://github.com/allenai/scispacy/issues/371:25,testability,plan,plans,25,"Sorry, currently have no plans to add ICD10, although if you do find a mapping between UMLS and ICD10, please let me know!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/371
https://github.com/allenai/scispacy/issues/372:83,deployability,instal,install,83,This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:41,modifiability,pac,package,41,This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:104,modifiability,pac,package,104,This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:85,deployability,instal,install,85,"> This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365. Thanks! For future reference, I uninstall nmslib then built from source using `pip install --no-binary :all: nmslib`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:233,deployability,instal,install,233,"> This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365. Thanks! For future reference, I uninstall nmslib then built from source using `pip install --no-binary :all: nmslib`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:43,modifiability,pac,package,43,"> This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365. Thanks! For future reference, I uninstall nmslib then built from source using `pip install --no-binary :all: nmslib`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:106,modifiability,pac,package,106,"> This is probably related to the `nmslib` package. Can you check if you are able to install and use that package? Looks like the same issue as #365. Thanks! For future reference, I uninstall nmslib then built from source using `pip install --no-binary :all: nmslib`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:76,deployability,instal,install,76,"this solved for me on MacOs M1 Monterey : `CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib`. https://github.com/nmslib/nmslib/issues/476#issuecomment-881044967",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/373:98,availability,down,download,98,"what do you mean by ""keep running"", it does take a few minutes the first time because it needs to download a bunch of files. And here is the example of using the linker in the readme (https://github.com/allenai/scispacy#example-usage-1)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:73,performance,time,time,73,"what do you mean by ""keep running"", it does take a few minutes the first time because it needs to download a bunch of files. And here is the example of using the linker in the readme (https://github.com/allenai/scispacy#example-usage-1)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:39,reliability,doe,does,39,"what do you mean by ""keep running"", it does take a few minutes the first time because it needs to download a bunch of files. And here is the example of using the linker in the readme (https://github.com/allenai/scispacy#example-usage-1)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:30,availability,down,downloading,30,"Oh yea, turns out it was just downloading a bunch of files. I waited and everything worked. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/375:128,integrability,transform,transformers,128,"hmm, it looks like this is a spacy internal issue, so I will leave it to them. An off the cuff guess is that something in spacy-transformers assumes that each token is less than 512 characters without checking. if it is not too hard, you could try preprocessing your data to make sure those really long tokens don't exist (just throw some space in the middle, or remove them, because it is pretty unlikely that spacy/scispacy are going to do anything reasonable with some long latex string)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:128,interoperability,transform,transformers,128,"hmm, it looks like this is a spacy internal issue, so I will leave it to them. An off the cuff guess is that something in spacy-transformers assumes that each token is less than 512 characters without checking. if it is not too hard, you could try preprocessing your data to make sure those really long tokens don't exist (just throw some space in the middle, or remove them, because it is pretty unlikely that spacy/scispacy are going to do anything reasonable with some long latex string)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:159,security,token,token,159,"hmm, it looks like this is a spacy internal issue, so I will leave it to them. An off the cuff guess is that something in spacy-transformers assumes that each token is less than 512 characters without checking. if it is not too hard, you could try preprocessing your data to make sure those really long tokens don't exist (just throw some space in the middle, or remove them, because it is pretty unlikely that spacy/scispacy are going to do anything reasonable with some long latex string)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:303,security,token,tokens,303,"hmm, it looks like this is a spacy internal issue, so I will leave it to them. An off the cuff guess is that something in spacy-transformers assumes that each token is less than 512 characters without checking. if it is not too hard, you could try preprocessing your data to make sure those really long tokens don't exist (just throw some space in the middle, or remove them, because it is pretty unlikely that spacy/scispacy are going to do anything reasonable with some long latex string)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:164,integrability,filter,filtering,164,"Thanks, the real data isn't this bad. The backslashes seem to cause the problem as long as there are enough of them, but not necessarily in a single sequence. I am filtering the sentences with equations from the data and everything looks good. Thanks for the quick response.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/376:45,integrability,filter,filter,45,"It isn't really that either, it is just that filter for definitions requires the use of two parameters,`filter_for_no_definitions` and `no_definition_threshold`. I suppose we didn't really need both of these parameters, and could have just used `no_definition_threshold`, and have setting it to `None` mean ""don't filter"".",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:314,integrability,filter,filter,314,"It isn't really that either, it is just that filter for definitions requires the use of two parameters,`filter_for_no_definitions` and `no_definition_threshold`. I suppose we didn't really need both of these parameters, and could have just used `no_definition_threshold`, and have setting it to `None` mean ""don't filter"".",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:92,modifiability,paramet,parameters,92,"It isn't really that either, it is just that filter for definitions requires the use of two parameters,`filter_for_no_definitions` and `no_definition_threshold`. I suppose we didn't really need both of these parameters, and could have just used `no_definition_threshold`, and have setting it to `None` mean ""don't filter"".",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:208,modifiability,paramet,parameters,208,"It isn't really that either, it is just that filter for definitions requires the use of two parameters,`filter_for_no_definitions` and `no_definition_threshold`. I suppose we didn't really need both of these parameters, and could have just used `no_definition_threshold`, and have setting it to `None` mean ""don't filter"".",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:136,deployability,API,API,136,"I'm actually not sure I want to make this change since the benefit is pretty small, but it would a breaking change to the entity linker API. Let me think about it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:136,integrability,API,API,136,"I'm actually not sure I want to make this change since the benefit is pretty small, but it would a breaking change to the entity linker API. Let me think about it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:136,interoperability,API,API,136,"I'm actually not sure I want to make this change since the benefit is pretty small, but it would a breaking change to the entity linker API. Let me think about it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:60,energy efficiency,current,current,60,"I think I am not going to do this, I think the docs for the current params is good enough, and don't want to introduce a breaking change for this small of a reason. Thank you though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/378:113,deployability,pipelin,pipeline,113,"I don't think this will work as the linkers would just be overwriting properties, so even if you did have 2 in a pipeline, the resulting ent._.kb_ents would only come from the last one you call. . The way to hack around it would be to add the possibility of passing the extension name (instead of kb_ents it could be like mash_ents and umls_ents) when you instantiate the linker. EDIT: just realized umls_ents already exists, in which case you would need 4 new ones not just 2 (so kb_ents, umls_ents, sth_else_ents, mesh_ents) .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:113,integrability,pipelin,pipeline,113,"I don't think this will work as the linkers would just be overwriting properties, so even if you did have 2 in a pipeline, the resulting ent._.kb_ents would only come from the last one you call. . The way to hack around it would be to add the possibility of passing the extension name (instead of kb_ents it could be like mash_ents and umls_ents) when you instantiate the linker. EDIT: just realized umls_ents already exists, in which case you would need 4 new ones not just 2 (so kb_ents, umls_ents, sth_else_ents, mesh_ents) .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:270,modifiability,extens,extension,270,"I don't think this will work as the linkers would just be overwriting properties, so even if you did have 2 in a pipeline, the resulting ent._.kb_ents would only come from the last one you call. . The way to hack around it would be to add the possibility of passing the extension name (instead of kb_ents it could be like mash_ents and umls_ents) when you instantiate the linker. EDIT: just realized umls_ents already exists, in which case you would need 4 new ones not just 2 (so kb_ents, umls_ents, sth_else_ents, mesh_ents) .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:208,security,hack,hack,208,"I don't think this will work as the linkers would just be overwriting properties, so even if you did have 2 in a pipeline, the resulting ent._.kb_ents would only come from the last one you call. . The way to hack around it would be to add the possibility of passing the extension name (instead of kb_ents it could be like mash_ents and umls_ents) when you instantiate the linker. EDIT: just realized umls_ents already exists, in which case you would need 4 new ones not just 2 (so kb_ents, umls_ents, sth_else_ents, mesh_ents) .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:277,availability,state,state,277,"^This is correct. I would suggest instantiating the linkers separately, and running them in a loop and then doing whatever you want with the output. something like. ```. doc = nlp_without_linker(text). for linker in linker:. linked_doc = linker(doc). # save/clear whatever doc state you want. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:277,integrability,state,state,277,"^This is correct. I would suggest instantiating the linkers separately, and running them in a loop and then doing whatever you want with the output. something like. ```. doc = nlp_without_linker(text). for linker in linker:. linked_doc = linker(doc). # save/clear whatever doc state you want. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:258,usability,clear,clear,258,"^This is correct. I would suggest instantiating the linkers separately, and running them in a loop and then doing whatever you want with the output. something like. ```. doc = nlp_without_linker(text). for linker in linker:. linked_doc = linker(doc). # save/clear whatever doc state you want. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:59,energy efficiency,model,models,59,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:66,energy efficiency,load,loaded,66,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:66,performance,load,loaded,66,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:59,security,model,models,59,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:34,testability,simpl,simply,34,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:34,usability,simpl,simply,34,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/379:42,availability,error,error,42,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:92,deployability,resourc,resources,92,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:79,energy efficiency,load,loading,79,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:92,energy efficiency,resourc,resources,92,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:239,energy efficiency,load,loaded,239,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:42,performance,error,error,42,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:79,performance,load,loading,79,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:92,performance,resourc,resources,92,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:239,performance,load,loaded,239,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:42,safety,error,error,42,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:92,safety,resourc,resources,92,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:92,testability,resourc,resources,92,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:42,usability,error,error,42,"The differential in size mentioned in the error makes me think you are somehow loading some resources for mesh and some for umls. Could you just triple check that you have the right paths for mesh, and those are the files actually getting loaded, before we dig deeper?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:145,usability,close,close,145,"I think you are correct. I redownloaded all the files, deleted the previous folder and copied again and now it seems to be working. Thanks! I'll close the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/380:19,deployability,pipelin,pipeline,19,"The tok2vec in the pipeline is just used for the parser/tagger, the ner has its own tok2vec currently. this is a bit confusing and suboptimal, but it is the way it works right now. you can check out the https://github.com/allenai/scispacy/tree/main/configs files for more details. so to just run NER, you should be able to disable everything else without changing output, although definitely let me know if you find that not to be the case!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:92,energy efficiency,current,currently,92,"The tok2vec in the pipeline is just used for the parser/tagger, the ner has its own tok2vec currently. this is a bit confusing and suboptimal, but it is the way it works right now. you can check out the https://github.com/allenai/scispacy/tree/main/configs files for more details. so to just run NER, you should be able to disable everything else without changing output, although definitely let me know if you find that not to be the case!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:19,integrability,pipelin,pipeline,19,"The tok2vec in the pipeline is just used for the parser/tagger, the ner has its own tok2vec currently. this is a bit confusing and suboptimal, but it is the way it works right now. you can check out the https://github.com/allenai/scispacy/tree/main/configs files for more details. so to just run NER, you should be able to disable everything else without changing output, although definitely let me know if you find that not to be the case!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:131,integrability,sub,suboptimal,131,"The tok2vec in the pipeline is just used for the parser/tagger, the ner has its own tok2vec currently. this is a bit confusing and suboptimal, but it is the way it works right now. you can check out the https://github.com/allenai/scispacy/tree/main/configs files for more details. so to just run NER, you should be able to disable everything else without changing output, although definitely let me know if you find that not to be the case!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/381:86,energy efficiency,model,models,86,Any embedding space will have some unexpected stuff. The word vectors in the scispacy models are word2vec trained on pubmed abstracts. Going to go ahead and close this issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:117,integrability,pub,pubmed,117,Any embedding space will have some unexpected stuff. The word vectors in the scispacy models are word2vec trained on pubmed abstracts. Going to go ahead and close this issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:124,integrability,abstract,abstracts,124,Any embedding space will have some unexpected stuff. The word vectors in the scispacy models are word2vec trained on pubmed abstracts. Going to go ahead and close this issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:124,modifiability,abstract,abstracts,124,Any embedding space will have some unexpected stuff. The word vectors in the scispacy models are word2vec trained on pubmed abstracts. Going to go ahead and close this issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:86,security,model,models,86,Any embedding space will have some unexpected stuff. The word vectors in the scispacy models are word2vec trained on pubmed abstracts. Going to go ahead and close this issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:157,usability,close,close,157,Any embedding space will have some unexpected stuff. The word vectors in the scispacy models are word2vec trained on pubmed abstracts. Going to go ahead and close this issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/382:125,availability,error,error,125,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:186,integrability,configur,configure-files,186,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:186,modifiability,configur,configure-files,186,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:125,performance,error,error,125,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:125,safety,error,error,125,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:186,security,configur,configure-files,186,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:125,usability,error,error,125,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:172,usability,user,userguide,172,This seems like a credentials issue in awscli:. https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:129,availability,error,error,129,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:241,availability,down,downloading,241,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:332,availability,error,error,332,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:350,availability,error,error,350,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:360,availability,error,error,360,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:409,availability,operat,operation,409,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:192,integrability,configur,configure-files,192,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:192,modifiability,configur,configure-files,192,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:129,performance,error,error,129,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:332,performance,error,error,332,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:350,performance,error,error,350,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:360,performance,error,error,360,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:500,reliability,doe,does,500,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:129,safety,error,error,129,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:332,safety,error,error,332,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:350,safety,error,error,350,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:360,safety,error,error,360,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:192,security,configur,configure-files,192,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:129,usability,error,error,129,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:178,usability,user,userguide,178,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:332,usability,error,error,332,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:350,usability,error,error,350,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:360,usability,error,error,360,> This seems like a credentials issue in awscli:. > https://aws.amazon.com/premiumsupport/knowledge-center/s3-locate-credentials-error/. > https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. Have you ever succeeded in downloading these three files? I have already tried this method but it shows another fatal error. '''. fatal error: An error occurred (403) when calling the HeadObject operation:. '''. Someone says that this is because it is not my bucket. I am not sure what does this mean and what to do next. I am totally new to the aws. Thanks for your response!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:166,integrability,pub,public,166,"Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:216,integrability,pub,publicly,216,"Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:294,safety,permiss,permissions,294,"Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:72,availability,error,error,72,"^ The umls type tree works for me, the onto notes doesn't with the same error. So I think this is the exact solution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:72,performance,error,error,72,"^ The umls type tree works for me, the onto notes doesn't with the same error. So I think this is the exact solution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:50,reliability,doe,doesn,50,"^ The umls type tree works for me, the onto notes doesn't with the same error. So I think this is the exact solution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:72,safety,error,error,72,"^ The umls type tree works for me, the onto notes doesn't with the same error. So I think this is the exact solution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:72,usability,error,error,72,"^ The umls type tree works for me, the onto notes doesn't with the same error. So I think this is the exact solution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:752,availability,error,error,752,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:888,availability,error,error,888,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:898,availability,error,error,898,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:947,availability,operat,operation,947,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1070,availability,error,error,1070,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1080,availability,error,error,1080,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1141,availability,operat,operation,1141,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1246,availability,Servic,Services,1246,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1478,availability,down,download,1478,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1246,deployability,Servic,Services,1246,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:168,integrability,pub,public,168,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:218,integrability,pub,publicly,218,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1246,integrability,Servic,Services,1246,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1366,integrability,pub,public,1366,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1440,interoperability,format,format,1440,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1246,modifiability,Servic,Services,1246,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:752,performance,error,error,752,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:888,performance,error,error,888,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:898,performance,error,error,898,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1070,performance,error,error,1070,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1080,performance,error,error,1080,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:734,reliability,doe,doesn,734,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:296,safety,permiss,permissions,296,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:752,safety,error,error,752,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:888,safety,error,error,888,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:898,safety,error,error,898,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1070,safety,error,error,1070,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1080,safety,error,error,1080,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1096,security,Access,AccessDenied,1096,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1152,security,Access,Access,1152,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:752,usability,error,error,752,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:888,usability,error,error,888,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:898,usability,error,error,898,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1070,usability,error,error,1070,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:1080,usability,error,error,1080,"> Do you have this issue for all files (e.g. try just this one `s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv`)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks! This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file, then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:182,deployability,depend,depending,182,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:148,energy efficiency,model,models,148,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:182,integrability,depend,depending,182,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:182,modifiability,depend,depending,182,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:182,safety,depend,depending,182,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:148,security,model,models,148,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:182,testability,depend,depending,182,"Try the ner folder again? The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:385,availability,error,error,385,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:190,deployability,depend,depending,190,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:156,energy efficiency,model,models,156,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:190,integrability,depend,depending,190,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:190,modifiability,depend,depending,190,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:385,performance,error,error,385,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:190,safety,depend,depending,190,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:385,safety,error,error,385,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:156,security,model,models,156,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:190,testability,depend,depending,190,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:385,usability,error,error,385,"> Try the ner folder again? > . > The other one is ontonotes (https://catalog.ldc.upenn.edu/LDC2013T19), which is the corpus used to train the normal spacy models. It might not be necessary depending on what you are trying to do. We mix it in during training, but mainly use biomedical data. Try the ner folder again?: I try on different computers but all show the following:. fatal error: Unable to locate credentials",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:21,availability,error,error,21,that is the original error. you need to set your aws credentials,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:21,performance,error,error,21,that is the original error. you need to set your aws credentials,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:21,safety,error,error,21,that is the original error. you need to set your aws credentials,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:21,usability,error,error,21,that is the original error. you need to set your aws credentials,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:23,availability,error,error,23,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:177,availability,error,error,177,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:187,availability,error,error,187,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:248,availability,operat,operation,248,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:23,performance,error,error,23,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:177,performance,error,error,177,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:187,performance,error,error,187,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:23,safety,error,error,23,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:177,safety,error,error,177,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:187,safety,error,error,187,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:203,security,Access,AccessDenied,203,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:259,security,Access,Access,259,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:23,usability,error,error,23,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:177,usability,error,error,177,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:187,usability,error,error,187,"> that is the original error. you need to set your aws credentials. Again,. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:129,availability,down,download,129,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:42,safety,permiss,permission,42,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:258,safety,test,test,258,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:427,safety,test,test,427,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:590,safety,test,test,590,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:750,safety,test,test,750,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:258,testability,test,test,258,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:427,testability,test,test,427,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:590,testability,test,test,590,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:750,testability,test,test,750,"Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files . - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. - s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:131,availability,down,download,131,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:868,availability,down,download,868,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:44,safety,permiss,permission,44,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:267,safety,test,test,267,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:442,safety,test,test,442,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:611,safety,test,test,611,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:777,safety,test,test,777,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:267,testability,test,test,267,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:442,testability,test,test,442,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:611,testability,test,test,611,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:777,testability,test,test,777,"> Sorry, I'm having trouble getting the AWS permission right...I will try to figure this out, but you should definitely be able to download the individual files. > . > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BC5CDR-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/BioNLP13CG-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/CRAFT-IOB/train.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/devel.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/test.tsv. > * s3://ai2-s2-scispacy/data/ner/JNLPBA-IOB/train.tsv. Thank you so much! I can download these files!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:45,usability,command,command,45,"Ok, I think I got it right now, the original command should work",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:181,availability,down,download,181,"> Ok, I think I got it right now, the original command should work. Yes, the original command works now. . Also, I registered on https://catalog.ldc.upenn.edu/LDC2013T19 and try to download ud_ontonotes.tar.gz to make all things run. Thanks for your patience and instruction!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:47,usability,command,command,47,"> Ok, I think I got it right now, the original command should work. Yes, the original command works now. . Also, I registered on https://catalog.ldc.upenn.edu/LDC2013T19 and try to download ud_ontonotes.tar.gz to make all things run. Thanks for your patience and instruction!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:86,usability,command,command,86,"> Ok, I think I got it right now, the original command should work. Yes, the original command works now. . Also, I registered on https://catalog.ldc.upenn.edu/LDC2013T19 and try to download ud_ontonotes.tar.gz to make all things run. Thanks for your patience and instruction!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/383:70,reliability,doe,doesn,70,"You _might_ just be missing `from scispacy.linking import *`. If that doesn't work, I'll have a closer look later.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:96,usability,close,closer,96,"You _might_ just be missing `from scispacy.linking import *`. If that doesn't work, I'll have a closer look later.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:35,availability,error,error,35,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:77,deployability,contain,containing,77,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:67,energy efficiency,load,load,67,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:35,performance,error,error,35,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:67,performance,load,load,67,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:35,safety,error,error,35,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:35,usability,error,error,35,I tried that and I got a different error:. ```. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,availability,error,error,19,Is that the entire error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,performance,error,error,19,Is that the entire error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,safety,error,error,19,Is that the entire error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,usability,error,error,19,Is that the entire error?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,availability,error,error,19,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1636,availability,error,error,1636,"ucting the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:172,deployability,modul,module,172,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1701,deployability,fail,fails,1701,"registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3351,deployability,contain,containing,3351,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3449,deployability,contain,containing,3449,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3055,energy efficiency,load,load,3055,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3086,energy efficiency,load,loaded,3086,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3125,energy efficiency,load,loaded,3125,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3202,energy efficiency,load,load,3202,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3341,energy efficiency,load,load,3341,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3439,energy efficiency,load,load,3439,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:934,integrability,schema,schema,934,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1064,integrability,schema,schema,1064,"----------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1071,integrability,schema,schema,1071,"---------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1215,integrability,schema,schema,1215,"spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1385,integrability,schema,schema,1385,"omponent = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1532,integrability,schema,schema,1532,"y_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_neare",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:704,interoperability,registr,registry,704,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:759,interoperability,registr,registry,759,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3133,interoperability,format,format,3133,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:172,modifiability,modul,module,172,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:209,modifiability,pac,packages,209,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:481,modifiability,pac,packages,481,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:885,modifiability,pac,packages,885,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1168,modifiability,pac,packages,1168,"module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1485,modifiability,pac,packages,1485,"/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.a",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1860,modifiability,pac,packages,1860,"l/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2262,modifiability,pac,packages,2262,"s_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2644,modifiability,pac,packages,2644,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2969,modifiability,pac,packages,2969,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3171,modifiability,pac,packages,3171,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,performance,error,error,19,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:156,performance,time,timed,156,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1636,performance,error,error,1636,"ucting the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2786,performance,perform,performance,2786,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3055,performance,load,load,3055,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3086,performance,load,loaded,3086,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3125,performance,load,loaded,3125,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3202,performance,load,load,3202,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3341,performance,load,load,3341,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:3439,performance,load,load,3439,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1701,reliability,fail,fails,1701,"registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,safety,error,error,19,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:172,safety,modul,module,172,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:330,safety,valid,validate,330,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:569,safety,valid,validate,569,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:627,safety,avoid,avoid,627,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:726,safety,valid,validate,726,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:735,safety,valid,validate,735,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:801,safety,valid,validate,801,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:810,safety,valid,validate,810,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:953,safety,valid,validate,953,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:968,safety,valid,validate,968,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1100,safety,valid,validate,1100,"---. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", defaul",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1109,safety,valid,validate,1109,"eError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], for",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1243,safety,valid,validate,1243,"(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1393,safety,valid,validate,1393,"= self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_sear",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1402,safety,valid,validate,1402,"eate_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1540,safety,valid,validate,1540,"ame, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1606,safety,except,except,1606,"nal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1636,safety,error,error,1636,"ucting the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:330,security,validat,validate,330,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:569,security,validat,validate,569,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:726,security,validat,validate,726,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:735,security,validat,validate,735,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:801,security,validat,validate,801,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:810,security,validat,validate,810,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:953,security,validat,validate,953,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:968,security,validat,validate,968,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1100,security,validat,validate,1100,"---. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", defaul",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1109,security,validat,validate,1109,"eError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], for",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1243,security,validat,validate,1243,"(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1393,security,validat,validate,1393,"= self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_sear",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1402,security,validat,validate,1402,"eate_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1540,security,validat,validate,1540,"ame, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:120,testability,Trace,Traceback,120,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1675,testability,trace,traceback,1675,"ice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generati",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:19,usability,error,error,19,"Here is the entire error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 771 lang_code=self.lang,. 772 ). --> 773 pipe_component = self.create_pipe(. 774 factory_name,. 775 name=name,. ~/.local/lib/python3.8/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1636,usability,error,error,1636,"ucting the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/.local/lib/python3.8/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 725 validate: bool = True,. 726 ) -> Dict[str, Any]:. --> 727 resolved, _ = cls._make(. 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 774 if not is_interpolated:. 775 config = Config(orig_config).interpolate(). --> 776 filled, _, resolved = cls._fill(. 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). ~/.local/lib/python3.8/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2786,usability,perform,performance,2786,"onfig.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/.local/lib/python3.8/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 82 Span.set_extension(""kb_ents"", default=[], force=True). 83 . ---> 84 self.candidate_generator = candidate_generator or CandidateGenerator(. 85 name=linker_name. 86 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 220 linker_paths = DEFAULT_PATHS.get(name, UmlsLinkerPaths). 221 . --> 222 self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. 223 linker_paths=linker_paths, ef_search=ef_search. 224 ). ~/.local/lib/python3.8/site-packages/scispacy/candidate_generation.py in load_approximate_nearest_neighbours_index(linker_paths, ef_search). 130 of magnitude for a small performance hit. 131 """""". --> 132 concept_alias_tfidfs = scipy.sparse.load_npz(. 133 cached_path(linker_paths.tfidf_vectors). 134 ).astype(numpy.float32). ~/.local/lib/python3.8/site-packages/scipy/sparse/_matrix_io.py in load_npz(file). 121 """""". 122 . --> 123 with np.load(file, **PICKLE_KWARGS) as loaded:. 124 try:. 125 matrix_format = loaded['format']. ~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding). 443 # Try a pickle. 444 if not allow_pickle:. --> 445 raise ValueError(""Cannot load file containing pickled data "". 446 ""when allow_pickle=False""). 447 try:. ValueError: Cannot load file containing pickled data when allow_pickle=False. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/384:489,energy efficiency,load,load,489,"I'd have to dig into this a bit more to debug, but you might want to open your tfidf vectors file and make sure that m/s is actually present. i suspect it might not be because its a character trigram vectorizer, and probably normalize out the `/`. I'm not 100% sure of this, but the things to check are 1) `m/s` is recognized as an entity (which you did) 2) `m/s` is present in your kb (you did) 3) check the representation that the vectorizer produces if you just pass it that text (just load the vectorizer object separately and poke around)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:489,performance,load,load,489,"I'd have to dig into this a bit more to debug, but you might want to open your tfidf vectors file and make sure that m/s is actually present. i suspect it might not be because its a character trigram vectorizer, and probably normalize out the `/`. I'm not 100% sure of this, but the things to check are 1) `m/s` is recognized as an entity (which you did) 2) `m/s` is present in your kb (you did) 3) check the representation that the vectorizer produces if you just pass it that text (just load the vectorizer object separately and poke around)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:66,energy efficiency,load,load,66,"Hello Daniel, thanks for the reply. However, I am not sure how to load the vectorizer separately. I tried doing this:. ```. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index="""",. tfidf_vectorizer=""output/tfidf_vectorizer.joblib"",. tfidf_vectors="""",. concept_aliases_list="""",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). linker = CandidateGenerator(name=""myCustom""). ```. but it gave me a `FileNotFoundError: file not found` . I also tried loading the joblib file but I am not sure where to go from here:. ```. import joblib. joblib.load('output/tfidf_vectorizer.joblib'). #TfidfVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>, min_df=10, ngram_range=(3, 3)). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:743,energy efficiency,load,loading,743,"Hello Daniel, thanks for the reply. However, I am not sure how to load the vectorizer separately. I tried doing this:. ```. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index="""",. tfidf_vectorizer=""output/tfidf_vectorizer.joblib"",. tfidf_vectors="""",. concept_aliases_list="""",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). linker = CandidateGenerator(name=""myCustom""). ```. but it gave me a `FileNotFoundError: file not found` . I also tried loading the joblib file but I am not sure where to go from here:. ```. import joblib. joblib.load('output/tfidf_vectorizer.joblib'). #TfidfVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>, min_df=10, ngram_range=(3, 3)). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:836,energy efficiency,load,load,836,"Hello Daniel, thanks for the reply. However, I am not sure how to load the vectorizer separately. I tried doing this:. ```. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index="""",. tfidf_vectorizer=""output/tfidf_vectorizer.joblib"",. tfidf_vectors="""",. concept_aliases_list="""",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). linker = CandidateGenerator(name=""myCustom""). ```. but it gave me a `FileNotFoundError: file not found` . I also tried loading the joblib file but I am not sure where to go from here:. ```. import joblib. joblib.load('output/tfidf_vectorizer.joblib'). #TfidfVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>, min_df=10, ngram_range=(3, 3)). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:66,performance,load,load,66,"Hello Daniel, thanks for the reply. However, I am not sure how to load the vectorizer separately. I tried doing this:. ```. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index="""",. tfidf_vectorizer=""output/tfidf_vectorizer.joblib"",. tfidf_vectors="""",. concept_aliases_list="""",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). linker = CandidateGenerator(name=""myCustom""). ```. but it gave me a `FileNotFoundError: file not found` . I also tried loading the joblib file but I am not sure where to go from here:. ```. import joblib. joblib.load('output/tfidf_vectorizer.joblib'). #TfidfVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>, min_df=10, ngram_range=(3, 3)). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:743,performance,load,loading,743,"Hello Daniel, thanks for the reply. However, I am not sure how to load the vectorizer separately. I tried doing this:. ```. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index="""",. tfidf_vectorizer=""output/tfidf_vectorizer.joblib"",. tfidf_vectors="""",. concept_aliases_list="""",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). linker = CandidateGenerator(name=""myCustom""). ```. but it gave me a `FileNotFoundError: file not found` . I also tried loading the joblib file but I am not sure where to go from here:. ```. import joblib. joblib.load('output/tfidf_vectorizer.joblib'). #TfidfVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>, min_df=10, ngram_range=(3, 3)). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:836,performance,load,load,836,"Hello Daniel, thanks for the reply. However, I am not sure how to load the vectorizer separately. I tried doing this:. ```. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index="""",. tfidf_vectorizer=""output/tfidf_vectorizer.joblib"",. tfidf_vectors="""",. concept_aliases_list="""",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). linker = CandidateGenerator(name=""myCustom""). ```. but it gave me a `FileNotFoundError: file not found` . I also tried loading the joblib file but I am not sure where to go from here:. ```. import joblib. joblib.load('output/tfidf_vectorizer.joblib'). #TfidfVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>, min_df=10, ngram_range=(3, 3)). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:48,testability,unit,units,48,"I also tried the same script but with different units (without the ""/"") like: `kg`, `bpm`, `ml` and all of them gave the same results. ```. text = ""TR Max Velocity: 2.3 kg"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(""xx""). print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. would give:. ```. Name: kg. ```. so I am not sure if ""/"" is the problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:57,deployability,pipelin,pipeline,57,"Can you 1) make sure that the linker is actually in your pipeline (`nlp.pipeline`) and 2) try changing the filtering, so set `filter_for_definitions=False` and `threshold=0.1`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:72,deployability,pipelin,pipeline,72,"Can you 1) make sure that the linker is actually in your pipeline (`nlp.pipeline`) and 2) try changing the filtering, so set `filter_for_definitions=False` and `threshold=0.1`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:57,integrability,pipelin,pipeline,57,"Can you 1) make sure that the linker is actually in your pipeline (`nlp.pipeline`) and 2) try changing the filtering, so set `filter_for_definitions=False` and `threshold=0.1`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:72,integrability,pipelin,pipeline,72,"Can you 1) make sure that the linker is actually in your pipeline (`nlp.pipeline`) and 2) try changing the filtering, so set `filter_for_definitions=False` and `threshold=0.1`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:107,integrability,filter,filtering,107,"Can you 1) make sure that the linker is actually in your pipeline (`nlp.pipeline`) and 2) try changing the filtering, so set `filter_for_definitions=False` and `threshold=0.1`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:297,deployability,pipelin,pipeline,297,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:465,deployability,pipelin,pipeline,465,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:536,deployability,pipelin,pipeline,536,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:605,deployability,pipelin,pipeline,605,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:625,deployability,Depend,DependencyParser,625,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:697,deployability,pipelin,pipeline,697,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:788,deployability,pipelin,pipeline,788,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:864,deployability,pipelin,pipeline,864,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1101,deployability,pipelin,pipeline,1101,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:45,energy efficiency,load,load,45,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:297,integrability,pipelin,pipeline,297,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:465,integrability,pipelin,pipeline,465,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:536,integrability,pipelin,pipeline,536,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:605,integrability,pipelin,pipeline,605,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:625,integrability,Depend,DependencyParser,625,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:697,integrability,pipelin,pipeline,697,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:788,integrability,pipelin,pipeline,788,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:864,integrability,pipelin,pipeline,864,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1101,integrability,pipelin,pipeline,1101,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:625,modifiability,Depend,DependencyParser,625,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:45,performance,load,load,45,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:625,safety,Depend,DependencyParser,625,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:625,testability,Depend,DependencyParser,625,"I did what you told me to:. ```. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom"",. ""filter_for_definitions"": False, ""threshold"": ""0.1""}). linker = CandidateGenerator(name=""myCustom""). print(nlp.pipe_names). print(nlp.pipeline). ```. which returned:. ```. ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'entity_ruler', 'ner', 'value_entity', 'scispacy_linker']. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f3fad155ef0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f3fb0732220>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f3fabf96d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f3fb0fbb980>), ('entity_ruler', <spacy.pipeline.entityruler.EntityRuler object at 0x7f3fb46b9840>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f3fabf96520>), ('value_entity', <function value_entity at 0x7f3fb1121b80>), ('scispacy_linker', <scispacy.linking.EntityLinker object at 0x7f3fabefc430>)]. ```. Showing that the linker is in my pipeline, then I ran the same sentence as above but I still got the same result.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:115,performance,time,time,115,"Ok, at this point I would suggest setting a breakpoint inside the entity linker and stepping through one line at a time until you can see why there are no entities returned. I'm not sure if it is because there is something wrong with your nmslib index, or some other issue",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:452,usability,custom,custom,452,"From debugging I noticed that the difference with my knowledge base compared to the UMLS one from scispacy is that there are no `batch_neighbors` and `batch_distances` when it calls `candidate_generation.py` from `linking.py` at line 116:. ```. batch_candidates = self.candidate_generator(mention_strings, self.k). ```. Do you know why this is the case? . is a possibility because my KB only has 25 entries (as I was just experimenting on how to use a custom KB in scispacy)? Would this problem disappear with more entries in the KB. Another thing to note was it only took around 1s for me to run the script to make the ann_index and in the discussions it normally takes hours.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:64,safety,reme,remember,64,"25 entries is very few entries, UMLS has ~4 Million as far as I remember",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:85,deployability,updat,update,85,"Ah so most likely the problem lies there, alright then I'll try with a larger KB and update. . But as far as the process to add the custom KB, is what I am doing correct?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:85,safety,updat,update,85,"Ah so most likely the problem lies there, alright then I'll try with a larger KB and update. . But as far as the process to add the custom KB, is what I am doing correct?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:85,security,updat,update,85,"Ah so most likely the problem lies there, alright then I'll try with a larger KB and update. . But as far as the process to add the custom KB, is what I am doing correct?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:132,usability,custom,custom,132,"Ah so most likely the problem lies there, alright then I'll try with a larger KB and update. . But as far as the process to add the custom KB, is what I am doing correct?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:416,deployability,api,api,416,"I'd be a bit surprised if the small number of entries was the problem, but I am not intimately familiar with nmslib. One thing to try is to load your nmslib index separately (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L116), and then see if you can play around with it following their docs (starting point here, https://nmslib.github.io/nmslib/api.html#nmslib-floatindex), and just make sure it ended up with the entries you expected and everything looks reasonable.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:140,energy efficiency,load,load,140,"I'd be a bit surprised if the small number of entries was the problem, but I am not intimately familiar with nmslib. One thing to try is to load your nmslib index separately (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L116), and then see if you can play around with it following their docs (starting point here, https://nmslib.github.io/nmslib/api.html#nmslib-floatindex), and just make sure it ended up with the entries you expected and everything looks reasonable.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:416,integrability,api,api,416,"I'd be a bit surprised if the small number of entries was the problem, but I am not intimately familiar with nmslib. One thing to try is to load your nmslib index separately (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L116), and then see if you can play around with it following their docs (starting point here, https://nmslib.github.io/nmslib/api.html#nmslib-floatindex), and just make sure it ended up with the entries you expected and everything looks reasonable.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:416,interoperability,api,api,416,"I'd be a bit surprised if the small number of entries was the problem, but I am not intimately familiar with nmslib. One thing to try is to load your nmslib index separately (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L116), and then see if you can play around with it following their docs (starting point here, https://nmslib.github.io/nmslib/api.html#nmslib-floatindex), and just make sure it ended up with the entries you expected and everything looks reasonable.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:140,performance,load,load,140,"I'd be a bit surprised if the small number of entries was the problem, but I am not intimately familiar with nmslib. One thing to try is to load your nmslib index separately (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L116), and then see if you can play around with it following their docs (starting point here, https://nmslib.github.io/nmslib/api.html#nmslib-floatindex), and just make sure it ended up with the entries you expected and everything looks reasonable.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:983,availability,state,statement,983,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1364,deployability,UPDAT,UPDATE,1364,"[umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m', 'mhg', 'mm ', '/m*', 'cm ', ' l ', 'und', 'cm', 'sec', 'm ', 'hg ', 'bpm', 'es ', ' l/', 'oun', ' kg', ' % ', 'm*m', ' m2', ' / ', 'cm2', '/se', ' lb', 'lbs', 'bs '}. ```. and the entities I tried with are `m/s` and `kg` which are both in the `stop_words_` list. Is this why the vectorizer shows none and if so how do I fix this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1464,deployability,contain,contains,1464,"[umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m', 'mhg', 'mm ', '/m*', 'cm ', ' l ', 'und', 'cm', 'sec', 'm ', 'hg ', 'bpm', 'es ', ' l/', 'oun', ' kg', ' % ', 'm*m', ' m2', ' / ', 'cm2', '/se', ' lb', 'lbs', 'bs '}. ```. and the entities I tried with are `m/s` and `kg` which are both in the `stop_words_` list. Is this why the vectorizer shows none and if so how do I fix this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1544,energy efficiency,load,load,1544,"[umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m', 'mhg', 'mm ', '/m*', 'cm ', ' l ', 'und', 'cm', 'sec', 'm ', 'hg ', 'bpm', 'es ', ' l/', 'oun', ' kg', ' % ', 'm*m', ' m2', ' / ', 'cm2', '/se', ' lb', 'lbs', 'bs '}. ```. and the entities I tried with are `m/s` and `kg` which are both in the `stop_words_` list. Is this why the vectorizer shows none and if so how do I fix this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:983,integrability,state,statement,983,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1544,performance,load,load,1544,"[umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m', 'mhg', 'mm ', '/m*', 'cm ', ' l ', 'und', 'cm', 'sec', 'm ', 'hg ', 'bpm', 'es ', ' l/', 'oun', ' kg', ' % ', 'm*m', ' m2', ' / ', 'cm2', '/se', ' lb', 'lbs', 'bs '}. ```. and the entities I tried with are `m/s` and `kg` which are both in the `stop_words_` list. Is this why the vectorizer shows none and if so how do I fix this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1364,safety,UPDAT,UPDATE,1364,"[umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m', 'mhg', 'mm ', '/m*', 'cm ', ' l ', 'und', 'cm', 'sec', 'm ', 'hg ', 'bpm', 'es ', ' l/', 'oun', ' kg', ' % ', 'm*m', ' m2', ' / ', 'cm2', '/se', ' lb', 'lbs', 'bs '}. ```. and the entities I tried with are `m/s` and `kg` which are both in the `stop_words_` list. Is this why the vectorizer shows none and if so how do I fix this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:1364,security,UPDAT,UPDATE,1364,"[umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m', 'mhg', 'mm ', '/m*', 'cm ', ' l ', 'und', 'cm', 'sec', 'm ', 'hg ', 'bpm', 'es ', ' l/', 'oun', ' kg', ' % ', 'm*m', ' m2', ' / ', 'cm2', '/se', ' lb', 'lbs', 'bs '}. ```. and the entities I tried with are `m/s` and `kg` which are both in the `stop_words_` list. Is this why the vectorizer shows none and if so how do I fix this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:351,usability,user,user-images,351,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:477,usability,custom,custom,477,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:491,usability,custom,custom,491,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:510,usability,user,user-images,510,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:804,usability,custom,custom,804,"I did some more digging and I am not sure if the problem is the ann_index from the function above. I found out that the tfidf values from. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L324. give different results. So here is the result from the normal UMLS KB:. ![umls kb](https://user-images.githubusercontent.com/54917098/128267665-b054a198-9192-4b58-aef9-03562076b83a.png). and here is the result of the custom KB:. ![custom kb](https://user-images.githubusercontent.com/54917098/128267691-6f4b1eac-c96a-491b-8fdb-57b4928466e8.png). This value affected the `batch_neighbors` and `batch_distances` when it was called in line 329:. ```. batch_neighbors, batch_distances = self.nmslib_knn_with_zero_vectors(tfidfs, k). ```. Using the custom KB, https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L251. would give the value of 1, so that made the if statement below True since `vectors` was `tfidf` that was passed to the function: ` self.nmslib_knn_with_zero_vectors(tfidfs, k)`. https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L263-L264. and made them return empty arrays of neighbors and distance. Is the vectorizer the problem then? or am I missing something? UPDATE:. I played around with the vectorizer and I found out that the stop_words_ in the vectorizer contains some of the entities in my KB. ```. import joblib. vectorizer = joblib.load(""output/tfidf_vectorizer.joblib""). print(vectorizer.stop_words_). ```. and this gave me:. ```. {'g/m', 'ml/', ' g ', 'pm ', 'ds ', ' mi', 'm/s', '*m ', 'min', 'inc', ' 2/', '/m ', ' m/', '/m2', ' m', 'ams', 'in ', 'ms/', 'gra', ' mm', '/s ', ' gr', 'l/m', 'mse', 's/m', ' m ', 'ec ', 'm2/', 'hes', 'pou', ' g/', ' ml', ' in', 'nds', ' cm', ' bp', 'mmh', 'ram', 'che', ' ms', '/mi', 'nch', ' hg', 'kg ', 'cm/', 'ml ', 'kg/', 'm2 ', 'm/m', ' po', '2/m'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:294,safety,test,test,294,"the vectorizer construction (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L415) has a `min_df` of 10, meaning that any ngram has to be in at least 10 docs to be included in the vectorizer. you can change this if you want to test on smaller data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:294,testability,test,test,294,"the vectorizer construction (https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/candidate_generation.py#L415) has a `min_df` of 10, meaning that any ngram has to be in at least 10 docs to be included in the vectorizer. you can change this if you want to test on smaller data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/385:35,deployability,instal,install,35,What is the issue? Are you able to install spacy on your machine?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/385
https://github.com/allenai/scispacy/issues/387:67,integrability,pub,pubmed,67,"those vectors are static. the word vectors are word2vec trained on pubmed abstracts, and then spacy does the aggregation into the doc tensor (I think it is just an average, although not sure). I'm not sure how easy it would be to null those, it might be straightforward.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:74,integrability,abstract,abstracts,74,"those vectors are static. the word vectors are word2vec trained on pubmed abstracts, and then spacy does the aggregation into the doc tensor (I think it is just an average, although not sure). I'm not sure how easy it would be to null those, it might be straightforward.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:74,modifiability,abstract,abstracts,74,"those vectors are static. the word vectors are word2vec trained on pubmed abstracts, and then spacy does the aggregation into the doc tensor (I think it is just an average, although not sure). I'm not sure how easy it would be to null those, it might be straightforward.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:100,reliability,doe,does,100,"those vectors are static. the word vectors are word2vec trained on pubmed abstracts, and then spacy does the aggregation into the doc tensor (I think it is just an average, although not sure). I'm not sure how easy it would be to null those, it might be straightforward.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:824,deployability,contain,contain,824,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:893,deployability,contain,contains,893,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:910,deployability,depend,dependent,910,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:150,energy efficiency,load,load,150,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1012,energy efficiency,model,model,1012,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:910,integrability,depend,dependent,910,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:957,integrability,transform,transformers,957,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:992,integrability,transform,transformer,992,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:957,interoperability,transform,transformers,957,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:992,interoperability,transform,transformer,992,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:910,modifiability,depend,dependent,910,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:150,performance,load,load,150,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:910,safety,depend,dependent,910,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:45,security,token,token,45,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:78,security,token,token,78,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:638,security,ident,identical,638,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1012,security,model,model,1012,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:902,testability,context,context-dependent,902,"Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? Because if I do:. ``` python. import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""). print(sent_1[1].tensor[:10]). print(sent_2[1].tensor[:10]). ```. I get:. ```. [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. ```. i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:22,deployability,pipelin,pipeline,22,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:65,deployability,pipelin,pipeline,65,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:130,deployability,pipelin,pipeline,130,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:202,deployability,pipelin,pipeline,202,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:362,deployability,pipelin,pipeline,362,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:382,deployability,Depend,DependencyParser,382,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:436,deployability,pipelin,pipeline,436,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:22,integrability,pipelin,pipeline,22,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:65,integrability,pipelin,pipeline,65,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:130,integrability,pipelin,pipeline,130,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:202,integrability,pipelin,pipeline,202,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:362,integrability,pipelin,pipeline,362,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:382,integrability,Depend,DependencyParser,382,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:436,integrability,pipelin,pipeline,436,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:525,integrability,transform,transformers,525,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:525,interoperability,transform,transformers,525,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:382,modifiability,Depend,DependencyParser,382,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:382,safety,Depend,DependencyParser,382,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:382,testability,Depend,DependencyParser,382,"( for reference, `nlp.pipeline` gives:. ```. [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fe728e0fc20>),. ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fe843db99a0>),. ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler at 0x7fe728d92f80>),. ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fe728dcb1c0>),. ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fe729615940>),. ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fe729615be0>)]. ```. So I'm positive that there's no transformers at work here)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:885,deployability,contain,contain,885,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:954,deployability,contain,contains,954,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:971,deployability,depend,dependent,971,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:167,energy efficiency,load,load,167,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1073,energy efficiency,model,model,1073,"s (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). print(sent_2[0].ve",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1240,energy efficiency,load,load,1240,"I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.00988646 -0.279009 0.083598 0.412393 0.233853 0.58461. 0.294148 -0.0245503 -0.0427367 0.14",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1937,energy efficiency,load,load,1937,".18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.00988646 -0.279009 0.083598 0.412393 0.233853 0.58461. 0.294148 -0.0245503 -0.0427367 0.144446 ]. [-0.00988646 -0.279009 0.083598 0.412393 0.233853 0.58461. 0.294148 -0.0245503 -0.0427367 0.144446 ]. [ 0.3273737 -0.04076735 0.1153962 -1.0918155 0.1258091 -0.8073256. 1.2124916 0.21230562 -0.7984005 -0.28116632]. [ 0.11558911 0.02216281 0.3639214 -1.09512 -0.05392472 -0.9066583. 1.360736 0.34528118 -0.72069037 -0.18247896]```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:971,integrability,depend,dependent,971,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1018,integrability,transform,transformers,1018,"alking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1053,integrability,transform,transformer,1053,"`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1018,interoperability,transform,transformers,1018,"alking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes""",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1053,interoperability,transform,transformer,1053,"`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:971,modifiability,depend,dependent,971,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:167,performance,load,load,167,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1240,performance,load,load,1240,"I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.00988646 -0.279009 0.083598 0.412393 0.233853 0.58461. 0.294148 -0.0245503 -0.0427367 0.14",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1937,performance,load,load,1937,".18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.00988646 -0.279009 0.083598 0.412393 0.233853 0.58461. 0.294148 -0.0245503 -0.0427367 0.144446 ]. [-0.00988646 -0.279009 0.083598 0.412393 0.233853 0.58461. 0.294148 -0.0245503 -0.0427367 0.144446 ]. [ 0.3273737 -0.04076735 0.1153962 -1.0918155 0.1258091 -0.8073256. 1.2124916 0.21230562 -0.7984005 -0.28116632]. [ 0.11558911 0.02216281 0.3639214 -1.09512 -0.05392472 -0.9066583. 1.360736 0.34528118 -0.72069037 -0.18247896]```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:971,safety,depend,dependent,971,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:47,security,token,token,47,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:80,security,token,token,80,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:693,security,ident,identical,693,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:1073,security,model,model,1073,"s (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2 = nlp(""I hate potatoes"") . print(sent_1[0].vector[:10]). print(sent_2[0].ve",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:963,testability,context,context-dependent,963,"> Are you sure you are talking about tensors (`token.tensor`) and not vectors (`token.vector`)? > . > Because if I do:. > . > ```python. > import spacy. > nlp = spacy.load(""en_core_sci_md""). > sent_1 = nlp(""I love potatoes""). > sent_2 = nlp(""I hate potatoes""). > . > print(sent_1[1].tensor[:10]). > print(sent_2[1].tensor[:10]). > ```. > . > I get:. > . > ```. > [ 0.9780569 -0.85239476 0.05534324 -0.536315 -0.7386089 1.3136992 0.15240714 0.11695394 -0.32724175 -0.28156167]. > [ 0.7323813 -0.56237364 0.2564265 -0.5926044 -1.2961963 1.0635824 -0.33420312 1.120001 -0.43774125 0.18489192]. > ```. > . > i.e. - two different tensors for the same word. Doing the same with `.vector` results in identical vectors, as expected. > . > According to this writeup: https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html, `.vector` should contain static information from word2vec embeddings, while `.tensor` contains context-dependent information that normally comes from transformers, but since there's no transformer in this model, I don't know what they represent? You are comparing tensors for ""love"" vs ""hate"", it's more weird that the vectors are the same. . ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""love""). sent_2 = nlp(""hate"") . print(sent_1[0].vector[:10]). print(sent_2[0].vector[:10]). print(sent_1[0].tensor[:10]). print(sent_2[0].tensor[:10]). [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [-0.11821 -0.0708988 -0.30936 -0.105225 0.415823 0.00097378. 0.300131 0.00324844 0.207479 0.125942 ]. [ 1.8751609 -1.6736817 -0.29442406 -0.81565964 -0.00817816 0.06152236. 0.641755 -0.8051793 -1.1027172 0.06668803]. [ 1.3759801 -0.8531469 0.7494343 -0.7312323 -0.6730688 -0.34606454. 0.4301284 0.01695809 -1.0967836 -0.01745698]```. Having said that this is what I'm seeing:. ```import spacy. nlp = spacy.load(""en_core_sci_md""). sent_1 = nlp(""I love potatoes""). sent_2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:358,interoperability,specif,specific,358,"Ah, sorry. Yes, I was talking about `token.vector`. I'm pretty sure the `Doc.tensor` comes from the `tok2vec` pipe. My guess is that you don't need to serialize the tensor (unless you want to use it for doc similarity or something), and can exclude it like so `doc.to_bytes(exclude=[""text"", ""tensor""])`. That being said, there shouldn't be anything scispacy specific about this, so if you have more questions, it might be worth just asking on the spacy github directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:37,security,token,token,37,"Ah, sorry. Yes, I was talking about `token.vector`. I'm pretty sure the `Doc.tensor` comes from the `tok2vec` pipe. My guess is that you don't need to serialize the tensor (unless you want to use it for doc similarity or something), and can exclude it like so `doc.to_bytes(exclude=[""text"", ""tensor""])`. That being said, there shouldn't be anything scispacy specific about this, so if you have more questions, it might be worth just asking on the spacy github directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:32,deployability,api,api,32,For reference: https://spacy.io/api/tok2vec,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:32,integrability,api,api,32,For reference: https://spacy.io/api/tok2vec,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:32,interoperability,api,api,32,For reference: https://spacy.io/api/tok2vec,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:64,integrability,pub,pubmed,64,"Just to be clear, the ""vectors"" are word2vec vectors trained on pubmed abstracts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:71,integrability,abstract,abstracts,71,"Just to be clear, the ""vectors"" are word2vec vectors trained on pubmed abstracts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:71,modifiability,abstract,abstracts,71,"Just to be clear, the ""vectors"" are word2vec vectors trained on pubmed abstracts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:11,usability,clear,clear,11,"Just to be clear, the ""vectors"" are word2vec vectors trained on pubmed abstracts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/388:103,performance,time,time,103,"Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:253,safety,test,tests,253,"Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:253,testability,test,tests,253,"Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:105,performance,time,time,105,"> Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it! We have both a requirement and capacity to work on this function, but may need some guidance on the spec. -Kate B., CDH (Databricks)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:255,safety,test,tests,255,"> Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it! We have both a requirement and capacity to work on this function, but may need some guidance on the spec. -Kate B., CDH (Databricks)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:255,testability,test,tests,255,"> Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it! We have both a requirement and capacity to work on this function, but may need some guidance on the spec. -Kate B., CDH (Databricks)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:353,usability,guidanc,guidance,353,"> Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it! We have both a requirement and capacity to work on this function, but may need some guidance on the spec. -Kate B., CDH (Databricks)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:116,usability,guidanc,guidance,116,"Hi @ulc0 I think the original issue is a reasonable description! Are there any particular areas you are looking for guidance on? If you'd like to propose a design, I'd be happy to take a look here.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/389:167,deployability,configurat,configuration,167,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:258,deployability,depend,dependency,258,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:167,integrability,configur,configuration,167,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:258,integrability,depend,dependency,258,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:167,modifiability,configur,configuration,167,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:258,modifiability,depend,dependency,258,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:258,safety,depend,dependency,258,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:167,security,configur,configuration,167,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:23,testability,plan,plans,23,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:258,testability,depend,dependency,258,"Hi, we do not have any plans to expand to other languages at this point. The entire training process is run using the spacy projects (https://spacy.io/usage/projects) configuration system. So the hard part of all of this is getting datasets for POS tagging, dependency parsing, and NER in whatever language you are interested in. If you have those datasets, you should be able to replace our training files with your training files and mostly just run our `project.yml`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/389:58,integrability,sub,subject,58,@fatemerhmi Hello! Did you happen to find anything on the subject?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/389
https://github.com/allenai/scispacy/issues/390:18,availability,slo,slow,18,the first time is slow because it needs to download a bunch of files. it should be faster all subsequent times.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/390
https://github.com/allenai/scispacy/issues/390:43,availability,down,download,43,the first time is slow because it needs to download a bunch of files. it should be faster all subsequent times.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/390
https://github.com/allenai/scispacy/issues/390:94,integrability,sub,subsequent,94,the first time is slow because it needs to download a bunch of files. it should be faster all subsequent times.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/390
https://github.com/allenai/scispacy/issues/390:10,performance,time,time,10,the first time is slow because it needs to download a bunch of files. it should be faster all subsequent times.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/390
https://github.com/allenai/scispacy/issues/390:105,performance,time,times,105,the first time is slow because it needs to download a bunch of files. it should be faster all subsequent times.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/390
https://github.com/allenai/scispacy/issues/390:18,reliability,slo,slow,18,the first time is slow because it needs to download a bunch of files. it should be faster all subsequent times.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/390
https://github.com/allenai/scispacy/issues/391:22,deployability,upgrad,upgrade,22,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:70,deployability,instal,install,70,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:83,deployability,version,version,83,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:115,deployability,instal,installing,115,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:170,deployability,upgrad,upgrading,170,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:83,integrability,version,version,83,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:22,modifiability,upgrad,upgrade,22,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:83,modifiability,version,version,83,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:170,modifiability,upgrad,upgrading,170,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:14,performance,time,time,14,"I haven't had time to upgrade to 3.1.x yet, but you can probably just install that version of spacy yourself after installing scispacy, given https://spacy.io/usage/v3-1#upgrading",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:280,availability,error,errors,280,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:290,availability,degrad,degraded,290,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:312,availability,down,download,312,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:428,availability,avail,available,428,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:252,deployability,version,version,252,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:398,deployability,version,version,398,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:438,deployability,updat,updates,438,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:143,energy efficiency,Model,Model,143,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:244,energy efficiency,current,current,244,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:340,energy efficiency,model,model,340,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:369,energy efficiency,model,model,369,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:384,energy efficiency,current,current,384,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:252,integrability,version,version,252,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:398,integrability,version,version,398,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:224,interoperability,compatib,compatible,224,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:329,interoperability,compatib,compatible,329,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:95,modifiability,pac,packages,95,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:252,modifiability,version,version,252,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:398,modifiability,version,version,398,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:280,performance,error,errors,280,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:299,performance,perform,performance,299,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:290,reliability,degrad,degraded,290,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:428,reliability,availab,available,428,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:280,safety,error,errors,280,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:428,safety,avail,available,428,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:438,safety,updat,updates,438,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:468,safety,valid,validate,468,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:143,security,Model,Model,143,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:340,security,model,model,340,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:369,security,model,model,369,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:428,security,availab,available,428,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:438,security,updat,updates,438,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:468,security,validat,validate,468,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:123,usability,User,UserWarning,123,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:280,usability,error,errors,280,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:299,usability,perform,performance,299,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:362,usability,custom,custom,362,"The libraries still work, but with a warning:. `/opt/anaconda3/envs/prodigy/lib/python3.8/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_ner_jnlpba_md' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:80,deployability,version,versions,80,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:178,deployability,version,versions,178,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:209,deployability,instal,install,209,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:230,deployability,version,version,230,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:263,deployability,instal,install,263,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:306,deployability,version,version,306,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:344,deployability,version,version,344,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:412,deployability,version,version,412,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:495,deployability,version,version,495,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:518,deployability,upgrad,upgrade,518,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:188,energy efficiency,Current,Currently,188,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:292,energy efficiency,model,models,292,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:80,integrability,version,versions,80,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:178,integrability,version,versions,178,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:230,integrability,version,version,230,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:306,integrability,version,version,306,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:344,integrability,version,version,344,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:412,integrability,version,version,412,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:495,integrability,version,version,495,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:450,interoperability,conflict,conflicts,450,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:80,modifiability,version,versions,80,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:178,modifiability,version,versions,178,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:230,modifiability,version,version,230,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:306,modifiability,version,version,306,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:344,modifiability,version,version,344,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:412,modifiability,version,version,412,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:495,modifiability,version,version,495,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:518,modifiability,upgrad,upgrade,518,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:68,reliability,ingress,ingress,68,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:292,security,model,models,292,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:51,usability,support,support,51,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:372,usability,support,support,372,"Hi,. I'm sort in the same boat. I'll be asking our support staff to ingress new versions of Prodigy, spacy and scispacy. But I want to make sure they are the _latest compatible_ versions. Currently, if I just install spacy, I get version 3.2.1. But as soon as I ""install"" one of the scispacy models, spacy version goes to 3.0.7. Prodigy latest version is 1.11.7 which has support for Spacy 3.2. . Whats the best version of all that works without any conflicts? Or is it OK to use all the latest version and ignore the upgrade warning?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:117,availability,degrad,degradation,117,"As far as I know it should be fine to use with the warning, but I haven't tried it to check for possible performance degradation",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:105,performance,perform,performance,105,"As far as I know it should be fine to use with the warning, but I haven't tried it to check for possible performance degradation",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:117,reliability,degrad,degradation,117,"As far as I know it should be fine to use with the warning, but I haven't tried it to check for possible performance degradation",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:105,usability,perform,performance,105,"As far as I know it should be fine to use with the warning, but I haven't tried it to check for possible performance degradation",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:4,deployability,upgrad,upgrade,4,"The upgrade to the latest spacy is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:123,deployability,upgrad,upgrade,123,"The upgrade to the latest spacy is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:4,modifiability,upgrad,upgrade,4,"The upgrade to the latest spacy is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:123,modifiability,upgrad,upgrade,123,"The upgrade to the latest spacy is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:41,safety,except,except,41,"The upgrade to the latest spacy is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:187,deployability,version,version,187,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:187,integrability,version,version,187,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:203,interoperability,compatib,compatible,203,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:275,interoperability,incompatib,incompatibility,275,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:187,modifiability,version,version,187,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:306,safety,safe,safely,306,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:44,usability,support,support,44,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:138,usability,support,support,138,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:173,usability,confirm,confirm,173,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/391:260,usability,User,UserWarning,260,"Hi, there. Given scispacy v0.5.0 and v0.4.0 support spacy 3.2 and 3.0 respectively, am I right in thinking there is seemingly no official support on spacy 3.1? Could anyone confirm which version is more compatible with spacy 3.1 and whether the aforementioned UserWarning on incompatibility can be ignored safely?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/391
https://github.com/allenai/scispacy/issues/392:124,deployability,pipelin,pipeline,124,There shouldn't be anything special about scispacy for this. You can follow spacy's documentation for how to add pipes to a pipeline (https://spacy.io/api/language#add_pipe).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/392
https://github.com/allenai/scispacy/issues/392:151,deployability,api,api,151,There shouldn't be anything special about scispacy for this. You can follow spacy's documentation for how to add pipes to a pipeline (https://spacy.io/api/language#add_pipe).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/392
https://github.com/allenai/scispacy/issues/392:124,integrability,pipelin,pipeline,124,There shouldn't be anything special about scispacy for this. You can follow spacy's documentation for how to add pipes to a pipeline (https://spacy.io/api/language#add_pipe).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/392
https://github.com/allenai/scispacy/issues/392:151,integrability,api,api,151,There shouldn't be anything special about scispacy for this. You can follow spacy's documentation for how to add pipes to a pipeline (https://spacy.io/api/language#add_pipe).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/392
https://github.com/allenai/scispacy/issues/392:151,interoperability,api,api,151,There shouldn't be anything special about scispacy for this. You can follow spacy's documentation for how to add pipes to a pipeline (https://spacy.io/api/language#add_pipe).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/392
https://github.com/allenai/scispacy/issues/392:84,usability,document,documentation,84,There shouldn't be anything special about scispacy for this. You can follow spacy's documentation for how to add pipes to a pipeline (https://spacy.io/api/language#add_pipe).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/392
https://github.com/allenai/scispacy/issues/394:185,availability,Robust,Robust-Models-for-Biomedical-Neumann-King,185,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:192,energy efficiency,Model,Models-for-Biomedical-Neumann-King,192,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:577,energy efficiency,measur,measure,577,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:681,energy efficiency,measur,measure,681,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:1135,energy efficiency,profil,profile,1135,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:138,interoperability,semant,semanticscholar,138,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:1135,performance,profil,profile,1135,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:185,reliability,Robust,Robust-Models-for-Biomedical-Neumann-King,185,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:185,safety,Robust,Robust-Models-for-Biomedical-Neumann-King,185,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:1114,safety,reme,remember,1114,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:192,security,Model,Models-for-Biomedical-Neumann-King,192,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:805,testability,context,context,805,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:13,usability,document,documentation,13,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:467,usability,help,help,467,"1. Yes, this documentation is out of date, thank you! 2. We do have a paper, although it may or may not answer your question (https://www.semanticscholar.org/paper/ScispaCy%3A-Fast-and-Robust-Models-for-Biomedical-Neumann-King/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29). Figure 3 shows the recall curve as we increase the number of candidates with our candidate generator. We did not dive into what remains in that last bit of recall and whether word embeddings could help. The paper we compare against (https://aclanthology.org/P18-1010.pdf) uses a different string similarity measure for the candidate generation. Character trigram similarity is a pretty common string similarity measure, which is why we choose to use it. The intention is really that a further entity linker that takes into account the context of the mention rather than just the mention string itself should be used. If I were going to experiment, I would probably start by experimenting with reranking the candidates we generate to increase precision, rather than improving the candidate generator to increase recall. 3. Sorry, I don't really remember the compute profile of creating the index. We use nmslib (https://github.com/nmslib/nmslib) though, so you might be able to find some more information there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:34,deployability,version,versions,34,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:17,integrability,coupl,couple,17,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:34,integrability,version,versions,34,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:58,interoperability,plug,plugged,58,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:310,interoperability,ontolog,ontology,310,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:380,interoperability,ontolog,ontologies,380,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:17,modifiability,coupl,couple,17,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:34,modifiability,version,versions,34,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:130,safety,except,exception,130,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:201,security,sign,significant,201,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/394:17,testability,coupl,couple,17,"I have trained a couple of my own versions of linkers and plugged into scispacy:. 2) the 3-char works surprisingly well, with one exception (on my usecases), because ""ing"" is 3 chars, it makes quite a significant difference between ""bleed"" and ""bleeding"" etc. . 3) I created an index on an i7 processor for an ontology with the size of ~120k, and it took ~1h total, so for larger ontologies (especially if you decide to go with word vectors which are not sparse) you definitely need something beefier.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/394
https://github.com/allenai/scispacy/issues/395:31,energy efficiency,model,models,31,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:41,energy efficiency,CPU,CPU,41,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:174,energy efficiency,model,model,174,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:267,interoperability,share,shareable,267,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:240,modifiability,interm,intermediate,240,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:41,performance,CPU,CPU,41,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:31,security,model,models,31,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:174,security,model,model,174,"I've actually only trained the models on CPU, but it was always on very large machines. I would expect training on smaller rmachines to work though. I think I've trained the model on my laptop before. I don't actually have any of the saved intermediate output. It is shareable, I just don't have it. Sorry about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:27,energy efficiency,gpu,gpu,27,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:68,energy efficiency,model,model,68,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:116,energy efficiency,gpu,gpu,116,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:163,energy efficiency,reduc,reduce,163,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:186,energy efficiency,reduc,reducing,186,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:199,integrability,batch,batch,199,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:27,performance,gpu,gpu,27,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:116,performance,gpu,gpu,116,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:133,performance,memor,memory,133,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:170,performance,memor,memory,170,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:199,performance,batch,batch,199,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:68,security,model,model,68,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:5,usability,command,command,5,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:133,usability,memor,memory,133,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/395:170,usability,memor,memory,170,"that command you ran has a gpu id because someone else trained that model not me, but they probably trained it on a gpu with 48gb of memory. you should be able to reduce memory usage by reducing the batch size in the config file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/395
https://github.com/allenai/scispacy/issues/396:5,deployability,instal,installed,5,"Just installed scispacy 0.4.0 on python 3.10 and I'm receiving the same warning while using the included linker. `nlp.add_pipe(""scispacy_linker"", config={""linker_name"": ""rxnorm""})`. ... so it's not just a custom KB issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/396
https://github.com/allenai/scispacy/issues/396:205,usability,custom,custom,205,"Just installed scispacy 0.4.0 on python 3.10 and I'm receiving the same warning while using the included linker. `nlp.add_pipe(""scispacy_linker"", config={""linker_name"": ""rxnorm""})`. ... so it's not just a custom KB issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/396
https://github.com/allenai/scispacy/issues/396:36,deployability,instal,installation,36,"The first warning is just about the installation of nmslib and I believe is fine to ignote, but you can try to install nmslib as it says if you want. The second warning should be fixed, but I don't know if/when it will be, and I don't think it should cause any issues other than printing the warning.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/396
https://github.com/allenai/scispacy/issues/396:111,deployability,instal,install,111,"The first warning is just about the installation of nmslib and I believe is fine to ignote, but you can try to install nmslib as it says if you want. The second warning should be fixed, but I don't know if/when it will be, and I don't think it should cause any issues other than printing the warning.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/396
https://github.com/allenai/scispacy/issues/397:6,integrability,repositor,repository,6,"Wrong repository. If possible, please delete this issue",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/397
https://github.com/allenai/scispacy/issues/397:6,interoperability,repositor,repository,6,"Wrong repository. If possible, please delete this issue",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/397
https://github.com/allenai/scispacy/issues/398:7,deployability,instal,install,7,Try to install nmslib separately from scispacy https://github.com/nmslib/nmslib,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:85,availability,error,error,85,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:134,availability,error,error,134,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:141,availability,error,error,141,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:171,availability,error,error,171,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:824,availability,error,error,824,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1010,availability,error,error,1010,"lling nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1084,availability,ERROR,ERROR,1084,"ror. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc',",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1263,availability,error,error,1263,"output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": ht",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1270,availability,error,error,1270,". Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://v",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1300,availability,error,error,1300,"2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2166,availability,error,error,2166," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2352,availability,error,error,2352," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2426,availability,error,error,2426," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2448,availability,failur,failure,2448," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2471,availability,error,error,2471," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2624,availability,failur,failure,2624," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:8,deployability,instal,installing,8,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:49,deployability,instal,install,49,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:93,deployability,Build,Building,93,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:275,deployability,Depend,Dependence,275,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:567,deployability,version,versions,567,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:681,deployability,build,build,681,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:795,deployability,build,building,795,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:908,deployability,Build,Build,908,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:968,deployability,build,build-tools,968,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1091,deployability,Fail,Failed,1091,"uilding wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/open",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1098,deployability,build,building,1098,"wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1160,deployability,Fail,Failed,1160,"ed-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extensio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1170,deployability,build,build,1170,"ror.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1184,deployability,Instal,Installing,1184,"tup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1240,deployability,instal,install,1240,"e: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsof",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1326,deployability,instal,install,1326,".10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1413,deployability,Depend,Dependence,1413,"\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1705,deployability,version,versions,1705," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1798,deployability,instal,install,1798," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1871,deployability,instal,install,1871," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1925,deployability,instal,install,1925," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1952,deployability,build,build,1952," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2023,deployability,build,build,2023," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2137,deployability,build,building,2137," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2250,deployability,Build,Build,2250," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2310,deployability,build,build-tools,2310," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2440,deployability,instal,install-failure,2440," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2493,deployability,instal,install,2493," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2624,deployability,fail,failure,2624," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:148,integrability,sub,subprocess-exited-with-error,148,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:275,integrability,Depend,Dependence,275,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:567,integrability,version,versions,567,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1034,integrability,sub,subprocess,1034,"t doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running b",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1277,integrability,sub,subprocess-exited-with-error,1277," ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.micros",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1413,integrability,Depend,Dependence,1413,"\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1705,integrability,version,versions,1705," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2376,integrability,sub,subprocess,2376," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1976,interoperability,standard,standards-based,1976," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:275,modifiability,Depend,Dependence,275,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:446,modifiability,pac,packages,446,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:567,modifiability,version,versions,567,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:813,modifiability,extens,extension,813,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1205,modifiability,pac,packages,1205,"id not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1413,modifiability,Depend,Dependence,1413,"\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1584,modifiability,pac,packages,1584,"the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1705,modifiability,version,versions,1705," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1843,modifiability,pac,packages,1843," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2155,modifiability,extens,extension,2155," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2501,modifiability,pac,package,2501," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2554,modifiability,pac,package,2554," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:85,performance,error,error,85,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:134,performance,error,error,134,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:141,performance,error,error,141,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:171,performance,error,error,171,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:824,performance,error,error,824,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1010,performance,error,error,1010,"lling nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1084,performance,ERROR,ERROR,1084,"ror. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc',",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1263,performance,error,error,1263,"output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": ht",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1270,performance,error,error,1270,". Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://v",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1300,performance,error,error,1300,"2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2166,performance,error,error,2166," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2352,performance,error,error,2352," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2426,performance,error,error,2426," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2448,performance,failur,failure,2448," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2471,performance,error,error,2471," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2624,performance,failur,failure,2624," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:41,reliability,doe,doesn,41,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1091,reliability,Fail,Failed,1091,"uilding wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/open",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1160,reliability,Fail,Failed,1160,"ed-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extensio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2448,reliability,fail,failure,2448," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2624,reliability,fail,failure,2624," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:85,safety,error,error,85,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:134,safety,error,error,134,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:141,safety,error,error,141,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:171,safety,error,error,171,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:275,safety,Depend,Dependence,275,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:824,safety,error,error,824,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1010,safety,error,error,1010,"lling nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1084,safety,ERROR,ERROR,1084,"ror. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc',",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1263,safety,error,error,1263,"output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": ht",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1270,safety,error,error,1270,". Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://v",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1300,safety,error,error,1300,"2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1413,safety,Depend,Dependence,1413,"\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2166,safety,error,error,2166," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2352,safety,error,error,2352," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2426,safety,error,error,2426," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2471,safety,error,error,2471," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:275,testability,Depend,Dependence,275,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1413,testability,Depend,Dependence,1413,"\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:85,usability,error,error,85,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:134,usability,error,error,134,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:141,usability,error,error,141,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:171,usability,error,error,171,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:413,usability,User,Users,413,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:479,usability,User,UserWarning,479,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:547,usability,support,supported,547,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:824,usability,error,error,824,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:841,usability,Visual,Visual,841,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:914,usability,Tool,Tools,914,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:930,usability,visual,visualstudio,930,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:957,usability,visual,visual-cpp-build-tools,957,"I tried installing nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1010,usability,error,error,1010,"lling nmslib separately, it doesn't install as well and gives following error. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1084,usability,ERROR,ERROR,1084,"ror. `Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error.  python setup.py bdist_wheel did not run successfully.  exit code: 1. > [9 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc',",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1263,usability,error,error,1263,"output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": ht",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1270,usability,error,error,1270,". Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://v",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1300,usability,error,error,1300,"2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1551,usability,User,Users,1551,"ed in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1617,usability,User,UserWarning,1617,"le' instead. warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1685,usability,support,supported,1685," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1810,usability,User,Users,1810," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1863,usability,command,command,1863," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:1992,usability,tool,tools,1992," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2166,usability,error,error,2166," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2183,usability,Visual,Visual,2183," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2256,usability,Tool,Tools,2256," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2272,usability,visual,visualstudio,2272," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2299,usability,visual,visual-cpp-build-tools,2299," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2352,usability,error,error,2352," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2426,usability,error,error,2426," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2471,usability,error,error,2471," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/398:2588,usability,hint,hint,2588," warnings.warn(. running bdist_wheel. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Failed to build nmslib. Installing collected packages: nmslib. Running setup.py install for nmslib ... error. error: subprocess-exited-with-error.  Running setup.py install for nmslib did not run successfully.  exit code: 1. > [11 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\dist.py:739: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. running install. C:\Users\Yantra\biomedical\lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools. warnings.warn(. running build. running build_ext. Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\""2.1.1\\""']. building 'nmslib' extension. error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure.  Encountered error while trying to install package. > nmslib. note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. `.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/398
https://github.com/allenai/scispacy/issues/400:23,deployability,instal,install,23,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:90,deployability,version,version,90,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:118,deployability,upgrad,upgrading,118,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:44,energy efficiency,model,models,44,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:90,integrability,version,version,90,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:90,modifiability,version,version,90,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:118,modifiability,upgrad,upgrading,118,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:169,performance,time,timeline,169,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:44,security,model,models,44,"if you can force it to install anyways, the models should work fine with the latest spacy version. I'll try to get to upgrading it sometime soon, but can't give you any timeline. sorry about that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:4,deployability,upgrad,upgrade,4,"The upgrade is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:103,deployability,upgrad,upgrade,103,"The upgrade is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:4,modifiability,upgrad,upgrade,4,"The upgrade is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:103,modifiability,upgrad,upgrade,103,"The upgrade is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/400:21,safety,except,except,21,"The upgrade is done, except that I am having some issues with pypi. You can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/400
https://github.com/allenai/scispacy/issues/401:75,energy efficiency,model,model,75,as far as I know you can just replace that `name` with any token embedding model in huggingface.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/401
https://github.com/allenai/scispacy/issues/401:59,security,token,token,59,as far as I know you can just replace that `name` with any token embedding model in huggingface.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/401
https://github.com/allenai/scispacy/issues/401:75,security,model,model,75,as far as I know you can just replace that `name` with any token embedding model in huggingface.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/401
https://github.com/allenai/scispacy/issues/402:83,deployability,version,version,83,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:166,energy efficiency,load,load,166,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:83,integrability,version,version,83,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:83,modifiability,version,version,83,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:158,performance,time,time,158,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:166,performance,load,load,166,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:36,safety,test,test,36,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:77,safety,test,test,77,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:36,testability,test,test,36,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:77,testability,test,test,77,"Do you need the full linker for the test? If not you could always make up a ""test"" version of it with a much smaller index, since that is what takes the most time to load.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:84,energy efficiency,model,model,84,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:135,energy efficiency,load,loading,135,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:195,energy efficiency,load,loading,195,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:135,performance,load,loading,135,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:152,performance,cach,cache,152,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:195,performance,load,loading,195,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:64,safety,test,test,64,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:84,security,model,model,84,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:64,testability,test,test,64,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:178,testability,simpl,simply,178,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:178,usability,simpl,simply,178,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:19,energy efficiency,load,loading,19,I think it is just loading weights. The UMLS index is quite beefy from what I remember (~2Gb or sth),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:19,performance,load,loading,19,I think it is just loading weights. The UMLS index is quite beefy from what I remember (~2Gb or sth),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:78,safety,reme,remember,78,I think it is just loading weights. The UMLS index is quite beefy from what I remember (~2Gb or sth),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:25,availability,slo,slow,25,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:66,availability,down,download,66,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:152,integrability,sub,subsequent,152,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:40,performance,time,time,40,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:140,performance,cach,cached,140,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:25,reliability,slo,slow,25,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:210,usability,experien,experiencing,210,"It should only be really slow the first time, because it needs to download some large files, including the umls index. These files are then cached, and subsequent calls should be fast. Is this not what you are experiencing?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:46,energy efficiency,load,load,46,"I think for me it's usually ~15-20 seconds to load it in, but this is on a laptop.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:46,performance,load,load,46,"I think for me it's usually ~15-20 seconds to load it in, but this is on a laptop.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:66,energy efficiency,load,loading,66,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:147,energy efficiency,profil,profiling,147,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:192,energy efficiency,profil,profiler,192,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:479,energy efficiency,load,load,479,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:521,energy efficiency,current,current,521,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:727,energy efficiency,current,current,727,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:238,modifiability,deco,decoding,238,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:388,modifiability,deco,decoder,388,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:36,performance,cach,cached,36,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:66,performance,load,loading,66,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:147,performance,profil,profiling,147,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:192,performance,profil,profiler,192,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:224,performance,time,time,224,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:479,performance,load,load,479,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:546,performance,memor,memory,546,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:53,testability,simpl,simply,53,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:24,usability,effectiv,effectively,24,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:53,usability,simpl,simply,53,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:455,usability,efficien,efficient,455,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:546,usability,memor,memory,546,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. . @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:. ```. ncalls tottime percall cumtime percall filename:lineno(function). 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode). ```. I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:. 1. `pyarrow` to store the alias list . 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)? Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:148,security,barrier,barrier,148,"@vlievin are you aware of how aliasing is supported in faiss? . I always wanted to take a look and try at adding this to scispacy, but that was the barrier I could not find an answer to",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/402:42,usability,support,supported,42,"@vlievin are you aware of how aliasing is supported in faiss? . I always wanted to take a look and try at adding this to scispacy, but that was the barrier I could not find an answer to",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402
https://github.com/allenai/scispacy/issues/403:5,reliability,doe,does,5,This does not appear to be related to scispacy.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/403
https://github.com/allenai/scispacy/issues/404:36,deployability,version,version,36,This was probably fixed in the next version of scispacy. See #298,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/404
https://github.com/allenai/scispacy/issues/404:36,integrability,version,version,36,This was probably fixed in the next version of scispacy. See #298,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/404
https://github.com/allenai/scispacy/issues/404:36,modifiability,version,version,36,This was probably fixed in the next version of scispacy. See #298,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/404
https://github.com/allenai/scispacy/issues/405:42,availability,state,statement,42,"This is the expected behavior, the import statement is necessary.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/405
https://github.com/allenai/scispacy/issues/405:42,integrability,state,statement,42,"This is the expected behavior, the import statement is necessary.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/405
https://github.com/allenai/scispacy/issues/405:21,usability,behavi,behavior,21,"This is the expected behavior, the import statement is necessary.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/405
https://github.com/allenai/scispacy/issues/406:57,safety,detect,detected,57,This is working as intended. An abbreviation can only be detected if both the long form and short form exist in the text.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:57,security,detect,detected,57,This is working as intended. An abbreviation can only be detected if both the long form and short form exist in the text.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:101,deployability,automat,automatically,101,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:243,integrability,compon,component,243,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:243,interoperability,compon,component,243,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:243,modifiability,compon,component,243,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:101,testability,automat,automatically,101,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:207,testability,understand,understandable,207,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/406:276,usability,behavi,behavior,276,@dakinggg We were under the impression that given an abbreviation it would provide the expanded form automatically and this is what we are looking to achieve in order to normalize that data and make it more understandable. Is there some other component that would enable such behavior?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/406
https://github.com/allenai/scispacy/issues/410:122,deployability,Depend,Depending,122,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:181,deployability,patch,patch,181,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:122,integrability,Depend,Depending,122,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:122,modifiability,Depend,Depending,122,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:122,safety,Depend,Depending,122,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:181,safety,patch,patch,181,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:181,security,patch,patch,181,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:101,testability,simpl,simple,101,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:122,testability,Depend,Depending,122,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/issues/410:101,usability,simpl,simple,101,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410
https://github.com/allenai/scispacy/pull/411:66,availability,slo,slowly,66,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:142,energy efficiency,model,models,142,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:219,energy efficiency,model,model,219,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:314,energy efficiency,model,model,314,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:66,reliability,slo,slowly,66,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:142,security,model,models,142,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:219,security,model,model,219,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:314,security,model,model,314,"I'm not at the Allen institute anymore, so this might move pretty slowly. But the other thing that would need to be done is check whether the models need to be retrained. I guess to check this you would need to run the model evaluations in project.yml and make sure the metrics are the same for at least the small model or something, on spacy 3.1 and spacy 3.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:97,deployability,build,build,97,"I should also note that if you urgently need this for your workflow, you can make the change and build scispacy from source",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:59,usability,workflow,workflow,59,"I should also note that if you urgently need this for your workflow, you can make the change and build scispacy from source",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:56,deployability,manag,managed,56,@dakinggg Does that mean that the SciSpacy is no longer managed under the Allen Institute?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:56,energy efficiency,manag,managed,56,@dakinggg Does that mean that the SciSpacy is no longer managed under the Allen Institute?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:10,reliability,Doe,Does,10,@dakinggg Does that mean that the SciSpacy is no longer managed under the Allen Institute?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:56,safety,manag,managed,56,@dakinggg Does that mean that the SciSpacy is no longer managed under the Allen Institute?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:13,modifiability,pac,package,13,It means the package is very lightly maintained right now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:37,modifiability,maintain,maintained,37,It means the package is very lightly maintained right now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:37,safety,maintain,maintained,37,It means the package is very lightly maintained right now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:36,energy efficiency,model,models,36,Closing because I am retraining the models for spacy 3.2,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/pull/411:36,security,model,models,36,Closing because I am retraining the models for spacy 3.2,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/411
https://github.com/allenai/scispacy/issues/412:21,availability,error,error,21,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:42,availability,error,error,42,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:67,deployability,instal,install,67,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:21,performance,error,error,21,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:42,performance,error,error,42,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:21,safety,error,error,21,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:42,safety,error,error,42,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:21,usability,error,error,21,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:42,usability,error,error,42,"This looks an nmslib error not a scispacy error, can you do:. `pip install nmslib` and see what happens ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:34,availability,error,error,34,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:74,availability,error,error,74,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:128,availability,error,error,128,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:178,availability,error,error,178,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:198,availability,error,error,198,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:340,availability,error,error,340,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:524,availability,ERROR,ERROR,524,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:851,availability,ERROR,ERROR,851,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:866,availability,error,errored,866,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:439,deployability,fail,failed,439,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:531,deployability,Fail,Failed,531,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:538,deployability,build,building,538,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:610,deployability,automat,automatically,610,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:669,deployability,fail,fails,669,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:692,deployability,Instal,Installing,692,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:766,deployability,instal,install,766,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:282,interoperability,convers,conversion,282,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:713,modifiability,pac,packages,713,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:34,performance,error,error,34,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:74,performance,error,error,74,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:128,performance,error,error,128,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:178,performance,error,error,178,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:198,performance,error,error,198,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:340,performance,error,error,340,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:524,performance,ERROR,ERROR,524,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:851,performance,ERROR,ERROR,851,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:866,performance,error,errored,866,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:439,reliability,fail,failed,439,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:531,reliability,Fail,Failed,531,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:669,reliability,fail,fails,669,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:34,safety,error,error,34,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:74,safety,error,error,74,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:128,safety,error,error,128,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:178,safety,error,error,178,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:198,safety,error,error,198,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:340,safety,error,error,340,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:524,safety,ERROR,ERROR,524,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:851,safety,ERROR,ERROR,851,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:866,safety,error,errored,866,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:326,security,loss,loss,326,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:610,testability,automat,automatically,610,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:34,usability,error,error,34,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:74,usability,error,error,74,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:128,usability,error,error,128,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:178,usability,error,error,178,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:198,usability,error,error,198,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:340,usability,error,error,340,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:347,usability,command,command,347,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:391,usability,Visual,Visual,391,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:524,usability,ERROR,ERROR,524,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:851,usability,ERROR,ERROR,851,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:858,usability,Command,Command,858,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:866,usability,error,errored,866,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/412:888,usability,statu,status,888,"I just did again, and that is the error I get ... ```cml. nmslib.cc(454): error C2017: illegal escape sequence. nmslib.cc(454): error C2001: newline in constant. nmslib.cc(459): error C2059: syntax error: 'pybind11::enum_<similarity::DistType>'. nmslib.cc(730): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data. error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe' failed with exit code 2. ----------------------------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. ```. Then automatically `runs setup.py clean for nmslib` and it also fails giving:. ```cml. Installing collected packages: nmslib, conllu, scispacy. Running setup.py install for nmslib ... /. ---------------------------------------------------------. ERROR: Command errored out with exit status 1: .... ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/412
https://github.com/allenai/scispacy/issues/413:122,availability,error,error,122,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:188,deployability,updat,updating,188,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:258,deployability,instal,install,258,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:122,performance,error,error,122,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:122,safety,error,error,122,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:188,safety,updat,updating,188,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:188,security,updat,updating,188,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/413:122,usability,error,error,122,"If running on Google Colab, see this comment: https://github.com/allenai/scispacy/issues/328#issuecomment-791043751. This error appears in Colab due to preinstalled libraries. At present, updating the `blis` library appears to solve the problem, e.g., `!pip install -U blis`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/413
https://github.com/allenai/scispacy/issues/415:128,modifiability,variab,variable,128,"I think you actually can do this, although admittedly I have not tried it. Can you try setting the `SCISPACY_CACHE` environment variable (used on this line https://github.com/allenai/scispacy/blob/3d153ddad1f11f000f961f7a92c0d862b93c0973/scispacy/file_cache.py#L16) to whatever folder you want to use, before importing the library?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:561,availability,down,download,561,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:349,deployability,updat,update,349,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1159,integrability,sub,subfolder,1159,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:372,modifiability,variab,variable,372,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:623,modifiability,variab,variable,623,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:873,modifiability,paramet,parameter,873,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:103,performance,cach,cached,103,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:148,performance,cach,caching,148,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:166,performance,cach,cache,166,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:252,performance,content,content,252,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:287,performance,content,content,287,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:452,performance,content,content,452,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:536,performance,cach,cached,536,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:766,performance,cach,cached,766,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1082,performance,content,content,1082,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1189,performance,content,content,1189,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:518,reliability,doe,does,518,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:275,safety,test,test,275,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:310,safety,test,test,310,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:349,safety,updat,update,349,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:475,safety,test,test,475,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1105,safety,test,test,1105,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1212,safety,test,test,1212,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:349,security,updat,update,349,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:275,testability,test,test,275,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:310,testability,test,test,310,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:475,testability,test,test,475,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:860,testability,simpl,simply,860,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1105,testability,test,test,1105,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:1212,testability,test,test,1212,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/415:860,usability,simpl,simply,860,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python. !mv /root/.scispacy/ /content/gdrive/MyDrive/test/. !ls /content/gdrive/MyDrive/test/.scispacy/. >>> datasets. ```. To update the environment variable, as described:. ```python. import os. os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'. ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python. nlp.add_pipe(. ""scispacy_linker"",. config={. ""resolve_abbreviations"": True,. ""linker_name"": ""umls"",. ""cache_folder"": ""/content/gdrive/MyDrive/test/""}). ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415
https://github.com/allenai/scispacy/issues/417:42,availability,error,error,42,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:208,availability,error,error,208,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:223,availability,error,error,223,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:300,availability,ERROR,ERROR,300,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:5,deployability,instal,installation,5,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:112,deployability,releas,released,112,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:150,deployability,instal,install,150,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:248,deployability,instal,installation,248,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:313,deployability,depend,dependency,313,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:396,deployability,instal,installed,396,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:453,deployability,depend,dependency,453,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:581,deployability,instal,installed,581,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:625,deployability,log,loggers-,625,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:723,deployability,updat,updated,723,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:342,energy efficiency,current,currently,342,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:313,integrability,depend,dependency,313,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:453,integrability,depend,dependency,453,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:68,interoperability,compatib,compatible,68,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:464,interoperability,conflict,conflicts,464,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:554,interoperability,incompatib,incompatible,554,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:121,modifiability,pac,package,121,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:313,modifiability,depend,dependency,313,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:378,modifiability,pac,packages,378,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:453,modifiability,depend,dependency,453,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:42,performance,error,error,42,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:208,performance,error,error,208,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:223,performance,error,error,223,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:300,performance,ERROR,ERROR,300,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:229,reliability,doe,does,229,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:333,reliability,doe,does,333,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:42,safety,error,error,42,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:208,safety,error,error,208,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:223,safety,error,error,223,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:300,safety,ERROR,ERROR,300,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:313,safety,depend,dependency,313,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:453,safety,depend,dependency,453,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:625,safety,log,loggers-,625,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:723,safety,updat,updated,723,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:625,security,log,loggers-,625,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:723,security,updat,updated,723,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:313,testability,depend,dependency,313,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:453,testability,depend,dependency,453,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:625,testability,log,loggers-,625,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:42,usability,error,error,42,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:208,usability,error,error,208,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:223,usability,error,error,223,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:300,usability,ERROR,ERROR,300,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:412,usability,behavi,behaviour,412,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:661,usability,support,support,661,"What installation process led you to this error? scispacy should be compatible with spacy 3.2.2 even though the released package says it is not. If I install scispacy and then spacy==3.2.2, I get a different error, but the error does not block the installation and everything still seems to work. ```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scispacy 0.4.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.2 which is incompatible. Successfully installed langcodes-3.3.0 spacy-3.2.2 spacy-loggers-1.0.1```. As for ""official"" support, I do not know if or when scispacy will be officially updated to use spacy 3.2.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:35,availability,error,error,35,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:55,energy efficiency,load,load,55,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:73,integrability,transform,transformers,73,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:73,interoperability,transform,transformers,73,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:35,performance,error,error,35,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:55,performance,load,load,55,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:35,safety,error,error,35,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:35,usability,error,error,35,"Thanks for the response, I get the error when I try to load spacy, spacy-transformers and then scispacy. . Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:50,availability,error,error,50,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:125,availability,error,error,125,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:8,interoperability,share,share,8,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:50,performance,error,error,50,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:125,performance,error,error,125,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:50,safety,error,error,50,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:125,safety,error,error,125,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:24,usability,command,commands,24,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:50,usability,error,error,50,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:125,usability,error,error,125,"Can you share the exact commands that produce the error? But also, the libraries probably still work even though you got the error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:363,availability,error,error,363,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:923,availability,ERROR,ERROR,923,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:201,deployability,instal,install,201,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:211,deployability,upgrad,upgrade,211,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:228,deployability,instal,install,228,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:247,deployability,instal,install,247,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:279,deployability,instal,install,279,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:324,deployability,instal,install,324,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:376,deployability,Instal,Installing,376,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:559,deployability,instal,installation,559,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:695,deployability,instal,installation,695,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:831,deployability,instal,installation,831,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:936,deployability,depend,dependency,936,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1019,deployability,instal,installed,1019,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1076,deployability,depend,dependency,1076,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1214,deployability,instal,installed,1214,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:965,energy efficiency,current,currently,965,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:261,integrability,transform,transformers,261,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:936,integrability,depend,dependency,936,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1076,integrability,depend,dependency,1076,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1104,integrability,transform,transformers,1104,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:261,interoperability,transform,transformers,261,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1087,interoperability,conflict,conflicts,1087,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1104,interoperability,transform,transformers,1104,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1187,interoperability,incompatib,incompatible,1187,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:211,modifiability,upgrad,upgrade,211,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:397,modifiability,pac,packages,397,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:936,modifiability,depend,dependency,936,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1001,modifiability,pac,packages,1001,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1076,modifiability,depend,dependency,1076,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:363,performance,error,error,363,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:923,performance,ERROR,ERROR,923,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:956,reliability,doe,does,956,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:363,safety,error,error,363,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:923,safety,ERROR,ERROR,923,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:936,safety,depend,dependency,936,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1076,safety,depend,dependency,1076,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:936,testability,depend,dependency,936,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1076,testability,depend,dependency,1076,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:27,usability,command,commands,27,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:363,usability,error,error,363,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:483,usability,learn,learn,483,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:923,usability,ERROR,ERROR,923,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1035,usability,behavi,behaviour,1035,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/417:1309,usability,learn,learn-,1309,"I am running the following commands in macOS v12.2.1 and python 3.9.10. ```. python. Python 3.9.10 (main, Jan 15 2022, 11:48:04) . [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```. python -m pip install --upgrade pip. pip install spacy. pip install spacy-transformers. pip install scispacy. ```. When executing . `pip install scispacy ` I get the following error:. ```. Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy. Attempting uninstall: click. Found existing installation: click 8.0.4. Uninstalling click-8.0.4:. Successfully uninstalled click-8.0.4. Attempting uninstall: typer. Found existing installation: typer 0.4.0. Uninstalling typer-0.4.0:. Successfully uninstalled typer-0.4.0. Attempting uninstall: spacy. Found existing installation: spacy 3.2.2. Uninstalling spacy-3.2.2:. Successfully uninstalled spacy-3.2.2. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible. Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417
https://github.com/allenai/scispacy/issues/418:48,testability,simpl,simplest,48,"spacy Docs are not really editable. I think the simplest way is to convert to string, replace the part of the string you want to, and then reprocess to a spacy doc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/418
https://github.com/allenai/scispacy/issues/418:48,usability,simpl,simplest,48,"spacy Docs are not really editable. I think the simplest way is to convert to string, replace the part of the string you want to, and then reprocess to a spacy doc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/418
https://github.com/allenai/scispacy/issues/419:50,security,sign,sign-request,50,fix is to adjust that final line to include `--no-sign-request` - ill issue a PR if i get a chance.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/419
https://github.com/allenai/scispacy/pull/420:22,safety,review,review,22,Tagging @dakinggg for review,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/420
https://github.com/allenai/scispacy/pull/420:22,testability,review,review,22,Tagging @dakinggg for review,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/420
https://github.com/allenai/scispacy/issues/421:199,integrability,coupl,couple,199,"I unfortunately don't have the exact details to provide you, @DeNeutoy might remember some more details, but I believe it was the same splits/processing spacy uses which appear to be referenced in a couple places (https://github.com/explosion/spaCy/issues/5276, https://github.com/explosion/spaCy/issues/3587#issuecomment-483191672).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/421
https://github.com/allenai/scispacy/issues/421:199,modifiability,coupl,couple,199,"I unfortunately don't have the exact details to provide you, @DeNeutoy might remember some more details, but I believe it was the same splits/processing spacy uses which appear to be referenced in a couple places (https://github.com/explosion/spaCy/issues/5276, https://github.com/explosion/spaCy/issues/3587#issuecomment-483191672).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/421
https://github.com/allenai/scispacy/issues/421:77,safety,reme,remember,77,"I unfortunately don't have the exact details to provide you, @DeNeutoy might remember some more details, but I believe it was the same splits/processing spacy uses which appear to be referenced in a couple places (https://github.com/explosion/spaCy/issues/5276, https://github.com/explosion/spaCy/issues/3587#issuecomment-483191672).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/421
https://github.com/allenai/scispacy/issues/421:199,testability,coupl,couple,199,"I unfortunately don't have the exact details to provide you, @DeNeutoy might remember some more details, but I believe it was the same splits/processing spacy uses which appear to be referenced in a couple places (https://github.com/explosion/spaCy/issues/5276, https://github.com/explosion/spaCy/issues/3587#issuecomment-483191672).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/421
https://github.com/allenai/scispacy/issues/424:153,availability,mainten,maintenance,153,"hopefully pretty soon, am training the new models right now, hopefully that goes smoothly. Neither I nor the other co creator work on this right now, so maintenance is slow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:168,availability,slo,slow,168,"hopefully pretty soon, am training the new models right now, hopefully that goes smoothly. Neither I nor the other co creator work on this right now, so maintenance is slow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:43,energy efficiency,model,models,43,"hopefully pretty soon, am training the new models right now, hopefully that goes smoothly. Neither I nor the other co creator work on this right now, so maintenance is slow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:153,reliability,mainten,maintenance,153,"hopefully pretty soon, am training the new models right now, hopefully that goes smoothly. Neither I nor the other co creator work on this right now, so maintenance is slow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:168,reliability,slo,slow,168,"hopefully pretty soon, am training the new models right now, hopefully that goes smoothly. Neither I nor the other co creator work on this right now, so maintenance is slow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:43,security,model,models,43,"hopefully pretty soon, am training the new models right now, hopefully that goes smoothly. Neither I nor the other co creator work on this right now, so maintenance is slow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:96,deployability,upgrad,upgrade,96,"This is done, except that I am having some issues with pypi, you can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:96,modifiability,upgrad,upgrade,96,"This is done, except that I am having some issues with pypi, you can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/424:14,safety,except,except,14,"This is done, except that I am having some issues with pypi, you can use master if you need the upgrade now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/424
https://github.com/allenai/scispacy/issues/426:12,usability,close,closed,12,This can be closed - I found I was using python 3.9 which isn't yet supported.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/426
https://github.com/allenai/scispacy/issues/426:68,usability,support,supported,68,This can be closed - I found I was using python 3.9 which isn't yet supported.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/426
https://github.com/allenai/scispacy/issues/429:486,deployability,integr,integration,486,"Hey Eric, thanks for the pointer to Gilda! I'm not familiar with it, but will have a look at some point. Id like for scispacy to have a trained entity linker (that compares the textual context with the entity definitions) on top of the entity linking (more of a candidate generator) that we added a while ago, but it's not something I've had time to do. I also haven't looked at spacys native entity linker, but that would be the natural place to start. If someone wanted to propose an integration with an existing entity linking library with some evaluation, I'd be happy to have a look!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/429
https://github.com/allenai/scispacy/issues/429:486,integrability,integr,integration,486,"Hey Eric, thanks for the pointer to Gilda! I'm not familiar with it, but will have a look at some point. Id like for scispacy to have a trained entity linker (that compares the textual context with the entity definitions) on top of the entity linking (more of a candidate generator) that we added a while ago, but it's not something I've had time to do. I also haven't looked at spacys native entity linker, but that would be the natural place to start. If someone wanted to propose an integration with an existing entity linking library with some evaluation, I'd be happy to have a look!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/429
https://github.com/allenai/scispacy/issues/429:486,interoperability,integr,integration,486,"Hey Eric, thanks for the pointer to Gilda! I'm not familiar with it, but will have a look at some point. Id like for scispacy to have a trained entity linker (that compares the textual context with the entity definitions) on top of the entity linking (more of a candidate generator) that we added a while ago, but it's not something I've had time to do. I also haven't looked at spacys native entity linker, but that would be the natural place to start. If someone wanted to propose an integration with an existing entity linking library with some evaluation, I'd be happy to have a look!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/429
https://github.com/allenai/scispacy/issues/429:486,modifiability,integr,integration,486,"Hey Eric, thanks for the pointer to Gilda! I'm not familiar with it, but will have a look at some point. Id like for scispacy to have a trained entity linker (that compares the textual context with the entity definitions) on top of the entity linking (more of a candidate generator) that we added a while ago, but it's not something I've had time to do. I also haven't looked at spacys native entity linker, but that would be the natural place to start. If someone wanted to propose an integration with an existing entity linking library with some evaluation, I'd be happy to have a look!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/429
https://github.com/allenai/scispacy/issues/429:342,performance,time,time,342,"Hey Eric, thanks for the pointer to Gilda! I'm not familiar with it, but will have a look at some point. Id like for scispacy to have a trained entity linker (that compares the textual context with the entity definitions) on top of the entity linking (more of a candidate generator) that we added a while ago, but it's not something I've had time to do. I also haven't looked at spacys native entity linker, but that would be the natural place to start. If someone wanted to propose an integration with an existing entity linking library with some evaluation, I'd be happy to have a look!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/429
